"""
æ¨ç†å±‚ï¼ˆModel Orchestratorï¼‰ï¼š
æ ¹æ® OPTIMIZATION_NOTES.md çš„äº”å±‚æ¶æ„è®¾è®¡

èŒè´£ï¼š
- è°ƒç”¨å¤šä¸ªå¤§æ¨¡å‹ APIï¼ˆé€šè¿‡ç»Ÿä¸€æ¥å£ AICanAPIï¼‰
- æ”¯æŒçš„æ¨¡å‹ï¼šGPT-4oã€Claudeã€Geminiã€Grokã€Qwenã€DeepSeek ç­‰
- å¹¶å‘è°ƒç”¨å¤šä¸ªæ¨¡å‹ï¼Œæé«˜æ•ˆç‡
- è§£ææ¨¡å‹è¿”å›çš„å†…å®¹ï¼šåˆ¤æ–­å€¾å‘ã€ç†ç”±è¯´æ˜ã€ä¸»è§‚æ¦‚ç‡
- ä» config/models.json è¯»å–æ¨¡å‹é…ç½®
- æ”¯æŒè‡ªåŠ¨é™çº§æœºåˆ¶

è¾“å…¥ï¼šå„æ¨¡å‹çš„ promptï¼ˆå­—ç¬¦ä¸²ï¼‰
è¾“å‡ºï¼šå„æ¨¡å‹çš„é¢„æµ‹ç»“æœ {probability, confidence, reasoning}
"""
import aiohttp
import asyncio
import json
import os
import time
import traceback
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dotenv import load_dotenv

load_dotenv()

# å„æ¨¡å‹è‡ªé€‚åº”è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰- åŸºäºå®é™…APIå“åº”é€Ÿåº¦
MODEL_TIMEOUTS = {
    "gpt-4o": 30,
    "claude-3-7-sonnet-latest": 50,
    "claude-3.7-sonnet": 50,
    "claude-opus-4-1": 50,
    "gemini-2.5-pro": 45,
    "gemini-2.5-flash": 40,
    "grok-4": 60,
    "grok-3": 55,
    "deepseek-chat": 35,
}


class ModelOrchestrator:
    """
    Orchestrates parallel calls to multiple AI models via AICanAPI.
    
    ç®¡ç†å¤šä¸ªAIæ¨¡å‹çš„å¹¶å‘è°ƒç”¨ï¼š
    - é€šè¿‡ AICanAPI ç»Ÿä¸€æ¥å£è°ƒç”¨ä¸åŒæ¨¡å‹
    - æ”¯æŒæ¨¡å‹æƒé‡é…ç½®
    - è§£æ JSON æ ¼å¼çš„é¢„æµ‹ç»“æœ
    - é”™è¯¯å¤„ç†å’Œè¶…æ—¶æ§åˆ¶
    - ä» config/models.json è¯»å–é…ç½®
    - æ”¯æŒè‡ªåŠ¨é™çº§æœºåˆ¶
    """
    
    # å„æ¨¡å‹è‡ªé€‚åº”è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰- æ ¹æ®å®é™…APIå“åº”é€Ÿåº¦è°ƒæ•´
    MODEL_TIMEOUTS = {
        "gpt-4o": 30,
        "claude-3-7-sonnet-latest": 50,
        "claude-opus-4-1": 50,
        "claude-3-5-opus-latest": 50,
        "gemini-2.5-pro": 45,
        "gemini-2.5-flash": 40,
        "grok-4": 60,  # Grokå“åº”è¾ƒæ…¢ï¼Œç»™æ›´å¤šæ—¶é—´
        "grok-3": 60,
        "deepseek-chat": 35,
    }
    
    # å…¼å®¹æ—§çš„å¸¸é‡ï¼ˆä½¿ç”¨æœ€å¤§è¶…æ—¶ä½œä¸ºé»˜è®¤å€¼ï¼‰
    SINGLE_MODEL_TIMEOUT = 60  # é»˜è®¤è¶…æ—¶ï¼ˆä½¿ç”¨æœ€å¤§æ¨¡å‹çš„è¶…æ—¶ï¼‰
    PARALLEL_CALLS_TIMEOUT = 90  # å¹¶è¡Œè°ƒç”¨æ€»è¶…æ—¶ï¼ˆè€ƒè™‘é‡è¯•ï¼‰
    MAX_TOTAL_WAIT_TIME = 90  # æœ€å¤§æ€»ç­‰å¾…æ—¶é—´ï¼ˆåŒ…æ‹¬é‡è¯•å’Œé™çº§ï¼‰
    
    # é‡è¯•é…ç½®
    MAX_RETRIES = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°
    RETRY_DELAY_BASE = 5  # åŸºç¡€é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
    RETRY_DELAY_MAX = 10  # æœ€å¤§é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
    
    # å¹¶å‘æ§åˆ¶
    MAX_CONCURRENT_MODELS = 3  # åŒæ—¶æœ€å¤šè¿è¡Œçš„æ¨¡å‹æ•°
    
    def __init__(self):
        """Initialize ModelOrchestrator and load model configurations from JSON."""
        self.models_config = self._load_models_config()
        self.MODELS = self._build_models_dict()
        self.active_models = {}  # Track actually used models (with fallback handling)
        self._log_model_versions()
    
    def _load_models_config(self) -> Dict:
        """Load model configurations from config/models.json."""
        config_path = Path(__file__).parent.parent / "config" / "models.json"
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            return config
        except FileNotFoundError:
            print(f"âš ï¸ é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {config_path}")
            print("   ä½¿ç”¨é»˜è®¤é…ç½®...")
            return self._get_default_config()
        except json.JSONDecodeError as e:
            print(f"âš ï¸ é…ç½®æ–‡ä»¶è§£æé”™è¯¯: {e}")
            print("   ä½¿ç”¨é»˜è®¤é…ç½®...")
            return self._get_default_config()
    
    def _get_default_config(self) -> Dict:
        """Return default configuration if JSON file is not available."""
        return {
            "models": {
                "gpt-4o": {
                    "display_name": "GPT-4o",
                    "model_id": "gpt-4o",
                    "source": "aicanapi",
                    "api_key_env": "AICANAPI_KEY",
                    "weight": 3.0,
                    "is_default": True,
                    "fallback": None,
                    "last_updated": "2024-11-21"
                }
            },
            "api_endpoints": {
                "aicanapi": "https://aicanapi.com/v1/chat/completions",
                "deepseek": "https://api.deepseek.com/v1/chat/completions"
            }
        }
    
    def _build_models_dict(self) -> Dict:
        """Build MODELS dict from JSON configuration, filtering enabled models."""
        models_dict = {}
        api_endpoints = self.models_config.get("api_endpoints", {})
        enabled_models = []
        disabled_models = []
        
        for model_key, model_config in self.models_config.get("models", {}).items():
            # Check if model is enabled (default to True for backward compatibility)
            if not model_config.get("enabled", True):
                disabled_models.append(model_config.get("display_name", model_config.get("model_id", model_key)))
                # Skip disabled models and their fallbacks
                continue
                
            model_id = model_config["model_id"]
            source = model_config["source"]
            url = api_endpoints.get(source, "")
            
            enabled_models.append(model_config.get("display_name", model_id))
            
            models_dict[model_id] = {
                "display_name": model_config.get("display_name", model_id),
                "source": source,
                "url": url,
                "api_key_env": model_config["api_key_env"],
                "weight": model_config["weight"],
                "fallback": model_config.get("fallback"),
                "fallback_display_name": model_config.get("fallback_display_name"),
                "last_updated": model_config.get("last_updated", "æœªçŸ¥"),
                "is_default": model_config.get("is_default", False)
            }
            
            # Add fallback model if exists (only if primary model is enabled)
            if model_config.get("fallback"):
                fallback_id = model_config["fallback"]
                models_dict[fallback_id] = {
                    "display_name": model_config.get("fallback_display_name", fallback_id),
                    "source": source,
                    "url": url,
                    "api_key_env": model_config["api_key_env"],
                    "weight": model_config["weight"] * 0.9,  # Slightly lower weight for fallback
                    "fallback": None,
                    "last_updated": model_config.get("last_updated", "æœªçŸ¥"),
                    "is_default": False
                }
        
        # Print confirmation log
        if enabled_models:
            enabled_str = ", ".join(enabled_models)
            print(f"[DEBUG] Active models: {enabled_str}")
        if disabled_models:
            disabled_str = ", ".join(disabled_models)
            print(f"[DEBUG] Disabled models: {disabled_str}")
        
        return models_dict
    
    def _log_model_versions(self):
        """Log current model versions."""
        active_versions = []
        for model_key, model_config in self.models_config.get("models", {}).items():
            display_name = model_config.get("display_name", model_key)
            active_versions.append(display_name)
        
        versions_str = " / ".join(active_versions)
        print(f"ğŸ“Š å½“å‰ä½¿ç”¨æ¨¡å‹ç‰ˆæœ¬: {versions_str}")
    
    def get_model_info(self, model_name: str) -> Optional[Dict]:
        """Get model information including display name and version."""
        if model_name in self.MODELS:
            return {
                "model_id": model_name,
                "display_name": self.MODELS[model_name].get("display_name", model_name),
                "last_updated": self.MODELS[model_name].get("last_updated", "æœªçŸ¥"),
                "weight": self.MODELS[model_name].get("weight", 1.0)
            }
        
        # Check if it's a fallback model
        for config in self.models_config.get("models", {}).values():
            if config.get("fallback") == model_name:
                return {
                    "model_id": model_name,
                    "display_name": config.get("fallback_display_name", model_name),
                    "last_updated": config.get("last_updated", "æœªçŸ¥"),
                    "weight": self.MODELS.get(model_name, {}).get("weight", 1.0) if model_name in self.MODELS else 1.0
                }
        
        return None
    
    def get_active_models_summary(self) -> Dict[str, Dict]:
        """Get summary of all active models with their versions."""
        summary = {}
        for model_name in self.MODELS.keys():
            info = self.get_model_info(model_name)
            if info:
                summary[model_name] = info
        return summary
    
    def _get_model_timeout(self, model_name: str) -> float:
        """
        è·å–æ¨¡å‹çš„è‡ªé€‚åº”è¶…æ—¶æ—¶é—´
        
        Args:
            model_name: æ¨¡å‹åç§°
        
        Returns:
            è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        """
        # å°è¯•ç²¾ç¡®åŒ¹é…
        if model_name.lower() in self.MODEL_TIMEOUTS:
            return self.MODEL_TIMEOUTS[model_name.lower()]
        
        # å°è¯•éƒ¨åˆ†åŒ¹é…ï¼ˆä¾‹å¦‚ "claude-3-7-sonnet-latest" åŒ¹é… "claude"ï¼‰
        model_lower = model_name.lower()
        for key, timeout in self.MODEL_TIMEOUTS.items():
            if key in model_lower or model_lower in key:
                return timeout
        
        # é»˜è®¤è¶…æ—¶
        return 40.0
    
    async def _call_single_model(self, model_name: str, prompt: str) -> Optional[Dict]:
        """
        å•æ¬¡æ¨¡å‹è°ƒç”¨ï¼ˆä¸åŒ…å«é‡è¯•é€»è¾‘ï¼‰
        
        Args:
            model_name: æ¨¡å‹åç§°
            prompt: æç¤ºè¯
        
        Returns:
            æ¨¡å‹å“åº”ç»“æœæˆ–None
        """
        return await self._call_model_internal(model_name, prompt)
    
    def _get_model_timeout(self, model_name: str) -> float:
        """
        è·å–æ¨¡å‹çš„è‡ªé€‚åº”è¶…æ—¶æ—¶é—´
        
        Args:
            model_name: æ¨¡å‹åç§°
        
        Returns:
            è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        """
        # å°è¯•ç²¾ç¡®åŒ¹é…
        if model_name in MODEL_TIMEOUTS:
            return MODEL_TIMEOUTS[model_name]
        
        # å°è¯•å°å†™åŒ¹é…
        model_lower = model_name.lower()
        for key, timeout in MODEL_TIMEOUTS.items():
            if key.lower() in model_lower or model_lower in key.lower():
                return timeout
        
        # é»˜è®¤è¶…æ—¶
        return self.SINGLE_MODEL_TIMEOUT
    
    async def call_model(self, model_name: str, prompt: str, max_retries: int = None) -> Optional[Dict]:
        """
        è°ƒç”¨å•ä¸ªæ¨¡å‹ï¼ˆå¸¦è‡ªé€‚åº”è¶…æ—¶ + é‡è¯•æœºåˆ¶ï¼‰
        
        Args:
            model_name: æ¨¡å‹åç§°
            prompt: æç¤ºè¯
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤ä½¿ç”¨ç±»é…ç½®ï¼‰
        
        Returns:
            Dict with 'probability', 'confidence', 'reasoning', or None on error
        """
        if max_retries is None:
            max_retries = self.MAX_RETRIES
        
        timeout_seconds = self._get_model_timeout(model_name)
        start_time = time.time()
        
        print(f"[DEBUG] Calling {model_name} (timeout={timeout_seconds}s, max_retries={max_retries})")
        
        for attempt in range(max_retries):
            attempt_start = time.time()
            try:
                # ä½¿ç”¨ asyncio.wait_for å®ç°è¶…æ—¶æ§åˆ¶ï¼ˆå…¼å®¹æ‰€æœ‰Pythonç‰ˆæœ¬ï¼‰
                result = await asyncio.wait_for(
                    self._call_model_internal(model_name, prompt),
                    timeout=timeout_seconds
                )
                elapsed = time.time() - start_time
                
                if result:
                    print(f"[DEBUG] âœ… {model_name} completed in {elapsed:.2f}s (attempt {attempt+1})")
                    return result
                else:
                    print(f"[DEBUG] âš ï¸ {model_name} returned None (attempt {attempt+1}/{max_retries})")
                    
            except asyncio.TimeoutError:
                attempt_elapsed = time.time() - attempt_start
                total_elapsed = time.time() - start_time
                print(f"[TIMEOUT] âš ï¸ {model_name} attempt {attempt+1}/{max_retries} exceeded {timeout_seconds}s (actual: {attempt_elapsed:.2f}s, total: {total_elapsed:.2f}s)")
                
                if attempt < max_retries - 1:
                    # è®¡ç®—é‡è¯•å»¶è¿Ÿï¼ˆé€’å¢ï¼š5s, 8s, 10sï¼‰
                    wait_time = min(self.RETRY_DELAY_BASE * (attempt + 1), self.RETRY_DELAY_MAX)
                    print(f"[RETRY] Retrying {model_name} after {wait_time}s (attempt {attempt+2}/{max_retries})...")
                    await asyncio.sleep(wait_time)
                else:
                    print(f"[FAIL] âŒ {model_name} failed after {max_retries} attempts (timeout).")
                    # è¿”å›ä½ç½®ä¿¡åº¦ç»“æœï¼Œç¡®ä¿æµç¨‹ä¸ä¸­æ–­
                    return {
                        "probability": 50.0,
                        "confidence": "low",
                        "reasoning": f"Timeout after {max_retries} attempts (last attempt exceeded {timeout_seconds}s)"
                    }
                    
            except Exception as e:
                attempt_elapsed = time.time() - attempt_start
                total_elapsed = time.time() - start_time
                print(f"[ERROR] {model_name} exception on attempt {attempt+1}/{max_retries}: {type(e).__name__}: {e} (elapsed: {attempt_elapsed:.2f}s)")
                traceback.print_exc()
                
                if attempt < max_retries - 1:
                    wait_time = min(3, self.RETRY_DELAY_BASE)  # å¼‚å¸¸æ—¶ä½¿ç”¨è¾ƒçŸ­å»¶è¿Ÿ
                    print(f"[RETRY] Retrying {model_name} after {wait_time}s (attempt {attempt+2}/{max_retries})...")
                    await asyncio.sleep(wait_time)
                else:
                    print(f"[FAIL] âŒ {model_name} failed after {max_retries} attempts (exception).")
                    return {
                        "probability": 50.0,
                        "confidence": "low",
                        "reasoning": f"Exception after {max_retries} attempts: {type(e).__name__}: {str(e)[:100]}"
                    }
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼ˆä¸åº”è¯¥åˆ°è¾¾è¿™é‡Œï¼Œå› ä¸ºä¸Šé¢å·²ç»returnäº†ï¼‰
        total_elapsed = time.time() - start_time
        print(f"[DEBUG] {model_name} total elapsed {total_elapsed:.2f}s (all attempts failed)")
        return {
            "probability": 50.0,
            "confidence": "low",
            "reasoning": "All retry attempts failed"
        }
    
    async def _call_model_internal(self, model_name: str, prompt: str) -> Optional[Dict]:
        """Internal method to call a model API with detailed logging."""
        if model_name not in self.MODELS:
            print(f"[DEBUG] {model_name} not in MODELS dict, skipping")
            return None
        
        config = self.MODELS[model_name]
        api_key = os.getenv(config["api_key_env"], "")
        
        # è¯¦ç»†æ—¥å¿—ï¼šAPI keyæ£€æŸ¥
        has_api_key = bool(api_key)
        print(f"[DEBUG] Start calling {model_name}")
        print(f"[DEBUG] {model_name} prompt length: {len(prompt)}")
        print(f"[DEBUG] {model_name} API key loaded: {has_api_key}")
        print(f"[DEBUG] {model_name} request started at: {time.time():.2f}")
        
        if not api_key:
            print(f"âš ï¸ [ERROR] No API key for {model_name}, skipping")
            return None
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}"
        }
        
        # AICanAPI and DeepSeek use OpenAI-compatible API format
        # Determine model identifier based on source
        if config["source"] == "deepseek":
            # DeepSeek official API uses "deepseek-chat" as model name
            model_identifier = "deepseek-chat"
        else:
            # AICanAPI uses the model_name as identifier
            model_identifier = model_name
        
        payload = {
            "model": model_identifier,
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.7,
            "max_tokens": 500
        }
        
        # è®°å½•è¯·æ±‚å¼€å§‹æ—¶é—´
        request_start_time = time.time()
        display_name = config.get("display_name", model_name)
        url = config.get("url", "")
        
        print(f"[DEBUG] {model_name} URL: {url}")
        print(f"[DEBUG] {model_name} Model identifier: {model_identifier}")
        
        try:
            print(f"ğŸ“¡ Calling {display_name} ({model_name}) at {url}")
            
            # ä½¿ç”¨ç¡¬æ€§è¶…æ—¶æ§åˆ¶
            async with aiohttp.ClientSession() as session:
                try:
                    async with session.post(
                        url,
                        json=payload,
                        headers=headers,
                        timeout=aiohttp.ClientTimeout(total=self.SINGLE_MODEL_TIMEOUT)
                    ) as response:
                        response_time = time.time() - request_start_time
                        print(f"[DEBUG] {model_name} received response at: {time.time():.2f} (took {response_time:.2f}s)")
                        if response.status == 200:
                            parse_start = time.time()
                            data = await response.json()
                            content = data.get("choices", [{}])[0].get("message", {}).get("content", "")
                            print(f"âœ… {display_name} responded: {content[:100]}...")
                            result = self._parse_model_response(content)
                            parse_time = time.time() - parse_start
                            
                            total_time = time.time() - request_start_time
                            print(f"[DEBUG] {model_name} parse time: {parse_time:.2f}s, total: {total_time:.2f}s")
                            
                            if result:
                                print(f"âœ… {display_name} parsed successfully: prob={result.get('probability')}%")
                            else:
                                print(f"âš ï¸ {display_name} response parsing failed")
                            return result
                        else:
                            error_text = await response.text()
                            total_time = time.time() - request_start_time
                            print(f"âŒ [ERROR] API error for {display_name}: {response.status} (took {total_time:.2f}s)")
                            print(f"Error details: {error_text[:500]}")
                            return None
                except asyncio.TimeoutError:
                    total_time = time.time() - request_start_time
                    print(f"â±ï¸ [TIMEOUT] {display_name} took too long, returning default. (>{self.SINGLE_MODEL_TIMEOUT}s, actual: {total_time:.2f}s)")
                    # è¿”å›é»˜è®¤å€¼è€Œä¸æ˜¯Noneï¼Œè®©ç³»ç»Ÿå¯ä»¥ç»§ç»­
                    return {
                        "probability": 50.0,
                        "confidence": "low",
                        "reasoning": f"Timeout after {total_time:.2f}s"
                    }
                except aiohttp.ClientError as e:
                    total_time = time.time() - request_start_time
                    print(f"ğŸŒ [ERROR] Network error calling {display_name}: {type(e).__name__}: {e} (took {total_time:.2f}s)")
                    return None
                except Exception as e:
                    total_time = time.time() - request_start_time
                    print(f"âŒ [ERROR] Unexpected error in {display_name} request: {type(e).__name__}: {e} (took {total_time:.2f}s)")
                    import traceback
                    traceback.print_exc()
                    return None
        except asyncio.TimeoutError:
            total_time = time.time() - request_start_time
            print(f"â±ï¸ [TIMEOUT] {display_name} outer timeout (>{self.SINGLE_MODEL_TIMEOUT}s, actual: {total_time:.2f}s)")
            return {
                "probability": 50.0,
                "confidence": "low",
                "reasoning": f"Timeout after {total_time:.2f}s"
            }
        except Exception as e:
            total_time = time.time() - request_start_time
            print(f"âŒ [ERROR] Outer exception calling {display_name}: {type(e).__name__}: {e} (took {total_time:.2f}s)")
            import traceback
            traceback.print_exc()
            return None
    
    def _parse_model_response(self, content: str) -> Optional[Dict]:
        """Parse JSON response from model."""
        if not content or not content.strip():
            print("âš ï¸ Empty response content")
            return None
            
        try:
            # Try to extract JSON from response
            original_content = content
            content = content.strip()
            
            # Look for JSON block
            if "```json" in content:
                start = content.find("```json") + 7
                end = content.find("```", start)
                if end != -1:
                    content = content[start:end].strip()
            elif "```" in content:
                start = content.find("```") + 3
                end = content.find("```", start)
                if end != -1:
                    content = content[start:end].strip()
            
            # Try to find JSON object
            start_brace = content.find("{")
            end_brace = content.rfind("}")
            if start_brace != -1 and end_brace != -1 and end_brace > start_brace:
                content = content[start_brace:end_brace + 1]
            
            # Try to parse JSON
            data = json.loads(content)
            
            # Validate and normalize
            prob = float(data.get("probability", 50.0))
            prob = max(0.0, min(100.0, prob))  # Clamp to [0, 100]
            
            confidence = data.get("confidence", "medium").lower()
            if confidence not in ["low", "medium", "high"]:
                confidence = "medium"
            
            reasoning = data.get("reasoning", "No reasoning provided.")
            if len(reasoning) > 200:
                reasoning = reasoning[:197] + "..."
            
            return {
                "probability": prob,
                "confidence": confidence,
                "reasoning": reasoning
            }
        except json.JSONDecodeError as e:
            print(f"âš ï¸ JSON decode error: {e}")
            print(f"Content preview: {original_content[:300]}")
            # Try to extract probability from text if JSON parsing fails
            import re
            prob_match = re.search(r'probability["\s:]+(\d+\.?\d*)', original_content, re.IGNORECASE)
            if prob_match:
                try:
                    prob = float(prob_match.group(1))
                    prob = max(0.0, min(100.0, prob))
                    print(f"âœ… Extracted probability from text: {prob}%")
                    return {
                        "probability": prob,
                        "confidence": "medium",
                        "reasoning": "Parsed from unstructured response."
                    }
                except:
                    pass
            return None
        except Exception as e:
            print(f"âš ï¸ Error parsing model response: {e}")
            print(f"Content preview: {original_content[:300]}")
            return None
    
    async def call_models_parallel(self, model_names: List[str], prompts: Dict[str, str]) -> Dict[str, Optional[Dict]]:
        """
        æ‰¹é‡è°ƒç”¨å¤šä¸ªæ¨¡å‹ï¼ˆå¸¦æ‰¹æ¬¡æ§åˆ¶å’Œè¯¦ç»†æ—¥å¿—ï¼‰
        
        Args:
            model_names: æ¨¡å‹åç§°åˆ—è¡¨
            prompts: Dict mapping model_name -> prompt
        
        Returns:
            Dict mapping model_name -> response_dict or None
        """
        overall_start_time = time.time()
        print(f"\n[DEBUG] ========== call_models_parallel START ==========")
        print(f"[DEBUG] Total models to call: {len(model_names)}")
        print(f"[DEBUG] Models: {model_names}")
        print(f"[DEBUG] Batch size: {self.MAX_CONCURRENT_MODELS} (max concurrent models)")
        print(f"[DEBUG] Max retries per model: {self.MAX_RETRIES}")
        
        results = {}
        batch_size = self.MAX_CONCURRENT_MODELS
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(model_names), batch_size):
            batch = model_names[i:i+batch_size]
            batch_num = i // batch_size + 1
            total_batches = (len(model_names) + batch_size - 1) // batch_size
            
            print(f"[DEBUG] ğŸš€ Starting batch {batch_num}/{total_batches}: {batch}")
            batch_start = time.time()
            
            # åˆ›å»ºä»»åŠ¡ï¼ˆæ¯ä¸ªä»»åŠ¡å†…éƒ¨åŒ…å«é‡è¯•é€»è¾‘ï¼‰
            tasks = [
                self.call_model(model_name, prompts[model_name])
                for model_name in batch
            ]
            
            # å¹¶å‘æ‰§è¡Œå½“å‰æ‰¹æ¬¡
            try:
                batch_results = await asyncio.gather(*tasks, return_exceptions=True)
                
                # å¤„ç†ç»“æœ
                for model_name, result in zip(batch, batch_results):
                    if isinstance(result, Exception):
                        print(f"[ERROR] {model_name} raised exception: {type(result).__name__}: {result}")
                        results[model_name] = {
                            "probability": 50.0,
                            "confidence": "low",
                            "reasoning": f"Exception: {type(result).__name__}"
                        }
                    else:
                        results[model_name] = result
                
                batch_duration = time.time() - batch_start
                success_count = sum(1 for r in batch_results if r and not isinstance(r, Exception))
                print(f"[DEBUG] âœ… Finished batch {batch_num}/{total_batches} in {batch_duration:.2f}s ({success_count}/{len(batch)} successful)")
                
            except Exception as e:
                print(f"[ERROR] Batch {batch_num} failed: {type(e).__name__}: {e}")
                # ä¸ºå¤±è´¥çš„æ‰¹æ¬¡å¡«å……é»˜è®¤å€¼
                for model_name in batch:
                    if model_name not in results:
                        results[model_name] = {
                            "probability": 50.0,
                            "confidence": "low",
                            "reasoning": "Batch execution failed"
                        }
            
            # æ‰¹æ¬¡é—´çŸ­æš‚åœé¡¿ï¼Œé¿å…APIé™æµ
            if i + batch_size < len(model_names):
                await asyncio.sleep(1.5)
        
        overall_duration = time.time() - overall_start_time
        success_count = sum(1 for r in results.values() if r and r.get("probability") is not None)
        
        print(f"\n[DEBUG] ========== call_models_parallel END ==========")
        print(f"[DEBUG] All batches completed in {overall_duration:.2f}s")
        print(f"[DEBUG] Success: {success_count}/{len(model_names)} models")
        print(f"ğŸ“Š æ¨¡å‹è°ƒç”¨å®Œæˆ: {success_count}/{len(model_names)} æˆåŠŸ")
        
        return results
    
    async def call_all_models(self, prompts: Dict[str, str]) -> Dict[str, Optional[Dict]]:
        """
        Call all models in parallel with individual timeouts and detailed logging.
        å…¼å®¹æ—§æ¥å£ï¼Œå†…éƒ¨è°ƒç”¨call_models_parallel
        
        Args:
            prompts: Dict mapping model_name -> prompt
        
        Returns:
            Dict mapping model_name -> response_dict or None
        """
        model_names = list(prompts.keys())
        return await self.call_models_parallel(model_names, prompts)
        
        async def call_with_timeout(model_name: str, prompt: str) -> Tuple[str, Optional[Dict]]:
            """Call a single model with timeout protection and detailed logging."""
            call_start = time.time()
            print(f"[DEBUG] Task started for {model_name} at {call_start:.2f}")
            
            try:
                # è°ƒç”¨call_modelï¼ˆå®ƒå†…éƒ¨ä¼šè°ƒç”¨_call_model_internalï¼‰
                result = await asyncio.wait_for(
                    self.call_model(model_name, prompt),
                    timeout=self.SINGLE_MODEL_TIMEOUT + 2  # ç¨å¾®å®½æ¾ä¸€ç‚¹ï¼Œå› ä¸º_call_model_internalå·²ç»æœ‰è¶…æ—¶
                )
                call_duration = time.time() - call_start
                print(f"[DEBUG] {model_name} completed in {call_duration:.2f}s")
                return (model_name, result)
            except asyncio.TimeoutError:
                call_duration = time.time() - call_start
                print(f"â±ï¸ [TIMEOUT] {model_name} took too long, returning default. (>={self.SINGLE_MODEL_TIMEOUT}s, actual: {call_duration:.2f}s)")
                # è¿”å›é»˜è®¤å€¼è€Œä¸æ˜¯None
                return (model_name, {
                    "probability": 50.0,
                    "confidence": "low",
                    "reasoning": f"Timeout after {call_duration:.2f}s"
                })
            except Exception as e:
                call_duration = time.time() - call_start
                print(f"âŒ [ERROR] æ¨¡å‹è°ƒç”¨å¼‚å¸¸ â€“ {model_name}: {type(e).__name__}: {e} (took {call_duration:.2f}s)")
                import traceback
                traceback.print_exc()
                return (model_name, None)
        
        # Create tasks with individual timeout protection
        # IMPORTANT: asyncio.wait() requires Task objects, not coroutines
        print(f"[DEBUG] Creating {len(prompts)} tasks...")
        tasks = [
            asyncio.create_task(call_with_timeout(model_name, prompt))
            for model_name, prompt in prompts.items()
        ]
        
        print(f"[DEBUG] All tasks created, starting parallel execution...")
        parallel_start = time.time()
        
        # Use asyncio.wait with timeout for overall control
        try:
            done, pending = await asyncio.wait(
                tasks,
                timeout=self.MAX_TOTAL_WAIT_TIME,
                return_when=asyncio.ALL_COMPLETED
            )
            
            parallel_duration = time.time() - parallel_start
            print(f"[DEBUG] Parallel execution finished in {parallel_duration:.2f}s")
            print(f"[DEBUG] Completed: {len(done)}, Pending: {len(pending)}")
            
            # Cancel pending tasks if we hit overall timeout
            if pending:
                print(f"â±ï¸ [WARNING] å–æ¶ˆ {len(pending)} ä¸ªæœªå®Œæˆçš„æ¨¡å‹è°ƒç”¨ï¼ˆæ€»è¶…æ—¶ {self.MAX_TOTAL_WAIT_TIME}sï¼‰")
                for task in pending:
                    task_name = "unknown"
                    # å°è¯•è·å–ä»»åŠ¡åç§°
                    if hasattr(task, 'get_name'):
                        task_name = task.get_name()
                    print(f"[DEBUG] Cancelling task: {task_name}")
                    task.cancel()
                    try:
                        await task  # Wait for cancellation to complete
                    except asyncio.CancelledError:
                        pass
                    except Exception as e:
                        print(f"[DEBUG] Exception while cancelling task: {e}")
            
            # Collect results
            results_dict = {}
            collect_start = time.time()
            for task in done:
                try:
                    model_name, result = await task
                    results_dict[model_name] = result
                    if result:
                        print(f"[DEBUG] Collected result for {model_name}: prob={result.get('probability', 'N/A')}")
                    else:
                        print(f"[DEBUG] Collected None for {model_name}")
                except asyncio.CancelledError:
                    print(f"[DEBUG] Task was cancelled")
                except Exception as e:
                    print(f"âŒ [ERROR] è·å–æ¨¡å‹ç»“æœå¤±è´¥: {type(e).__name__}: {e}")
                    import traceback
                    traceback.print_exc()
            
            collect_duration = time.time() - collect_start
            print(f"[DEBUG] Result collection took {collect_duration:.2f}s")
            
            # Fill in None for any missing models
            for model_name in prompts.keys():
                if model_name not in results_dict:
                    results_dict[model_name] = {
                        "probability": 50.0,
                        "confidence": "low",
                        "reasoning": "No response received"
                    }
                    print(f"âš ï¸ [WARNING] æ¨¡å‹ {model_name} æœªè¿”å›ç»“æœï¼Œä½¿ç”¨é»˜è®¤å€¼")
        
        except Exception as e:
            overall_duration = time.time() - overall_start_time
            print(f"âŒ [ERROR] call_all_models å‘ç”Ÿå¼‚å¸¸: {type(e).__name__}: {e} (after {overall_duration:.2f}s)")
            import traceback
            traceback.print_exc()
            # Return empty dict with default values as fallback
            results_dict = {
                model_name: {
                    "probability": 50.0,
                    "confidence": "low",
                    "reasoning": f"Exception: {type(e).__name__}"
                }
                for model_name in prompts.keys()
            }
        
        # Count successful calls
        success_count = sum(1 for r in results_dict.values() if r is not None)
        total_count = len(prompts)
        overall_duration = time.time() - overall_start_time
        
        print(f"\n[DEBUG] ========== call_all_models END ==========")
        print(f"[DEBUG] Total execution time: {overall_duration:.2f}s")
        print(f"ğŸ“Š æ¨¡å‹è°ƒç”¨å®Œæˆ: {success_count}/{total_count} æˆåŠŸ")
        
        return results_dict
    
    def get_model_weight(self, model_name: str) -> float:
        """Get weight for a model in fusion."""
        return self.MODELS.get(model_name, {}).get("weight", 1.0)
    
    def get_available_models(self) -> List[str]:
        """Get list of models that have API keys configured."""
        available = []
        for model_name, config in self.MODELS.items():
            api_key = os.getenv(config["api_key_env"], "")
            if api_key:
                available.append(model_name)
        return available

