"""
Event Analyzer: å…¨é¢å‡çº§ç‰ˆ
åŒ…å«ï¼šå¸‚åœºè¶‹åŠ¿ã€äº‹ä»¶ç±»åˆ«ã€èˆ†æƒ…ä¿¡å·ã€è§„åˆ™æ‘˜è¦ã€ä¸–ç•Œæ¸©åº¦
æ”¯æŒç¼“å­˜å’Œé™æµæœºåˆ¶ä»¥èŠ‚çœAPIé¢åº¦
"""
import re
import json
import aiohttp
import asyncio
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from urllib.parse import quote_plus
import os
import sys
from dotenv import load_dotenv

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

load_dotenv()

# å¯¼å…¥ä¸–ç•Œæƒ…ç»ªå¼•æ“å’Œæ–°é—»æ‘˜è¦
try:
    from src.world_sentiment_engine import compute_world_temperature, get_world_temperature_summary
    from src.openrouter_assistant import get_news_summary
    WORLD_TEMP_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ ä¸–ç•Œæ¸©åº¦æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    WORLD_TEMP_AVAILABLE = False


class EventAnalyzer:
    """å…¨é¢å‡çº§çš„äº‹ä»¶åˆ†æå™¨ï¼ŒåŒ…å«å¸‚åœºè¶‹åŠ¿ã€äº‹ä»¶ç±»åˆ«ã€èˆ†æƒ…ä¿¡å·ã€è§„åˆ™æ‘˜è¦"""
    
    # Model specialization mapping
    MODEL_SPECIALIZATIONS = {
        "gpt-4o": {
            "name": "ç»¼åˆé€»è¾‘åˆ†æ",
            "dimensions": ["é€»è¾‘æ¨ç†", "ç»¼åˆåˆ†æ", "å¯èƒ½æ€§è¯„ä¼°"],
            "weight": 3.0
        },
        "claude-3-7-sonnet-latest": {
            "name": "é£é™©ä¸æ‰¹åˆ¤æ€§æ€ç»´",
            "dimensions": ["é£é™©è¯„ä¼°", "æ‰¹åˆ¤æ€§åˆ†æ", "é™·é˜±è¯†åˆ«"],
            "weight": 2.5
        },
        "claude-3-5-opus-latest": {
            "name": "é£é™©ä¸æ‰¹åˆ¤æ€§æ€ç»´ (Opus)",
            "dimensions": ["é£é™©è¯„ä¼°", "æ‰¹åˆ¤æ€§åˆ†æ", "é™·é˜±è¯†åˆ«"],
            "weight": 2.25
        },
        "deepseek-chat": {
            "name": "æ·±åº¦æ¨ç†ä¸é‡åŒ–åˆ†æ",
            "dimensions": ["æ·±åº¦æ¨ç†", "é‡åŒ–åˆ†æ", "æ•°å­¦å»ºæ¨¡", "é€»è¾‘é“¾åˆ†æ"],
            "weight": 2.0
        },
        "gemini-2.5-pro": {
            "name": "æ¨¡å¼è¯†åˆ«ä¸æ•°æ®",
            "dimensions": ["å†å²æ¨¡å¼", "æ•°æ®ç±»æ¯”", "è¶‹åŠ¿è¯†åˆ«"],
            "weight": 2.0
        },
        "gemini-2.5-flash": {
            "name": "æ¨¡å¼è¯†åˆ«ä¸æ•°æ® (Flash)",
            "dimensions": ["å†å²æ¨¡å¼", "æ•°æ®ç±»æ¯”", "è¶‹åŠ¿è¯†åˆ«"],
            "weight": 1.8
        },
        "grok-4": {
            "name": "å¦ç±»è§†è§’",
            "dimensions": ["å¸‚åœºæƒ…ç»ª", "å¦ç±»è§‚ç‚¹", "é»‘å¤©é¹…å› ç´ "],
            "weight": 2.0
        },
        "grok-3": {
            "name": "å¦ç±»è§†è§’ (v3)",
            "dimensions": ["å¸‚åœºæƒ…ç»ª", "å¦ç±»è§‚ç‚¹", "é»‘å¤©é¹…å› ç´ "],
            "weight": 1.8
        }
    }
    
    # Event category keywords mapping (ä¸­æ–‡ -> è‹±æ–‡)
    EVENT_CATEGORIES = {
        "geopolitics": {
            "keywords": ["election", "president", "russia", "china", "war", "conflict", "ceasefire", 
                        "ukraine", "taiwan", "israel", "palestine", "geopolitical", "jinping", "xi",
                        "trump", "biden", "leader", "government", "political", "power", "senate",
                        "parliament", "referendum", "coup", "military"],
            "display_name": "åœ°ç¼˜æ”¿æ²»"
        },
        "economy": {
            "keywords": ["gdp", "inflation", "rate", "fed", "unemployment", "economy", "market", 
                        "stock", "crypto", "bitcoin", "financial", "treasury", "recession",
                        "currency", "interest", "bond", "tariff"],
            "display_name": "ç»æµæŒ‡æ ‡"
        },
        "tech": {
            "keywords": ["apple", "google", "gpt", "ai", "gemini", "release", "launch", "product", 
                        "iphone", "app store", "technology", "nasa", "spacex", "rocket", "chip",
                        "semiconductor", "cloud", "quantum"],
            "display_name": "ç§‘æŠ€äº§å“"
        },
        "social": {
            "keywords": ["protest", "pandemic", "health", "disaster", "earthquake", "disease"],
            "display_name": "ç¤¾ä¼šäº‹ä»¶"
        },
        "sports": {
            "keywords": ["world cup", "olympics", "championship", "tournament", "nba", "nfl",
                        "fifa", "premier league", "grand slam", "formula 1", "super bowl"],
            "display_name": "ä½“è‚²èµ›äº‹"
        },
        "entertainment": {
            "keywords": ["movie", "film", "box office", "netflix", "disney", "hollywood",
                        "oscars", "grammys", "concert", "album", "series", "show"],
            "display_name": "æ–‡å¨±ä¼ åª’"
        },
        "science": {
            "keywords": ["space", "mission", "satellite", "telescope", "gene", "vaccine",
                        "research", "discovery", "experiment", "quantum"],
            "display_name": "ç§‘å­¦æ¢ç´¢"
        },
        "climate": {
            "keywords": ["climate", "emission", "carbon", "temperature", "warming", "hurricane",
                        "wildfire", "flood", "drought", "rainfall"],
            "display_name": "æ°”å€™ç¯å¢ƒ"
        }
    }
    
    # APIé…ç½®
    GDELT_URL = "https://api.gdeltproject.org/api/v2/doc/doc"
    NEWSAPI_URL = "https://newsapi.org/v2/everything"
    MEDIASTACK_URL = "http://api.mediastack.com/v1/news"
    
    NEWSAPI_KEY = "f085b39aba844082b0c4485ca5772467"
    MEDIASTACK_KEY = "1798203edaccb3399bdb738bf0cc10fe"
    
    # é™æµé…ç½®
    RATE_LIMIT_INTERVAL = 30  # ç§’
    NEWSAPI_HOURLY_LIMIT = 20
    MEDIASTACK_HOURLY_LIMIT = 20
    
    # ç¼“å­˜é…ç½®
    CACHE_DURATION_HOURS = 3
    CACHE_FILE = Path(__file__).parent.parent / "sentiment_cache.json"
    
    def __init__(self):
        """åˆå§‹åŒ–EventAnalyzerï¼ŒåŠ è½½ç¼“å­˜å’Œé™æµè®°å½•"""
        self.sentiment_cache = self._load_sentiment_cache()
        self.rate_limit_log = self._load_rate_limit_log()
        self.last_api_call = {}  # {api_name: timestamp}
        
    def _load_sentiment_cache(self) -> Dict:
        """åŠ è½½èˆ†æƒ…ç¼“å­˜"""
        try:
            if self.CACHE_FILE.exists():
                with open(self.CACHE_FILE, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            print(f"âš ï¸ åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
        return {}
    
    def _save_sentiment_cache(self):
        """ä¿å­˜èˆ†æƒ…ç¼“å­˜"""
        try:
            with open(self.CACHE_FILE, 'w', encoding='utf-8') as f:
                json.dump(self.sentiment_cache, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
    
    def _load_rate_limit_log(self) -> Dict:
        """åŠ è½½é™æµè®°å½•"""
        rate_file = Path(__file__).parent.parent / "rate_limit_log.json"
        try:
            if rate_file.exists():
                with open(rate_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception:
            pass
        return {
            "newsapi": {"calls": [], "hourly_count": 0, "reset_time": None},
            "mediastack": {"calls": [], "hourly_count": 0, "reset_time": None}
        }
    
    def _save_rate_limit_log(self):
        """ä¿å­˜é™æµè®°å½•"""
        rate_file = Path(__file__).parent.parent / "rate_limit_log.json"
        try:
            with open(rate_file, 'w', encoding='utf-8') as f:
                json.dump(self.rate_limit_log, f, indent=2)
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜é™æµè®°å½•å¤±è´¥: {e}")
    
    def _check_cache(self, keyword: str) -> Optional[Dict]:
        """æ£€æŸ¥ç¼“å­˜ï¼Œ3å°æ—¶å†…æœ‰æ•ˆ"""
        if keyword not in self.sentiment_cache:
            return None
        
        cached = self.sentiment_cache[keyword]
        timestamp = datetime.fromisoformat(cached["timestamp"])
        now = datetime.now()
        
        if (now - timestamp).total_seconds() < self.CACHE_DURATION_HOURS * 3600:
            print(f"âœ… ä½¿ç”¨ç¼“å­˜æ•°æ®ï¼ˆå…³é”®è¯ï¼š{keyword}ï¼‰")
            return cached
        
        # ç¼“å­˜è¿‡æœŸï¼Œåˆ é™¤
        del self.sentiment_cache[keyword]
        return None
    
    def _check_rate_limit(self, api_name: str) -> bool:
        """æ£€æŸ¥é™æµ"""
        if api_name not in self.rate_limit_log:
            return True
        
        log = self.rate_limit_log[api_name]
        now = datetime.now()
        
        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å°é—´éš”
        if api_name in self.last_api_call:
            elapsed = (now - self.last_api_call[api_name]).total_seconds()
            # ã€é˜²å¾¡ã€‘ç¡®ä¿ elapsed ä¸ä¸º None
            elapsed = elapsed or 0.0
            if elapsed is None:
                print("âš™ï¸ [SAFE] ä¿®å¤ç©ºå€¼ä¿æŠ¤: elapsed")
                elapsed = 0.0
            if elapsed < self.RATE_LIMIT_INTERVAL:
                print(f"â¸ï¸ {api_name} é™æµï¼šè·ç¦»ä¸Šæ¬¡è°ƒç”¨ä»… {(elapsed or 0.0):.1f} ç§’")
                return False
        
        # æ£€æŸ¥æ¯å°æ—¶è°ƒç”¨æ¬¡æ•°
        if log.get("reset_time"):
            reset_time = datetime.fromisoformat(log["reset_time"])
            if now > reset_time:
                # é‡ç½®è®¡æ•°å™¨
                log["calls"] = []
                log["hourly_count"] = 0
        
        hourly_limit = self.NEWSAPI_HOURLY_LIMIT if api_name == "newsapi" else self.MEDIASTACK_HOURLY_LIMIT
        
        if log.get("hourly_count", 0) >= hourly_limit:
            print(f"â¸ï¸ {api_name} é™æµï¼šå·²è¾¾åˆ°æ¯å°æ—¶ {hourly_limit} æ¬¡é™åˆ¶")
            return False
        
        return True
    
    def _update_rate_limit(self, api_name: str):
        """æ›´æ–°é™æµè®°å½•"""
        if api_name not in self.rate_limit_log:
            self.rate_limit_log[api_name] = {"calls": [], "hourly_count": 0, "reset_time": None}
        
        log = self.rate_limit_log[api_name]
        now = datetime.now()
        
        log["calls"].append(now.isoformat())
        log["hourly_count"] = len([c for c in log["calls"] 
                                  if (now - datetime.fromisoformat(c)).total_seconds() < 3600])
        
        # è®¾ç½®é‡ç½®æ—¶é—´ï¼ˆä¸‹ä¸€ä¸ªæ•´ç‚¹ï¼‰
        reset_time = (now + timedelta(hours=1)).replace(minute=0, second=0, microsecond=0)
        log["reset_time"] = reset_time.isoformat()
        
        self.last_api_call[api_name] = now
        self._save_rate_limit_log()
    
    async def analyze_event_full(
        self,
        event_title: str,
        event_rules: str = "",
        market_prob: Optional[float] = None,
        market_slug: Optional[str] = None
    ) -> Dict:
        """
        å…¨é¢åˆ†æäº‹ä»¶ï¼Œè¿”å›æ‰€æœ‰ä¿¡å·
        
        Returns:
            {
                "event_title": str,
                "event_category": str,  # geopolitics, economy, tech, social, sports, general
                "market_trend": str,    # "+12.4%" or "æ•°æ®ä¸è¶³"
                "sentiment_trend": str,  # "positive", "negative", "neutral", "unknown"
                "sentiment_score": float,
                "sentiment_sample": int,
                "sentiment_source": str,  # "GDELT", "NewsAPI", "Mediastack"
                "rules_summary": str,
                "current_market_prob": float
            }
        """
        event_lower = event_title.lower()
        
        # 1. äº‹ä»¶ç±»åˆ«
        event_category = self._detect_category(event_lower)
        
        # 2. å¸‚åœºè¶‹åŠ¿ï¼ˆå¼‚æ­¥ï¼Œå¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼‰
        market_trend = await self._get_market_trend(market_slug, market_prob) if market_slug else "æ•°æ®ä¸è¶³ï¼Œæ— æ³•è®¡ç®—"
        
        # 3. èˆ†æƒ…ä¿¡å·ï¼ˆå¼‚æ­¥ï¼Œå¸¦ç¼“å­˜ï¼‰
        # ä¼˜åŒ–ï¼šæ ¹æ®æ ·æœ¬æ•°é‡è°ƒæ•´èˆ†æƒ…ä¿¡å·çš„å½±å“åŠ›æƒé‡
        sentiment_data = await self._get_sentiment_signal(event_title)
        
        # æƒé‡å·²åœ¨ _get_sentiment_signal() ä¸­è®¡ç®—ï¼Œè¿™é‡Œä¸å†é‡å¤è®¡ç®—
        
        # 4. è§„åˆ™æ‘˜è¦
        rules_summary = self._extract_rules_summary(
            event_rules,
            market_prob=market_prob,
            sentiment_data=sentiment_data
        )
        
        # 5. ä¸–ç•Œæ¸©åº¦è®¡ç®—ï¼ˆæ–°å¢ï¼‰
        world_temp_data = None
        world_sentiment_summary = None
        if WORLD_TEMP_AVAILABLE:
            try:
                world_temp_data = compute_world_temperature()
                if world_temp_data:
                    world_sentiment_summary = get_world_temperature_summary(world_temp_data)
            except Exception as e:
                print(f"âš ï¸ è®¡ç®—ä¸–ç•Œæ¸©åº¦æ—¶å‡ºé”™: {type(e).__name__}: {e}")
                world_temp_data = None
                world_sentiment_summary = None
        
        result = {
            "event_title": event_title,
            "event_category": event_category,
            "market_trend": market_trend,
            "sentiment_trend": sentiment_data.get("sentiment", "unknown"),
            "sentiment_score": sentiment_data.get("score", 0.0),
            "sentiment_sample": sentiment_data.get("sample_count", 0),
            "sentiment_source": sentiment_data.get("source", "æœªçŸ¥"),
            "rules_summary": rules_summary,
            "current_market_prob": market_prob,
            # ã€è½»é‡æè¿°æ¨¡å¼ã€‘world_temp ç°åœ¨å­˜å‚¨æè¿°å­—ç¬¦ä¸²ï¼Œè€Œä¸æ˜¯æ•°å€¼
            "world_temp": world_temp_data.get("description") if world_temp_data else None,
            "world_temp_data": world_temp_data,  # å®Œæ•´æ•°æ®ï¼ˆåŒ…å« description, positive, negative, neutralï¼‰
            "world_sentiment_summary": world_sentiment_summary
        }
        
        return result
    
    def _detect_category(self, event_text: str) -> str:
        """
        ä½¿ç”¨åˆ†æ•°æœºåˆ¶å¯¹äº‹ä»¶è¿›è¡Œåˆ†ç±»ï¼Œå¯è¦†ç›–æ›´å¤šé¢†åŸŸï¼ˆæ”¿æ²»ã€ç»æµã€ç§‘æŠ€ã€ä½“è‚²ã€å¨±ä¹ç­‰ï¼‰
        """
        if not event_text:
            return "general"
        
        normalized = event_text.lower()
        tokens = set(re.findall(r"[a-z0-9']+", normalized))
        category_scores: Dict[str, int] = {}
        
        for category_id, cfg in self.EVENT_CATEGORIES.items():
            score = 0
            for keyword in cfg.get("keywords", []):
                keyword_lower = keyword.lower()
                if " " in keyword_lower:
                    if keyword_lower in normalized:
                        score += len(keyword_lower.split()) + 1
                else:
                    if keyword_lower in tokens:
                        score += 1
            category_scores[category_id] = score
        
        # é¢å¤–å¯å‘å¼ï¼šæ£€æµ‹ â€œvsâ€/â€œvs.â€/â€œ@â€ æ¨¡å¼ï¼Œå¤šç”¨äºä½“è‚²èµ›äº‹
        if re.search(r"\b(vs\.?|vs|@)\b", normalized):
            category_scores["sports"] = category_scores.get("sports", 0) + 2
        # é‡‘è/åŠ å¯†èµ„äº§çš„è¡¥å……åˆ¤æ–­
        if re.search(r"\b(bitcoin|ethereum|crypto|token)\b", normalized):
            category_scores["economy"] = category_scores.get("economy", 0) + 2
        # å¨±ä¹åœˆå¸¸è§è¯
        if re.search(r"\b(oscars?|grammys?|box office|streaming)\b", normalized):
            category_scores["entertainment"] = category_scores.get("entertainment", 0) + 2
        
        top_category = "general"
        top_score = 0
        for cat, score in category_scores.items():
            if score > top_score:
                top_category = cat
                top_score = score
        
        return top_category if top_score > 0 else "general"
    
    def _get_category_display_name(self, category_id: str) -> str:
        """è·å–ç±»åˆ«æ˜¾ç¤ºåç§°"""
        return self.EVENT_CATEGORIES.get(category_id, {}).get("display_name", "é€šç”¨äº‹ä»¶")
    
    async def _get_market_trend(self, market_slug: str, current_prob: Optional[float]) -> str:
        """
        è·å–å¸‚åœºè¶‹åŠ¿ï¼ˆè¿‡å»7å¤©ï¼‰
        å¿«é€Ÿå¤±è´¥æœºåˆ¶ï¼šè‹¥æ•°æ®ä¸è¶³æˆ–æ¥å£æ…¢ï¼Œç«‹å³è¿”å›ï¼Œä¸é˜»å¡ä¸»æµç¨‹
        """
        if not market_slug or current_prob is None:
            return "æ–°å¸‚åœºï¼Œæ•°æ®ä¸è¶³"
        
        try:
            # è®¾ç½®å¿«é€Ÿè¶…æ—¶ï¼ˆ5ç§’ï¼‰ï¼Œé¿å…é•¿æ—¶é—´ç­‰å¾…å†å²æ•°æ®
            TREND_TIMEOUT = 5
            
            # å°è¯•ä»Polymarket APIè·å–å†å²æ•°æ®
            # æ³¨æ„ï¼šPolymarket APIå¯èƒ½ä¸ç›´æ¥æä¾›å†å²ä»·æ ¼ï¼Œè¿™é‡Œä½¿ç”¨ç®€åŒ–é€»è¾‘
            # å®é™…å®ç°å¯èƒ½éœ€è¦é€šè¿‡CLOB APIæˆ–å…¶ä»–æ•°æ®æº
            
            # æš‚æ—¶è¿”å›å ä½ç¬¦ï¼Œå®é™…éœ€è¦å®ç°å†å²æ•°æ®è·å–é€»è¾‘
            # TODO: å®ç°çœŸå®çš„å†å²ä»·æ ¼è·å–
            
            # å¿«é€Ÿæ£€æŸ¥ï¼šå¦‚æœæ˜¯æ–°å¸‚åœºï¼ˆslugåŒ…å«ç‰¹å®šæ ‡è¯†ï¼‰ï¼Œç«‹å³è¿”å›
            # è¿™é‡Œå¯ä»¥æ·»åŠ å®é™…çš„å†å²æ•°æ®è·å–é€»è¾‘ï¼Œä½†å¿…é¡»æœ‰è¶…æ—¶ä¿æŠ¤
            await asyncio.sleep(0.1)  # å ä½ç¬¦ï¼Œå®é™…åº”è¯¥æ˜¯APIè°ƒç”¨
            
            # æ¨¡æ‹Ÿï¼šå‡è®¾æˆ‘ä»¬æ— æ³•è·å–çœŸå®å†å²æ•°æ®
            return "æ–°å¸‚åœºï¼Œæ•°æ®ä¸è¶³"
            
        except asyncio.TimeoutError:
            print(f"â±ï¸ [WARNING] å¸‚åœºè¶‹åŠ¿æ•°æ®è·å–è¶…æ—¶ï¼Œè·³è¿‡")
            return "æ–°å¸‚åœºï¼Œæ•°æ®ä¸è¶³"
        except Exception as e:
            print(f"âš ï¸ [WARNING] è·å–å¸‚åœºè¶‹åŠ¿å¤±è´¥: {type(e).__name__}: {e}")
            return "æ–°å¸‚åœºï¼Œæ•°æ®ä¸è¶³"
    
    async def _get_sentiment_signal(self, event_title: str) -> Dict:
        """
        è·å–èˆ†æƒ…ä¿¡å·ï¼Œä¼˜å…ˆçº§ï¼šGDELT â†’ NewsAPI â†’ Mediastack
        æ·»åŠ å¿«é€Ÿå¤±è´¥æœºåˆ¶ï¼Œé¿å…é•¿æ—¶é—´ç­‰å¾…
        """
        # æå–å…³é”®è¯ï¼ˆç®€åŒ–ï¼šä½¿ç”¨äº‹ä»¶æ ‡é¢˜çš„ä¸»è¦éƒ¨åˆ†ï¼‰
        keywords = self._extract_keywords(event_title)
        keyword_str = " ".join(keywords[:3])  # ä½¿ç”¨å‰3ä¸ªå…³é”®è¯
        
        # æ£€æŸ¥ç¼“å­˜ï¼ˆä¼˜å…ˆä½¿ç”¨ç¼“å­˜ï¼Œé¿å…APIè°ƒç”¨ï¼‰
        cached = self._check_cache(keyword_str)
        if cached:
            print(f"ğŸ“Š ä½¿ç”¨ç¼“å­˜çš„èˆ†æƒ…æ•°æ®: {cached['source']}")
            sentiment_data = {
                "sentiment": cached["sentiment"],
                "score": cached["score"],
                "sample_count": cached.get("sample_count", 0),
                "source": cached["source"]
            }
            # æ ¹æ®æ ·æœ¬æ•°é‡è°ƒæ•´æƒé‡å’Œè®¡ç®—è°ƒæ•´åçš„è¶‹åŠ¿
            sample_count = sentiment_data.get("sample_count", 0)
            if sample_count < 30:
                sentiment_data["weight"] = 0.2
            elif sample_count < 100:
                sentiment_data["weight"] = 0.6
            else:
                sentiment_data["weight"] = 1.0
            
            sentiment_data["adjusted_trend"] = sentiment_data["score"] * sentiment_data["weight"]
            return sentiment_data
        
        # è®¾ç½®æ¯ä¸ªAPIè°ƒç”¨çš„è¶…æ—¶æ—¶é—´ï¼ˆå¿«é€Ÿå¤±è´¥ï¼‰
        API_TIMEOUT = 8  # æ¯ä¸ªAPIæœ€å¤šç­‰å¾…8ç§’
        
        # æŒ‰ä¼˜å…ˆçº§å°è¯•å„ä¸ªAPIï¼ˆå¿«é€Ÿå¤±è´¥æœºåˆ¶ï¼‰
        async def run_source(source_name: str, fetcher):
            try:
                data = await asyncio.wait_for(fetcher(), timeout=API_TIMEOUT)
                return source_name, data
            except asyncio.TimeoutError:
                print(f"â±ï¸ [WARNING] {source_name.upper()} API è¶…æ—¶ï¼ˆ>{API_TIMEOUT}sï¼‰ï¼Œè·³è¿‡")
                return source_name, None
            except Exception as e:
                print(f"âš ï¸ [WARNING] {source_name.upper()} API å¤±è´¥: {type(e).__name__}: {e}")
                return source_name, None
        
        source_results = {}
        tasks = []
        # Always attempt GDELT
        tasks.append(run_source("gdelt", lambda: self._fetch_gdelt_sentiment(keyword_str)))
        
        # Conditionally schedule NewsAPI / Mediastack based on rate limits
        newsapi_allowed = self._check_rate_limit("newsapi")
        mediastack_allowed = self._check_rate_limit("mediastack")
        if newsapi_allowed:
            tasks.append(run_source("newsapi", lambda: self._fetch_newsapi_sentiment(keyword_str)))
        else:
            source_results["newsapi"] = None
        if mediastack_allowed:
            tasks.append(run_source("mediastack", lambda: self._fetch_mediastack_sentiment(keyword_str)))
        else:
            source_results["mediastack"] = None
        
        fetched = await asyncio.gather(*tasks, return_exceptions=True)
        for entry in fetched:
            if isinstance(entry, Exception):
                print(f"âš ï¸ [WARNING] èˆ†æƒ…ä»»åŠ¡å¼‚å¸¸: {entry}")
                continue
            source_name, data = entry
            source_results[source_name] = data
        
        def prepare_result(data: Optional[Dict]) -> Optional[Dict]:
            if not data:
                return None
            sample_count = data.get("sample_count", 0)
            if sample_count < 30:
                data["weight"] = 0.2
            elif sample_count < 100:
                data["weight"] = 0.6
            else:
                data["weight"] = 1.0
            data["adjusted_trend"] = data.get("score", 0.0) * data["weight"]
            return data
        
        # Priority order
        if source_results.get("gdelt") and source_results["gdelt"].get("sample_count", 0) >= 5:
            prepared = prepare_result(source_results["gdelt"])
            if prepared:
                self._save_to_cache(keyword_str, prepared)
                return prepared
        
        if newsapi_allowed and source_results.get("newsapi"):
            prepared = prepare_result(source_results["newsapi"])
            if prepared:
                self._update_rate_limit("newsapi")
                self._save_to_cache(keyword_str, prepared)
                return prepared
        
        if mediastack_allowed and source_results.get("mediastack"):
            prepared = prepare_result(source_results["mediastack"])
            if prepared:
                self._update_rate_limit("mediastack")
                self._save_to_cache(keyword_str, prepared)
                return prepared
        
        # æ‰€æœ‰APIéƒ½å¤±è´¥æˆ–è¶…æ—¶ï¼Œè¿”å›é»˜è®¤å€¼ï¼ˆä¸é˜»å¡æµç¨‹ï¼‰
        print(f"âš ï¸ æ‰€æœ‰èˆ†æƒ…APIè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼")
        sentiment_data = {
            "sentiment": "unknown",
            "score": 0.0,
            "sample_count": 0,
            "source": "æœªçŸ¥"
        }
        
        # æ ¹æ®æ ·æœ¬æ•°é‡è°ƒæ•´æƒé‡å’Œè®¡ç®—è°ƒæ•´åçš„è¶‹åŠ¿
        sample_count = sentiment_data.get("sample_count", 0)
        if sample_count < 30:
            sentiment_data["weight"] = 0.2
        elif sample_count < 100:
            sentiment_data["weight"] = 0.6
        else:
            sentiment_data["weight"] = 1.0
        
        sentiment_data["adjusted_trend"] = sentiment_data["score"] * sentiment_data["weight"]
        
        return sentiment_data
    
    def _extract_keywords(self, text: str) -> List[str]:
        """ä»äº‹ä»¶æ ‡é¢˜æå–å…³é”®è¯"""
        # ç§»é™¤æ ‡ç‚¹ï¼Œè½¬ä¸ºå°å†™ï¼Œåˆ†è¯
        clean_text = re.sub(r'[^\w\s]', ' ', text.lower())
        words = clean_text.split()
        
        # ç§»é™¤å¸¸è§åœç”¨è¯
        stopwords = {'will', 'be', 'in', 'by', 'the', 'a', 'an', 'and', 'or', 'to', 'of', 'for'}
        keywords = [w for w in words if len(w) > 2 and w not in stopwords]
        
        return keywords[:5]  # è¿”å›å‰5ä¸ªå…³é”®è¯
    
    async def _fetch_json_with_retry(
        self,
        url: str,
        source_name: str,
        timeout: int = 10,
        attempts: int = 2,
        headers: Optional[Dict[str, str]] = None
    ) -> Optional[Dict]:
        """é€šç”¨çš„å¸¦é‡è¯• JSON è¯·æ±‚"""
        last_error: Optional[Exception] = None
        for attempt in range(1, attempts + 1):
            try:
                timeout_cfg = aiohttp.ClientTimeout(total=timeout)
                async with aiohttp.ClientSession(timeout=timeout_cfg) as session:
                    async with session.get(url, headers=headers) as response:
                        if response.status == 200:
                            return await response.json()
                        last_error = RuntimeError(f"status {response.status}")
                        print(f"âš ï¸ [{source_name}] HTTP {response.status} (attempt {attempt}/{attempts})")
            except Exception as exc:
                last_error = exc
                print(f"âš ï¸ [{source_name}] è¯·æ±‚å¼‚å¸¸ (attempt {attempt}/{attempts}): {type(exc).__name__}: {exc}")
            if attempt < attempts:
                await asyncio.sleep(min(2, attempt * 1.5))
        if last_error:
            print(f"âŒ [{source_name}] è¿ç»­å¤±è´¥ï¼ˆ{attempts} æ¬¡ï¼‰ï¼š{last_error}")
        return None
    
    async def _fetch_gdelt_sentiment(self, keyword: str) -> Optional[Dict]:
        """ä»GDELTè·å–èˆ†æƒ…"""
        url = f"{self.GDELT_URL}?query={quote_plus(keyword)}&mode=ArtList&format=json&maxrecords=20"
        data = await self._fetch_json_with_retry(url, "GDELT", timeout=10, attempts=3)
        if not data:
            return None
        
        articles = data.get("articles", [])
        if len(articles) >= 5:
            score = 0.0  # é»˜è®¤ä¸­æ€§
            sentiment = "neutral"
            return {
                "sentiment": sentiment,
                "score": score,
                "sample_count": len(articles),
                "source": "GDELT"
            }
        
        return None
    
    async def _fetch_newsapi_sentiment(self, keyword: str) -> Optional[Dict]:
        """ä»NewsAPIè·å–èˆ†æƒ…"""
        url = f"{self.NEWSAPI_URL}?q={quote_plus(keyword)}&language=en&sortBy=publishedAt&apiKey={self.NEWSAPI_KEY}"
        data = await self._fetch_json_with_retry(url, "NewsAPI", timeout=10, attempts=3)
        if not data:
            return None
        
        articles = data.get("articles", [])
        if articles:
            score = 0.05  # ç¤ºä¾‹ï¼šè½»å¾®æ­£é¢
            sentiment = "neutral"
            return {
                "sentiment": sentiment,
                "score": score,
                "sample_count": len(articles),
                "source": "NewsAPI"
            }
        
        return None
    
    async def _fetch_mediastack_sentiment(self, keyword: str) -> Optional[Dict]:
        """ä»Mediastackè·å–èˆ†æƒ…"""
        url = f"{self.MEDIASTACK_URL}?access_key={self.MEDIASTACK_KEY}&keywords={keyword}&languages=en"
        data = await self._fetch_json_with_retry(url, "Mediastack", timeout=10, attempts=3)
        if not data:
            return None
        
        articles = data.get("data", [])
        if articles:
            score = -0.1  # ç¤ºä¾‹ï¼šè½»å¾®è´Ÿé¢
            sentiment = "neutral"
            return {
                "sentiment": sentiment,
                "score": score,
                "sample_count": len(articles),
                "source": "Mediastack"
            }
        
        return None
    
    def _save_to_cache(self, keyword: str, result: Dict):
        """ä¿å­˜ç»“æœåˆ°ç¼“å­˜"""
        self.sentiment_cache[keyword] = {
            "keyword": keyword,
            "sentiment": result["sentiment"],
            "score": result["score"],
            "sample_count": result.get("sample_count", 0),
            "source": result["source"],
            "timestamp": datetime.now().isoformat()
        }
        self._save_sentiment_cache()
    
    def _build_market_sentiment_hint(
        self,
        market_prob: Optional[float],
        sentiment_data: Optional[Dict[str, Any]]
    ) -> str:
        """æ ¹æ®å¸‚åœºæ¦‚ç‡å’Œèˆ†æƒ…ä¿¡å·ç”Ÿæˆé™„åŠ æç¤º"""
        hints: List[str] = []
        if isinstance(market_prob, (int, float)):
            distance = abs(50 - market_prob)
            if distance < 12:
                hints.append("âš ï¸ å¸‚åœºä»·æ ¼æ¥è¿‘ 50%ï¼Œå¤šç©ºåˆ†æ­§è¾ƒå¤§ï¼Œéœ€è¦ä¿æŒå®¡æ…ã€‚")
            elif market_prob < 20 or market_prob > 80:
                hints.append("â„¹ï¸ å¸‚åœºä»·æ ¼æç«¯ï¼Œå¯èƒ½å­˜åœ¨ç¾¤ä½“æ€§åå¥½æˆ–æ³¢åŠ¨æ”¾å¤§æ•ˆåº”ã€‚")
        if sentiment_data:
            score = sentiment_data.get("score", 0.0) or 0.0
            sample = sentiment_data.get("sample_count", 0) or 0
            if sample < 20:
                hints.append("â„¹ï¸ èˆ†æƒ…æ ·æœ¬æœ‰é™ï¼ˆ<20ï¼‰ï¼Œä¿¡å·ä¸ç¨³å®šã€‚")
            else:
                if score <= -0.2:
                    hints.append("ğŸ“‰ èˆ†æƒ…æ˜¾è‘—åè´Ÿé¢ï¼Œåº”é€‚åº¦é™ä½æ¨¡å‹ä¹è§‚åº¦ã€‚")
                elif score >= 0.2:
                    hints.append("ğŸ“ˆ èˆ†æƒ…åæ­£é¢ï¼Œå¯å‚è€ƒä½†ä»éœ€éªŒè¯åŸºæœ¬é¢ã€‚")
        return "\n".join(hints)
    
    def _extract_rules_summary(
        self,
        rules: str,
        market_prob: Optional[float] = None,
        sentiment_data: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        æå–è§„åˆ™æ‘˜è¦ï¼Œä¼˜åŒ–ç‰ˆæœ¬ï¼šæå–å®Œæ•´å¥å­
        """
        if not rules:
            return "âš ï¸ æœªæ‰¾åˆ°å¸‚åœºè§„åˆ™"
        
        rules = re.sub(r'\s+', ' ', rules.strip())
        
        match = re.search(r'([A-Z][^.?!]*[.?!])', rules)
        if match:
            summary = match.group(1)
        else:
            summary = rules[:180] + ("..." if len(rules) > 180 else "")
        
        sentiment_hint = self._build_market_sentiment_hint(market_prob, sentiment_data)
        if sentiment_hint:
            summary = f"{summary}\n{sentiment_hint}"
        return summary
    
    # ä¿ç•™åŸæœ‰çš„ analyze_event æ–¹æ³•ä»¥ä¿æŒå…¼å®¹æ€§
    def analyze_event(self, event_title: str, event_rules: str = "", available_models: List[str] = None, orchestrator=None) -> Dict:
        """
        åŸæœ‰çš„åˆ†ææ–¹æ³•ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
        
        Args:
            event_title: Event title to analyze
            event_rules: Event rules/description (optional)
            available_models: Optional list of available model IDs (from orchestrator, already filtered)
            orchestrator: Optional ModelOrchestrator instance to auto-fetch enabled models
        
        Returns:
            Dict with model assignments and specialized prompts
        """
        # Auto-fetch available models from orchestrator if not provided
        if available_models is None:
            if orchestrator is not None and hasattr(orchestrator, 'MODELS'):
                available_models = list(orchestrator.MODELS.keys())
                print(f"[DEBUG] Auto-fetched {len(available_models)} models from orchestrator: {available_models}")
            else:
                # Default fallback list (commonly enabled models)
                available_models = ["gpt-4o", "claude-3-7-sonnet-latest", "gemini-2.5-pro", "deepseek-chat"]
                print(f"[DEBUG] Using default model list (orchestrator not available): {available_models}")
        
        event_lower = event_title.lower()
        event_category = self._detect_category(event_lower)
        dimensions = self._get_dimensions_for_category(event_category, event_lower)
        # Filter by available_models to skip disabled models (e.g., Grok)
        model_assignments = self._assign_models_to_dimensions(dimensions, available_models=available_models)
        
        return {
            "category": self._get_category_display_name(event_category),
            "category_id": event_category,
            "dimensions": dimensions,
            "model_assignments": model_assignments
        }
    
    def _get_dimensions_for_category(self, category: str, event_text: str) -> List[str]:
        """Get relevant dimensions based on event category."""
        # æ˜ å°„è‹±æ–‡ç±»åˆ«åˆ°ä¸­æ–‡
        category_map = {
            "geopolitics": "åœ°ç¼˜æ”¿æ²»",
            "economy": "ç»æµæŒ‡æ ‡",
            "tech": "ç§‘æŠ€äº§å“",
            "social": "ç¤¾ä¼šäº‹ä»¶",
            "sports": "ä½“è‚²èµ›äº‹",
            "entertainment": "æ–‡å¨±ä¼ åª’",
            "science": "ç§‘å­¦æ¢ç´¢",
            "climate": "æ°”å€™ç¯å¢ƒ",
            "general": "é€šç”¨äº‹ä»¶"
        }
        chinese_category = category_map.get(category, "é€šç”¨äº‹ä»¶")
        
        dimension_map = {
            "åœ°ç¼˜æ”¿æ²»": [
                {
                    "name": "æ”¿æ²»å› ç´ åˆ†æ",
                    "description": "åˆ†æç›¸å…³çš„æ”¿æ²»å› ç´ ï¼ŒåŒ…æ‹¬æ”¿ç­–å˜åŒ–ã€é¢†å¯¼äººå†³ç­–ã€å›½é™…å…³ç³»ç­‰",
                    "model": "gpt-4o"
                },
                {
                    "name": "é£é™©è¯„ä¼°",
                    "description": "è¯„ä¼°å†²çªå‡çº§ã€æ„å¤–äº‹ä»¶ã€é»‘å¤©é¹…äº‹ä»¶çš„é£é™©",
                    "model": "claude-3-7-sonnet-latest"
                },
                {
                    "name": "æ·±åº¦æ¨ç†ä¸é€»è¾‘é“¾åˆ†æ",
                    "description": "é€šè¿‡æ·±åº¦æ¨ç†å’Œé€»è¾‘é“¾åˆ†æï¼Œé‡åŒ–è¯„ä¼°æ”¿æ²»äº‹ä»¶çš„å¯èƒ½æ€§",
                    "model": "deepseek-chat"
                },
                {
                    "name": "å†å²æ¨¡å¼å¯¹æ¯”",
                    "description": "å¯¹æ¯”ç±»ä¼¼å†å²äº‹ä»¶çš„ç»“æœæ¨¡å¼ï¼Œå¯»æ‰¾å¯ç±»æ¯”çš„æƒ…å†µ",
                    "model": "gemini-2.5-pro"
                },
                {
                    "name": "å¸‚åœºæƒ…ç»ªä¸å¦ç±»è§†è§’",
                    "description": "åˆ†æå¸‚åœºæƒ…ç»ªã€èˆ†è®ºèµ°å‘ã€ä»¥åŠå¯èƒ½è¢«å¿½è§†çš„å¦ç±»å› ç´ ",
                    "model": "grok-4"
                }
            ],
            "ç§‘æŠ€äº§å“": [
                {
                    "name": "æŠ€æœ¯å¯è¡Œæ€§åˆ†æ",
                    "description": "åˆ†ææŠ€æœ¯å®ç°çš„å¯è¡Œæ€§ã€æ—¶é—´çº¿ã€æŠ€æœ¯éšœç¢ç­‰",
                    "model": "gpt-4o"
                },
                {
                    "name": "é‡åŒ–åˆ†æä¸æ•°å­¦å»ºæ¨¡",
                    "description": "é€šè¿‡é‡åŒ–åˆ†æå’Œæ•°å­¦å»ºæ¨¡ï¼Œç²¾ç¡®è¯„ä¼°æŠ€æœ¯å®ç°çš„æ—¶é—´æ¦‚ç‡å’Œå¸‚åœºå½±å“",
                    "model": "deepseek-chat"
                },
                {
                    "name": "å¸‚åœºååº”é¢„æµ‹",
                    "description": "é¢„æµ‹äº§å“å‘å¸ƒåçš„å¸‚åœºååº”ã€ç”¨æˆ·æ¥å—åº¦ã€ç«äº‰å½±å“",
                    "model": "grok-4"
                },
                {
                    "name": "å†å²å‘å¸ƒæ¨¡å¼",
                    "description": "å¯¹æ¯”ç±»ä¼¼äº§å“çš„å†å²å‘å¸ƒæ¨¡å¼ã€å»¶è¿ŸåŸå› ã€æˆåŠŸå› ç´ ",
                    "model": "gemini-2.5-pro"
                },
                {
                    "name": "é£é™©è¯„ä¼°",
                    "description": "è¯†åˆ«å¯èƒ½çš„æŠ€æœ¯é£é™©ã€å¸‚åœºé£é™©ã€ç›‘ç®¡é£é™©",
                    "model": "claude-3-7-sonnet-latest"
                }
            ],
            "ç»æµæŒ‡æ ‡": [
                {
                    "name": "å®è§‚ç»æµåˆ†æ",
                    "description": "åˆ†æå®è§‚ç»æµå› ç´ ã€æ”¿ç­–å½±å“ã€å¸‚åœºç¯å¢ƒ",
                    "model": "gpt-4o"
                },
                {
                    "name": "é‡åŒ–åˆ†æä¸æ¦‚ç‡å»ºæ¨¡",
                    "description": "é€šè¿‡é‡åŒ–åˆ†æå’Œæ¦‚ç‡å»ºæ¨¡ï¼Œç²¾ç¡®è®¡ç®—ç»æµæŒ‡æ ‡çš„å¯èƒ½æ€§åˆ†å¸ƒ",
                    "model": "deepseek-chat"
                },
                {
                    "name": "é£é™©å› å­è¯†åˆ«",
                    "description": "è¯†åˆ«ç»æµä¸‹è¡Œé£é™©ã€æ„å¤–å› ç´ ã€å¸‚åœºæ³¢åŠ¨é£é™©",
                    "model": "claude-3-7-sonnet-latest"
                },
                {
                    "name": "å†å²æ•°æ®æ¨¡å¼",
                    "description": "åˆ†æå†å²æ•°æ®è¶‹åŠ¿ã€å‘¨æœŸæ€§æ¨¡å¼ã€å­£èŠ‚æ€§å› ç´ ",
                    "model": "gemini-2.5-pro"
                },
                {
                    "name": "å¸‚åœºæƒ…ç»ªåˆ†æ",
                    "description": "åˆ†æå¸‚åœºé¢„æœŸã€æŠ•èµ„è€…æƒ…ç»ªã€æƒ…ç»ªé©±åŠ¨çš„æ³¢åŠ¨",
                    "model": "grok-4"
                }
            ],
            "æ–‡å¨±ä¼ åª’": [
                {
                    "name": "è§‚ä¼—å£ç¢‘ç›‘æµ‹",
                    "description": "è¿½è¸ªåª’ä½“è¯„ä»·ã€ç¤¾äº¤åª’ä½“çƒ­åº¦ä»¥åŠç²‰ä¸æƒ…ç»ªæ³¢åŠ¨",
                    "model": "grok-4"
                },
                {
                    "name": "ç¥¨æˆ¿ä¸æ”¶è§†é¢„æµ‹",
                    "description": "ç»“åˆå†å²è¡¨ç°å’Œå‘è¡ŒèŠ‚å¥é¢„æµ‹ç¥¨æˆ¿/æ”¶è§†è¡¨ç°",
                    "model": "deepseek-chat"
                },
                {
                    "name": "å®£å‘åŠç›‘ç®¡é£é™©",
                    "description": "è¯†åˆ«è‰ºäººèˆ†æƒ…ã€ç›‘ç®¡å®¡æŸ¥ã€å®£å‘æŠ•å…¥ç­‰é£é™©",
                    "model": "claude-3-7-sonnet-latest"
                }
            ],
            "ç§‘å­¦æ¢ç´¢": [
                {
                    "name": "æŠ€æœ¯æˆç†Ÿåº¦è¯„ä¼°",
                    "description": "è¯„ä¼°ç§‘ç ”é¡¹ç›®æˆ–ä»»åŠ¡çš„æŠ€æœ¯æˆç†Ÿåº¦ä¸å®éªŒé‡Œç¨‹ç¢‘",
                    "model": "gpt-4o"
                },
                {
                    "name": "å†å²ä»»åŠ¡æ¯”å¯¹",
                    "description": "å¯¹æ¯”ç±»ä¼¼ç§‘ç ”ä»»åŠ¡çš„æˆåŠŸç‡å’Œå¤±è´¥åŸå› ",
                    "model": "gemini-2.5-pro"
                },
                {
                    "name": "èµ„é‡‘/æ”¿ç­–é£é™©",
                    "description": "è¯†åˆ«èµ„é‡‘ä¸­æ–­ã€æ”¿ç­–å®¡æŸ¥ã€ä¾›åº”é“¾é—®é¢˜ç­‰é£é™©",
                    "model": "claude-3-7-sonnet-latest"
                }
            ],
            "æ°”å€™ç¯å¢ƒ": [
                {
                    "name": "æ°”å€™æ•°æ®åˆ†æ",
                    "description": "åˆ†ææ¸©åº¦ã€é™æ°´ã€é£æš´ç­‰å†å²/é¢„æµ‹æ•°æ®",
                    "model": "deepseek-chat"
                },
                {
                    "name": "æ”¿ç­–ä¸ç›‘ç®¡è§£æ",
                    "description": "è¯„ä¼°ç¢³ç¨ã€æ’æ”¾é™é¢ã€å›½é™…åå®šç­‰æ”¿ç­–å˜é‡",
                    "model": "gpt-4o"
                },
                {
                    "name": "æç«¯äº‹ä»¶é£é™©",
                    "description": "è¯†åˆ«æç«¯å¤©æ°”ã€è‡ªç„¶ç¾å®³å¯¹äº‹ä»¶ç»“æœçš„å½±å“",
                    "model": "grok-4"
                }
            ],
            "é€šç”¨äº‹ä»¶": [
                {
                    "name": "ç»¼åˆåˆ†æ",
                    "description": "å…¨é¢åˆ†æäº‹ä»¶çš„å„ç§å¯èƒ½å› ç´ å’Œé€»è¾‘æ¨ç†",
                    "model": "gpt-4o"
                },
                {
                    "name": "æ·±åº¦æ¨ç†ä¸é‡åŒ–åˆ†æ",
                    "description": "é€šè¿‡æ·±åº¦æ¨ç†å’Œé‡åŒ–åˆ†æï¼Œç²¾ç¡®è¯„ä¼°äº‹ä»¶å‘ç”Ÿçš„å¯èƒ½æ€§",
                    "model": "deepseek-chat"
                },
                {
                    "name": "é£é™©è¯„ä¼°",
                    "description": "è¯„ä¼°äº‹ä»¶å¯èƒ½çš„é£é™©å’Œä¸ç¡®å®šæ€§",
                    "model": "claude-3-7-sonnet-latest"
                },
                {
                    "name": "æ¨¡å¼è¯†åˆ«",
                    "description": "è¯†åˆ«ç±»ä¼¼å†å²äº‹ä»¶å’Œæ¨¡å¼",
                    "model": "gemini-2.5-pro"
                },
                {
                    "name": "å¦ç±»è§†è§’",
                    "description": "æä¾›å¦ç±»è§†è§’å’Œå¯èƒ½è¢«å¿½è§†çš„å› ç´ ",
                    "model": "grok-4"
                }
            ]
        }
        
        return dimension_map.get(chinese_category, dimension_map["é€šç”¨äº‹ä»¶"])
    
    def _assign_models_to_dimensions(self, dimensions: List[Dict], available_models: List[str] = None) -> Dict[str, Dict]:
        """
        Assign models to dimensions.
        
        Args:
            dimensions: List of dimension dicts with 'model' key
            available_models: Optional list of available model IDs (if None, assign all)
        
        Returns:
            Dict mapping model_id -> assignment dict
        """
        assignments = {}
        
        for dim in dimensions:
            model_name = dim["model"]
            # Skip if model is not available (disabled or not in orchestrator)
            if available_models and model_name not in available_models:
                continue
            # Skip if model is not in specializations (e.g., fallback models)
            if model_name not in self.MODEL_SPECIALIZATIONS:
                continue
                
            if model_name not in assignments:
                assignments[model_name] = {
                    "dimension_name": dim["name"],
                    "dimension_description": dim["description"],
                    "weight": self.MODEL_SPECIALIZATIONS[model_name]["weight"],
                    "specialization": self.MODEL_SPECIALIZATIONS[model_name]["name"]
                }
        
        return assignments
