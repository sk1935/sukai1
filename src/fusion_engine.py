"""
èåˆå±‚ï¼ˆFusion Engineï¼‰ï¼š
æ ¹æ® OPTIMIZATION_NOTES.md çš„äº”å±‚æ¶æ„è®¾è®¡

èŒè´£ï¼š
- å¯¹ä¸åŒæ¨¡å‹çš„é¢„æµ‹è¿›è¡ŒåŠ æƒèåˆ
- è®¡ç®— AI å…±è¯†æ¦‚ç‡ä¸å¸‚åœºæ¦‚ç‡çš„èåˆç»“æœï¼ˆ80% AI + 20% å¸‚åœºï¼‰
- ç”Ÿæˆç®€çŸ­çš„"AIå…±è¯†è§‚ç‚¹æ€»ç»“"ï¼ˆä¸­æ–‡ï¼‰
- è¯„ä¼°æ¨¡å‹é—´çš„åˆ†æ­§ç¨‹åº¦
- ä» LMArena.ai åŸºç¡€æƒé‡é…ç½®æ–‡ä»¶åŠ è½½æ¨¡å‹æƒé‡

è¾“å…¥ï¼šå„æ¨¡å‹çš„é¢„æµ‹ç»“æœ {probability, confidence, reasoning}
è¾“å‡ºï¼šèåˆåçš„é¢„æµ‹ {final_prob, model_only_prob, uncertainty, summary, disagreement}
"""
import json
import math
import time
import re
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union
from datetime import datetime, timezone


class FusionEngine:
    """
    Fuses multiple model predictions with weighted averaging.
    
    èåˆå¤šä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœï¼š
    - ä½¿ç”¨æ¨¡å‹æƒé‡å’Œç½®ä¿¡åº¦æƒé‡è®¡ç®—åŠ æƒå¹³å‡
    - å°†AIå…±è¯†ï¼ˆ80%ï¼‰ä¸å¸‚åœºæ¦‚ç‡ï¼ˆ20%ï¼‰èåˆ
    - ç”Ÿæˆä¸­æ–‡æ‘˜è¦
    - è¯„ä¼°åˆ†æ­§ç¨‹åº¦
    - ä» LMArena.ai é…ç½®æ–‡ä»¶åŠ è½½åŸºç¡€æƒé‡
    """
    
    CONFIDENCE_WEIGHTS = {
        "low": 1.0,
        "medium": 2.0,
        "high": 3.0
    }
    
    MARKET_WEIGHT = 0.2  # Weight for market probability (é™ä½å¸‚åœºæƒé‡)
    MODEL_WEIGHT = 0.8   # Weight for model consensus (æé«˜æ¨¡å‹æƒé‡)
    
    # Model name mapping: actual model_id -> LMArena config key
    MODEL_NAME_MAPPING = {
        "gpt-4o": "gpt-4o-latest",
        "claude-3-7-sonnet-latest": "claude-opus-4-1",  # Map to closest match
        "claude-3-5-opus-latest": "claude-opus-4-1",
        "gemini-2.5-pro": "gemini-2.5-pro",
        "gemini-2.5-flash": "gemini-2.5-pro",  # Fallback to pro if flash not found
        "grok-4": "grok-4-fast",
        "grok-3": "grok-4-fast",  # Fallback
        "deepseek-chat": "deepseek-v3.2-exp"
    }
    
    def __init__(self, experiment_config=None):
        """
        Initialize FusionEngine and load LMArena base weights.
        
        Args:
            experiment_config: Optional ExperimentConfig instance for ablation studies
        """
        self.lmarena_config = self._load_lmarena_config()
        self.base_weights = self.lmarena_config.get("weights", {})
        self.metadata = self.lmarena_config.get("metadata", {})
        self.fusion_config = self.lmarena_config.get("fusion", {})
        
        # Experiment configuration (for ablation studies)
        self.experiment_config = experiment_config
        
        # Update confidence weights from config if available
        if "default_confidence_multiplier" in self.fusion_config:
            self.CONFIDENCE_WEIGHTS.update(self.fusion_config["default_confidence_multiplier"])
        
        # Log weights on initialization
        self._log_base_weights()
        
        # Calibration state (for binning/platt calibration)
        self.calibration_map = None  # Will be fitted during calibration
    
    def _load_lmarena_config(self) -> Dict:
        """Load base weights from LMArena configuration file."""
        config_path = Path(__file__).parent.parent / "config" / "base_weights_lmarena.json"
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            return config
        except FileNotFoundError:
            print(f"âš ï¸ LMArenaæƒé‡é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {config_path}")
            print("   ä½¿ç”¨é»˜è®¤æƒé‡é…ç½®...")
            return {
                "metadata": {"source": "é»˜è®¤é…ç½®", "updated_at": "æœªçŸ¥"},
                "weights": {},
                "fusion": {}
            }
        except json.JSONDecodeError as e:
            print(f"âš ï¸ LMArenaæƒé‡é…ç½®æ–‡ä»¶è§£æé”™è¯¯: {e}")
            print("   ä½¿ç”¨é»˜è®¤æƒé‡é…ç½®...")
            return {
                "metadata": {"source": "é»˜è®¤é…ç½®", "updated_at": "æœªçŸ¥"},
                "weights": {},
                "fusion": {}
            }
    
    def _log_base_weights(self):
        """Log base weights information on startup."""
        source = self.metadata.get("source", "æœªçŸ¥æ¥æº")
        updated_at = self.metadata.get("updated_at", "æœªçŸ¥")
        
        print(f"ğŸ“Š æ¨¡å‹åŸºç¡€æƒé‡æ¥æºï¼š{source}")
        
        # Collect weight info for active models
        weight_info = []
        for model_key, config in self.base_weights.items():
            base_weight = config.get("base_weight", 1.0) or 0.0
            # ã€é˜²å¾¡ã€‘ç¡®ä¿ base_weight ä¸ä¸º None
            if base_weight is None:
                print(f"âš ï¸ base_weight is None for {model_key}, using default 0.0")
                base_weight = 0.0
            # Find display name (simplified)
            if "gemini" in model_key:
                weight_info.append(f"Gemini {(base_weight or 0.0):.2f}")
            elif "claude" in model_key:
                weight_info.append(f"Claude {(base_weight or 0.0):.2f}")
            elif "gpt" in model_key:
                weight_info.append(f"GPT-4o {(base_weight or 0.0):.2f}")
            elif "grok" in model_key:
                weight_info.append(f"Grok {(base_weight or 0.0):.2f}")
            elif "deepseek" in model_key:
                weight_info.append(f"DeepSeek {(base_weight or 0.0):.2f}")
        
        if weight_info:
            print(" / ".join(weight_info))
        else:
            print("æœªåŠ è½½åˆ°æƒé‡é…ç½®")
    
    def _get_base_weight(self, model_name: str) -> float:
        """
        Get base weight for a model from LMArena config.
        
        Args:
            model_name: Actual model ID used in the system
            
        Returns:
            Base weight from LMArena config, or 1.0 if not found
        """
        # Map model name to LMArena config key
        lmarena_key = self.MODEL_NAME_MAPPING.get(model_name, model_name)
        
        # Try exact match first
        if lmarena_key in self.base_weights:
            return self.base_weights[lmarena_key].get("base_weight", 1.0)
        
        # Try partial match (e.g., "claude-3-7-sonnet" might match "claude-opus-4-1")
        for key, config in self.base_weights.items():
            if model_name in key or key in model_name:
                return config.get("base_weight", 1.0)
        
        # Fallback: return 1.0
        return 1.0
    
    def fuse_predictions(
        self,
        model_results: Dict[str, Optional[Dict]],
        model_weights: Dict[str, float],
        market_prob: float,
        orchestrator=None  # Optional: pass orchestrator instance to avoid creating new one
    ) -> Dict:
        """
        Fuse predictions from multiple models.
        
        Args:
            model_results: Dict mapping model_name -> {probability, confidence, reasoning}
            model_weights: Dict mapping model_name -> weight
            market_prob: Market probability from Polymarket
        
        Returns:
            Dict with final_prob, uncertainty, summary, disagreement
        """
        start_time = time.time()
        print(f"[DEBUG] ========== fuse_predictions START ==========")
        print(f"[DEBUG] Model results count: {len(model_results)}")
        print(f"[DEBUG] Market prob: {market_prob}")
        # Filter out None results
        valid_results = {
            name: result
            for name, result in model_results.items()
            if result is not None
        }
        
        if not valid_results:
            # Fallback if no models responded
            return {
                "final_prob": market_prob,
                "model_only_prob": None,  # No model prediction available
                "uncertainty": 10.0,
                "summary": "æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹å“åº”ã€‚ä½¿ç”¨å¸‚åœºæ¦‚ç‡ã€‚",
                "disagreement": "é«˜",
                "model_count": 0
            }
        
        # Calculate weighted probabilities
        probabilities: List[float] = []
        weights: List[float] = []
        reasonings = []
        deepseek_reasoning = None  # Store DeepSeek's reasoning separately
        
        for model_name, result in valid_results.items():
            prob = self._safe_float(result.get("probability"))
            confidence = result["confidence"]
            reasoning = self._combine_reasonings(result)
            
            # Use LMArena base weight instead of model_weights parameter
            base_weight = self._get_base_weight(model_name)
            confidence_weight = self.CONFIDENCE_WEIGHTS.get(confidence, 2.0)
            total_weight = self._safe_float(base_weight, 0.0) * self._safe_float(confidence_weight, 0.0)
            
            probabilities.append(prob)
            weights.append(total_weight)
            # Only add reasoning, not model name
            reasonings.append(reasoning)
            
            # Extract DeepSeek reasoning separately
            if model_name == "deepseek-chat":
                deepseek_reasoning = reasoning
        
        # Weighted mean / variance without numpy (é˜²æ­¢ numpy å¯¼å…¥å¯¼è‡´å´©æºƒ)
        weighted_mean = self._weighted_mean(probabilities, weights)
        uncertainty = self._weighted_std(probabilities, weights, weighted_mean)
        
        # Apply consensus coefficient if enabled
        if self._should_use_consensus_coef():
            consensus_weight = weighted_mean
        else:
            # If consensus_coef is disabled, use simple average
            consensus_weight = self._mean(probabilities)
        
        # Combine with market probability (if market_bias enabled)
        if self._should_use_market_bias():
            final_prob = (
                self.MODEL_WEIGHT * consensus_weight +
                self.MARKET_WEIGHT * market_prob
            )
        else:
            # If market_bias is disabled, use pure AI consensus
            final_prob = consensus_weight
        
        # Apply de-marketization penalty (if enabled)
        demarket_applied = False
        if self._should_use_demarket_penalty():
            penalty_result = self._apply_demarket_penalty(
                final_prob, consensus_weight, market_prob, valid_results
            )
            if penalty_result:
                final_prob = penalty_result["adjusted_prob"]
                demarket_applied = True
        
        # Apply post-calibration if enabled
        calibration_applied = False
        calibration_method = self._get_calibration_method()
        if calibration_method and calibration_method != "none":
            calibrated_prob = self._apply_calibration(final_prob, calibration_method)
            if calibrated_prob is not None:
                final_prob = calibrated_prob
                calibration_applied = True
        
        # Calculate disagreement level
        disagreement = self._calculate_disagreement(probabilities, uncertainty)
        
        # Generate summary
        summary = self._generate_summary(reasonings, consensus_weight, market_prob)
        
        # Add weight source information
        weight_source = {
            "file": "base_weights_lmarena.json",
            "source": self.metadata.get("source", "æœªçŸ¥"),
            "updated_at": self.metadata.get("updated_at", "æœªçŸ¥")
        }
        
        result = {
            "final_prob": round(final_prob, 2),
            "model_only_prob": round(consensus_weight, 2),  # Pure model prediction before market fusion
            "uncertainty": round(uncertainty, 2),
            "summary": summary,
            "disagreement": disagreement,
            "model_count": len(valid_results),
            "deepseek_reasoning": deepseek_reasoning,  # DeepSeek's reasoning for separate display
            "model_versions": self._get_model_versions(valid_results, orchestrator),  # Add model versions info
            "weight_source": weight_source  # Add weight source info
        }
        
        # Add experiment flags if applicable
        if demarket_applied:
            result["demarket_applied"] = True
            result["demarket_note"] = "Applied de-marketization penalty to avoid pure tracking."
        if calibration_applied:
            result["calibration_applied"] = True
            result["calibration_method"] = calibration_method
        
        total_time = time.time() - start_time
        # ã€é˜²å¾¡ã€‘ç¡®ä¿ total_time ä¸ä¸º None
        total_time = total_time or 0.0
        if total_time is None:
            print("âš ï¸ total_time is None, using default 0.0")
            total_time = 0.0
        print(f"[DEBUG] Total model fusion time: {(total_time or 0.0):.2f}s")
        print(f"[DEBUG] ========== fuse_predictions END ==========")
        
        return result
    
    def _get_model_versions(self, model_results: Dict[str, Optional[Dict]], orchestrator=None) -> Dict[str, Dict]:
        """Get model version information from ModelOrchestrator."""
        try:
            # Use provided orchestrator if available, otherwise create a new one
            if orchestrator is None:
                from src.model_orchestrator import ModelOrchestrator
                orchestrator = ModelOrchestrator()
            
            versions = {}
            for model_name in model_results.keys():
                if model_results[model_name] is not None:  # Only for successful calls
                    info = orchestrator.get_model_info(model_name)
                    if info:
                        versions[model_name] = {
                            "display_name": info["display_name"],
                            "last_updated": info.get("last_updated", "æœªçŸ¥")
                        }
            
            return versions
        except Exception as e:
            print(f"âš ï¸ è·å–æ¨¡å‹ç‰ˆæœ¬ä¿¡æ¯å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {}
    
    def _calculate_disagreement(self, probabilities: List[float], uncertainty: float) -> str:
        """Calculate disagreement level based on uncertainty."""
        # Handle edge case: single model or zero uncertainty
        if len(probabilities) <= 1 or uncertainty == 0.0:
            return "ä½"
        elif uncertainty < 3.0:
            return "ä½"
        elif uncertainty < 8.0:
            return "ä¸­"
        else:
            return "é«˜"
    
    def _generate_summary(self, reasonings: List[str], model_prob: float, market_prob: float) -> str:
        """
        Generate a brief summary from model reasonings.
        ä¼˜åŒ–ï¼šç¡®ä¿ç”Ÿæˆçš„æ‘˜è¦æ›´ç®€æ´ï¼Œçªå‡ºå…³é”®è§‚ç‚¹ï¼Œä¸”ä¸ºä¸­æ–‡ã€‚
        """
        if not reasonings:
            return "æš‚æ— è¯¦ç»†æ¨ç†ä¿¡æ¯ã€‚"
        
        # Combine first few reasonings (already cleaned, no model names)
        # ä¼˜å…ˆä½¿ç”¨å‰2ä¸ªæ¨¡å‹çš„æ¨ç†ï¼Œå¦‚æœéƒ½æ˜¯ä¸­æ–‡åˆ™ç›´æ¥åˆå¹¶
        summary_parts = reasonings[:2]  # Use first 2 models' reasoning
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦
        has_chinese = any(any('\u4e00' <= char <= '\u9fff' for char in part) for part in summary_parts)
        
        if has_chinese:
            # å¦‚æœåŒ…å«ä¸­æ–‡ï¼Œç›´æ¥åˆå¹¶
            summary = " ".join(summary_parts)
        else:
            # å¦‚æœéƒ½æ˜¯è‹±æ–‡ï¼Œæ·»åŠ æç¤ºè¯´æ˜è¿™æ˜¯è‹±æ–‡æ¨ç†
            # å®é™…ä½¿ç”¨ä¸­ï¼Œç”±äºpromptè¦æ±‚ä¸­æ–‡ï¼Œè¿™é‡Œåº”è¯¥ä¸ä¼šè§¦å‘
            summary = "æ¨¡å‹æ¨ç†ï¼š" + " ".join(summary_parts)
        
        # Truncate if too long (ä¿æŒç®€æ´)
        summary = self._safe_shorten_text(summary, limit=800)

        return summary

    def _combine_reasonings(self, result: Dict) -> str:
        pieces = []
        for key in ("reasoning_short", "reasoning_long", "reasoning"):
            value = result.get(key)
            if value:
                cleaned = self._sanitize_reasoning_fragment(value, context=f"fusion_{key}")
                cleaned = cleaned.replace("Parsed from unstructured response.", "").replace("Parsed from unstructured response", "").strip()
                if cleaned:
                    pieces.append(cleaned)
        combined = " ".join(pieces).strip()
        if not combined:
            combined = "æš‚æ— è¯¦ç»†æ¨ç†ä¿¡æ¯ã€‚"
        combined = self._safe_shorten_text(combined, limit=800)
        print(f"[COMBINE] reasoning_merged (final_len={len(combined)})")
        return combined

    @staticmethod
    def _sanitize_reasoning_fragment(value: Optional[str], context: str = "fusion") -> str:
        if value is None:
            return ""
        cleaned = str(value)
        original = cleaned
        changed = False
        fence_pattern = re.compile(r"```(?:json)?[\s\S]*?```", re.IGNORECASE)
        new_cleaned = fence_pattern.sub("", cleaned)
        if new_cleaned != cleaned:
            cleaned = new_cleaned
            changed = True
        json_pattern = re.compile(r"\{[^{}]*:[^{}]*\}")
        while True:
            new_cleaned = json_pattern.sub("", cleaned)
            if new_cleaned == cleaned:
                break
            cleaned = new_cleaned
            changed = True
        if cleaned.count("{") > cleaned.count("}"):
            idx = cleaned.rfind("{")
            if idx != -1:
                cleaned = cleaned[:idx]
                changed = True
        cleaned = cleaned.replace("```", "")
        cleaned = re.sub(r"\s+", " ", cleaned).strip()
        if cleaned and cleaned[-1] in "{[,:":
            terminators = [cleaned.rfind(ch) for ch in "ã€‚ï¼ï¼Ÿ.!?"]
            terminators = [idx for idx in terminators if idx != -1]
            if terminators:
                cleaned = cleaned[: max(terminators) + 1]
                changed = True
        if changed and cleaned != original:
            print(f"[CLEANUP] Removed JSON artifacts ({context})")
        return cleaned
    
    @staticmethod
    def classify_multi_option_event(
        event_title: str,
        outcomes: List[Dict],
        event_rules: str = "",
        now_probs: Optional[List[float]] = None
    ) -> str:
        """Use layered signals to classify multi-option events."""
        if not outcomes:
            return "hybrid"

        event_title_lower = (event_title or "").lower()
        rules_lower = (event_rules or "").lower()
        option_names = [
            (outcome.get("name") or "").strip().lower()
            for outcome in outcomes
            if outcome.get("name")
        ]
        now_prob_values = now_probs or [
            outcome.get("market_prob")
            for outcome in outcomes
            if outcome.get("market_prob") is not None
        ]
        now_prob_values = [p for p in now_prob_values if p is not None]
        sum_market_fraction = None
        if now_prob_values:
            max_prob = max(now_prob_values)
            divisor = 100.0 if max_prob and max_prob > 1 else 1.0
            if divisor:
                sum_market_fraction = sum(now_prob_values) / divisor

        def log_decision(decision_type: str, source: str) -> str:
            extra = ""
            if sum_market_fraction is not None:
                extra = f" sum_market={sum_market_fraction:.3f}"
            print(f"[CLASSIFY] type={decision_type} source={source}{extra}")
            return decision_type

        # Signal 1: Rules pattern
        if rules_lower:
            mutually_patterns = [
                r"exactly one", r"only one", r"upper bound of the target federal funds range",
                r"wins the", r"which candidate", r"which party"
            ]
            conditional_patterns = [
                r"each option resolves", r"per contract", r"per date", r"resolves independently",
                r"for each date", r"multiple settlement"
            ]
            if any(re.search(pattern, rules_lower) for pattern in mutually_patterns):
                return log_decision("mutually_exclusive", "rules")
            if any(re.search(pattern, rules_lower) for pattern in conditional_patterns):
                return log_decision("conditional", "rules")

        # Signal 2: Option-set lexicon
        def _matches(name: str, keywords: List[str]) -> bool:
            return any(keyword in name for keyword in keywords)

        rate_keywords = ["bps", "basis points", "increase", "decrease", "no change", "cut", "hike"]
        candidate_keywords = ["trump", "biden", "harris", "newsom", "candidate", "party", "democrat", "republican"]
        if option_names:
            rate_ratio = sum(1 for name in option_names if _matches(name, rate_keywords)) / len(option_names)
            candidate_ratio = sum(1 for name in option_names if _matches(name, candidate_keywords)) / len(option_names)
            if rate_ratio >= 0.7:
                return log_decision("mutually_exclusive", "option_set_rate")
            if candidate_ratio >= 0.6:
                return log_decision("mutually_exclusive", "option_set_candidate")

        # Signal 3: Date buckets from option names
        date_patterns = [
            r'\d{1,2}[/-]\d{1,2}(?:[/-]\d{2,4})?',
            r'(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)[a-z]*\s+\d{1,2}',
            r'\d{1,2}\s+(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)[a-z]*'
        ]
        if option_names:
            date_hits = sum(1 for name in option_names if any(re.search(pattern, name) for pattern in date_patterns))
            if date_hits / len(option_names) >= 0.5:
                return log_decision("conditional", "date_bucket")

        # Signal 4: Sum-of-probabilities window
        if sum_market_fraction is not None and 0.95 <= sum_market_fraction <= 1.05:
            return log_decision("mutually_exclusive", "sum_window")

        # Signal 5: Structure hints
        structure_mutual_keywords = ["who will", "which of", "candidate", "federal funds", "champion"]
        structure_conditional_keywords = ["per day", "per date", "each day", "per outcome", "range"]
        if any(keyword in event_title_lower for keyword in structure_mutual_keywords):
            return log_decision("mutually_exclusive", "structure_title")
        if rules_lower and any(keyword in rules_lower for keyword in structure_conditional_keywords):
            return log_decision("conditional", "structure_rules")

        # Fallback heuristic
        price_patterns = [
            r'price\s+(above|below|over|under|at least|at most)\s+',
            r'\$\d+',
            r'\d+\s*(million|billion|trillion|k|m|b)\s*(usd|eur|â‚¬|Â¥)?',
        ]
        if any(re.search(pattern, event_title_lower, re.IGNORECASE) for pattern in price_patterns):
            return log_decision("conditional", "fallback_price")

        conditional_keywords = {
            "time": [
                "oct", "nov", "dec", "jan", "feb", "mar", "apr", "may", "jun", "jul", "aug", "sep",
                "october", "november", "december", "january", "february", "march", "april",
                "june", "july", "august", "september",
                "monday", "tuesday", "wednesday", "thursday", "friday", "saturday", "sunday",
                "mon", "tue", "wed", "thu", "fri", "sat", "sun",
                "2024", "2025", "2026", "2027", "2028", "2029", "2030",
                "day", "month", "year", "q1", "q2", "q3", "q4", "h1", "h2",
                "before", "after", "by", "until", "deadline",
                r'\d{1,2}[/-]\d{1,2}',
                r'\d{1,2}[/-]\d{1,2}[/-]\d{4}',
            ]
        }
        conditional_score = 0
        for keywords in conditional_keywords.values():
            for kw in keywords:
                if isinstance(kw, str) and kw in event_title_lower:
                    conditional_score += 2
                elif isinstance(kw, str) and kw.startswith('\\') and re.search(kw, event_title_lower):
                    conditional_score += 2
        if conditional_score >= 2:
            return log_decision("conditional", "fallback_title")

        return log_decision("mutually_exclusive", "fallback_default")

    @staticmethod
    def filter_invalid_outcomes(outcomes: List[Dict]) -> List[Dict]:
        """
        è‡ªåŠ¨è·³è¿‡å·²è¿‡æœŸæˆ–æ— æ•ˆçš„å­å¸‚åœºã€‚
        
        è¿‡æ»¤è§„åˆ™ï¼š
        - æ—¶é—´å‹ï¼šæ—¥æœŸæ—©äºå½“å‰ UTC æ—¥æœŸ
        - ä»·æ ¼å‹ï¼šæ¡ä»¶é€»è¾‘æ— æ•ˆï¼ˆå¦‚ "above 1000000%"ï¼‰
        - å…¶ä»–æ— æ•ˆé¡¹ï¼šé‡å¤æˆ–ç©ºæ ‡é¢˜
        
        Args:
            outcomes: é€‰é¡¹åˆ—è¡¨
        
        Returns:
            è¿‡æ»¤åçš„æœ‰æ•ˆé€‰é¡¹åˆ—è¡¨
        """
        if not outcomes:
            return []
        
        valid_outcomes = []
        seen_names = set()
        now_utc = datetime.now(timezone.utc)
        
        # æ—¥æœŸæ¨¡å¼åŒ¹é…
        date_patterns = [
            r'(\d{1,2})[/-](\d{1,2})[/-](\d{4})',  # MM/DD/YYYY or DD/MM/YYYY
            r'(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)\s+(\d{1,2}),?\s+(\d{4})',  # Month DD, YYYY
            r'(\d{4})[/-](\d{1,2})[/-](\d{1,2})',  # YYYY/MM/DD
        ]
        
        month_map = {
            'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6,
            'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12
        }
        
        for outcome in outcomes:
            name = outcome.get('name', '').strip()
            
            # è¿‡æ»¤ç©ºæ ‡é¢˜
            if not name:
                continue
            
            # è¿‡æ»¤é‡å¤é¡¹
            name_normalized = name.lower().strip()
            if name_normalized in seen_names:
                print(f"[DEBUG] è·³è¿‡é‡å¤é€‰é¡¹: {name}")
                continue
            seen_names.add(name_normalized)
            
            # å°è¯•è§£ææ—¥æœŸï¼ˆæ—¶é—´å‹é€‰é¡¹ï¼‰
            is_expired = False
            name_lower = name.lower()
            
            for pattern in date_patterns:
                match = re.search(pattern, name_lower, re.IGNORECASE)
                if match:
                    try:
                        if 'jan' in name_lower or 'feb' in name_lower:  # æœˆä»½åç§°æ ¼å¼
                            month_name = match.group(1).lower()[:3]
                            day = int(match.group(2))
                            year = int(match.group(3))
                            month = month_map.get(month_name)
                            if month:
                                option_date = datetime(year, month, day, tzinfo=timezone.utc)
                                if option_date < now_utc:
                                    is_expired = True
                                    print(f"[DEBUG] è·³è¿‡è¿‡æœŸé€‰é¡¹: {name} (æ—¥æœŸ: {option_date.date()})")
                        else:  # æ•°å­—æ ¼å¼æ—¥æœŸ
                            parts = match.groups()
                            if len(parts) == 3:
                                # å°è¯•ä¸åŒçš„æ—¥æœŸæ ¼å¼
                                try:
                                    # MM/DD/YYYY
                                    month, day, year = int(parts[0]), int(parts[1]), int(parts[2])
                                    if month <= 12 and day <= 31:
                                        option_date = datetime(year, month, day, tzinfo=timezone.utc)
                                        if option_date < now_utc:
                                            is_expired = True
                                            print(f"[DEBUG] è·³è¿‡è¿‡æœŸé€‰é¡¹: {name} (æ—¥æœŸ: {option_date.date()})")
                                except:
                                    try:
                                        # DD/MM/YYYY
                                        day, month, year = int(parts[0]), int(parts[1]), int(parts[2])
                                        if month <= 12 and day <= 31:
                                            option_date = datetime(year, month, day, tzinfo=timezone.utc)
                                            if option_date < now_utc:
                                                is_expired = True
                                                print(f"[DEBUG] è·³è¿‡è¿‡æœŸé€‰é¡¹: {name} (æ—¥æœŸ: {option_date.date()})")
                                    except:
                                        pass
                    except:
                        pass
                    break
            
            if is_expired:
                continue
            
            # è¿‡æ»¤æ— æ•ˆä»·æ ¼æ¡ä»¶ï¼ˆå¦‚ "above 1000000%" è¿™ç§æ˜æ˜¾ä¸åˆç†çš„æƒ…å†µï¼‰
            if '%' in name or '$' in name or 'usd' in name_lower:
                # æ£€æŸ¥æ˜¯å¦æœ‰æç«¯æ•°å€¼
                numbers = re.findall(r'[\d,]+\.?\d*', name)
                for num_str in numbers:
                    try:
                        num = float(num_str.replace(',', ''))
                        # å¦‚æœç™¾åˆ†æ¯”è¶…è¿‡ 1000% æˆ–ä»·æ ¼è¶…è¿‡åˆç†èŒƒå›´ï¼Œå¯èƒ½æ˜¯æ— æ•ˆé€‰é¡¹
                        if '%' in name and num > 1000:
                            print(f"[DEBUG] è·³è¿‡æ— æ•ˆä»·æ ¼é€‰é¡¹: {name} (æ•°å€¼å¼‚å¸¸: {num}%)")
                            is_expired = True
                            break
                    except:
                        pass
            
            if is_expired:
                continue
            
            # æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼Œæ·»åŠ æœ‰æ•ˆé€‰é¡¹
            valid_outcomes.append(outcome)
        
        if len(outcomes) != len(valid_outcomes):
            print(f"[DEBUG] è¿‡æ»¤æ— æ•ˆé€‰é¡¹: {len(outcomes)} â†’ {len(valid_outcomes)}")
        
        return valid_outcomes
    
    @staticmethod
    def normalize_all_predictions(
        outcomes: List[Dict],
        event_title: str = "",
        event_rules: str = "",
        now_probabilities: Optional[List[float]] = None
    ) -> Dict:
        """
        å¯¹æ‰€æœ‰ outcome çš„ AI é¢„æµ‹æ¦‚ç‡è¿›è¡Œå½’ä¸€åŒ–ï¼Œä½¿å…¶æ€»å’Œç­‰äº 100%ã€‚
        
        åŠŸèƒ½ï¼š
        1. æ ¹æ®äº‹ä»¶ç±»å‹åˆ¤æ–­æ˜¯å¦éœ€è¦å½’ä¸€åŒ–ï¼ˆäº’æ–¥äº‹ä»¶å½’ä¸€åŒ–ï¼Œæ¡ä»¶äº‹ä»¶ä¸å½’ä¸€åŒ–ï¼‰
        2. è¿‡æ»¤æ— æ•ˆ/è¿‡æœŸé€‰é¡¹
        3. ä¼ å…¥è§„åˆ™ä¸æœ€æ–°å¸‚åœºæŠ¥ä»·ï¼Œæå‡åˆ†ç±»å‡†ç¡®åº¦
        4. è‹¥ Î£(AI_probs) â‰  100ï¼ŒæŒ‰æ¯”ä¾‹ç¼©æ”¾ï¼›å½“ Î£(AI) ä¸åœ¨ [0.90, 1.10]ï¼Œè§¦å‘å®‰å…¨å½’ä¸€åŒ–
        5. åŒæ—¶å½’ä¸€åŒ–ä¸ç¡®å®šåº¦ï¼ˆuncertaintyï¼‰ï¼Œä¿æŒç›¸å¯¹æ¯”ä¾‹
        6. å¤„ç†å¼‚å¸¸å€¼ï¼ˆ>100 æˆ– <0ï¼‰è‡ªåŠ¨è£å‰ªè‡³ [0,100]
        7. å®¹é”™å¤„ç†ï¼šNone å€¼ä½¿ç”¨å›é€€æ–¹æ¡ˆï¼ˆå¸‚åœºä»·æ ¼â†’å‡åˆ†ï¼‰
        
        Args:
            outcomes: outcome åˆ—è¡¨ï¼ŒåŒ…å« prediction / market_prob / model_only_prob ç­‰å­—æ®µ
            event_title: äº‹ä»¶æ ‡é¢˜ï¼Œç”¨äºåˆ¤æ–­äº‹ä»¶ç±»å‹
            event_rules: å¸‚åœºè§„åˆ™æ–‡æœ¬
            now_probabilities: æœ€æ–°å¸‚åœºæŠ¥ä»·åˆ—è¡¨ï¼ˆç”¨äºæ€»å’Œçª—åˆ¤æ–­ï¼‰
        
        Returns:
            Dict with:
                - "normalized_outcomes": List[Dict]
                - "total_before": float (å½’ä¸€åŒ–å‰çš„æ€»å’Œ)
                - "total_after": float (å½’ä¸€åŒ–åçš„æ€»å’Œ)
                - "error": float
                - "skipped_count": int
                - "normalized": bool
                - "event_type": str
                - "reason": Optional[str] ("type" | "sum_guard" | None)
        """
        if not outcomes or len(outcomes) == 0:
            return {
                "normalized_outcomes": [],
                "total_before": 0.0,
                "total_after": 0.0,
                "error": 0.0,
                "skipped_count": 0,
                "normalized": False,
                "event_type": "hybrid",
                "reason": None
            }
        
        # ã€æ–°å¢ã€‘è¿‡æ»¤æ— æ•ˆ/è¿‡æœŸé€‰é¡¹
        filtered_outcomes = FusionEngine.filter_invalid_outcomes(outcomes)
        if len(filtered_outcomes) != len(outcomes):
            print(f"[DEBUG] è¿‡æ»¤åé€‰é¡¹æ•°é‡: {len(outcomes)} â†’ {len(filtered_outcomes)}")
        
        # ã€æ–°å¢ã€‘è¯†åˆ«äº‹ä»¶ç±»å‹
        if not event_title:
            print(f"[WARNING] event_title ä¸ºç©ºï¼Œå¯èƒ½å½±å“äº‹ä»¶ç±»å‹è¯†åˆ«")
        derived_now_probs = now_probabilities or [
            outcome.get("market_prob")
            for outcome in filtered_outcomes
            if outcome.get("market_prob") is not None
        ]
        event_type = FusionEngine.classify_multi_option_event(
            event_title or "",
            filtered_outcomes,
            event_rules=event_rules,
            now_probs=derived_now_probs
        )
        print(f"[DEBUG] æœ€ç»ˆäº‹ä»¶ç±»å‹: {event_type} (äº‹ä»¶: {(event_title or '')[:50]}...)")
        
        # ã€å…³é”®æ”¹è¿›ã€‘æ ¹æ®äº‹ä»¶ç±»å‹å†³å®šæ˜¯å¦å½’ä¸€åŒ–
        normalize_reason: Optional[str] = None
        should_normalize = (event_type == "mutually_exclusive")
        if should_normalize:
            normalize_reason = "type"

        # Sum guardï¼šAI æ€»å’Œè¶…å‡º [0.9, 1.1] å¼ºåˆ¶å½’ä¸€åŒ–
        ai_values_for_guard = [
            outcome.get("model_only_prob")
            for outcome in filtered_outcomes
            if outcome.get("model_only_prob") is not None
        ]
        ai_sum_guard = sum(ai_values_for_guard) if ai_values_for_guard else 0.0
        guard_fraction = (ai_sum_guard / 100.0) if filtered_outcomes else None
        sum_guard_triggered = False
        if filtered_outcomes and guard_fraction is not None:
            if guard_fraction < 0.90 or guard_fraction > 1.10:
                should_normalize = True
                normalize_reason = "sum_guard"
                sum_guard_triggered = True
        if sum_guard_triggered:
            print(f"[FusionEngine] Sum guard triggered: Î£AI={guard_fraction:.3f}, å¼ºåˆ¶å½’ä¸€åŒ–ã€‚")
        
        if not should_normalize:
            # æ¡ä»¶äº‹ä»¶æˆ–æ··åˆäº‹ä»¶ï¼šä¸å½’ä¸€åŒ–ï¼Œåªæ·»åŠ  normalized æ ‡è®°
            if event_type == "conditional":
                print(f"[FusionEngine] æ¡ä»¶äº‹ä»¶æ£€æµ‹åˆ°ï¼Œè·³è¿‡å½’ä¸€åŒ–ã€‚")
            
            marked_outcomes = []
            for outcome in filtered_outcomes:
                marked_outcome = outcome.copy()
                marked_outcome["normalized"] = False
                marked_outcomes.append(marked_outcome)
            
            # è®¡ç®—æ€»å’Œï¼ˆä»…ç”¨äºè°ƒè¯•ï¼Œä¸è¿”å›ç»™è¾“å‡ºå±‚ï¼‰
            total_before = sum(
                outcome.get("model_only_prob") or 0
                for outcome in filtered_outcomes
                if outcome.get("model_only_prob") is not None
            )
            
            return {
                "normalized_outcomes": marked_outcomes,
                "total_before": round(total_before, 2),
                "total_after": None,  # ã€ä¿®å¤ã€‘æ¡ä»¶äº‹ä»¶ä¸è¿”å› total_afterï¼Œè¡¨ç¤ºä¸åº”æ˜¾ç¤ºå½’ä¸€åŒ–æ£€æŸ¥
                "error": 0.0,
                "skipped_count": len(outcomes) - len(filtered_outcomes),
                "normalized": False,
                "event_type": event_type,
                "reason": None,
                "normalization_reason": "ok"  # æ¡ä»¶äº‹ä»¶ä¸å½’ä¸€åŒ–ï¼Œæ ‡è®°ä¸º "ok"
            }
        
        # äº’æ–¥äº‹ä»¶ï¼šæ‰§è¡Œå½’ä¸€åŒ–ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
        outcomes = filtered_outcomes  # ä½¿ç”¨è¿‡æ»¤åçš„é€‰é¡¹
        
        # æå–æ‰€æœ‰æœ‰æ•ˆçš„ AI é¢„æµ‹å€¼ï¼ˆä¼˜å…ˆä½¿ç”¨ model_only_probï¼Œå¦‚æœæ²¡æœ‰åˆ™å°è¯•ä» prediction æ¨ç®—ï¼‰
        valid_outcomes = []
        ai_probs = []
        uncertainties = []
        skipped_indices = []
        
        fallback_mode = normalize_reason == "sum_guard"
        equal_split_value = (100.0 / len(outcomes)) if outcomes else 0.0

        for i, outcome in enumerate(outcomes):
            ai_prob = outcome.get("model_only_prob")
            if ai_prob is None and fallback_mode:
                fallback_prob = outcome.get("market_prob")
                if fallback_prob is None:
                    fallback_prob = equal_split_value
                try:
                    ai_prob = float(fallback_prob)
                except (TypeError, ValueError):
                    ai_prob = equal_split_value
                print(f"[FusionEngine] Sum guard fallbackä½¿ç”¨ {'market_prob' if outcome.get('market_prob') is not None else 'equal-split'}: {outcome.get('name', i)} = {ai_prob:.2f}%")

            if ai_prob is None:
                skipped_indices.append(i)
                continue
            
            # è£å‰ªå¼‚å¸¸å€¼åˆ° [0, 100]
            ai_prob = max(0.0, min(100.0, float(ai_prob)))
            
            uncertainty = outcome.get("uncertainty", 0.0)
            if uncertainty is None:
                uncertainty = 0.0
            
            valid_outcomes.append(outcome)
            ai_probs.append(ai_prob)
            uncertainties.append(float(uncertainty))
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®
        if len(valid_outcomes) == 0:
            # æ‰€æœ‰é€‰é¡¹éƒ½æ— æ•ˆï¼Œè¿”å›åŸæ•°æ®
            return {
                "normalized_outcomes": outcomes,
                "total_before": 0.0,
                "total_after": 0.0,
                "error": 0.0,
                "skipped_count": len(outcomes),
                "event_type": event_type,
                "normalized": False,
                "reason": None
            }
        
        # è®¡ç®—æ€»å’Œ
        total_before = sum(ai_probs)
        
        # ã€å…³é”®æ”¹è¿›ã€‘äº’æ–¥äº‹ä»¶ï¼šå½’ä¸€åŒ–å¤„ç†
        if total_before == 0:
            # å…¨ä¸º 0 çš„æƒ…å†µï¼šå‡åˆ†ï¼ˆé¿å…é™¤é›¶ï¼‰
            normalized_probs = [100.0 / len(ai_probs)] * len(ai_probs)
            # ä¸ç¡®å®šåº¦ä¿æŒåŸå€¼ï¼ˆå·²ç»æ˜¯ç›¸å¯¹å€¼ï¼‰
            normalized_uncertainties = uncertainties
            total_after = 100.0
        else:
            # æŒ‰æ¯”ä¾‹ç¼©æ”¾
            scale_factor = 100.0 / total_before
            normalized_probs = [prob * scale_factor for prob in ai_probs]
            
            # å½’ä¸€åŒ–ä¸ç¡®å®šåº¦ï¼šä¿æŒç›¸å¯¹æ¯”ä¾‹ï¼Œä½†éœ€è¦ç›¸åº”ç¼©æ”¾
            # ç”±äºæ¦‚ç‡è¢«ç¼©æ”¾ï¼Œä¸ç¡®å®šåº¦ä¹Ÿåº”è¯¥æŒ‰ç›¸åŒæ¯”ä¾‹ç¼©æ”¾ï¼ˆä¿æŒç›¸å¯¹å…³ç³»ï¼‰
            normalized_uncertainties = [unc * scale_factor for unc in uncertainties]
            
            # éªŒè¯æ€»å’Œ
            total_after = sum(normalized_probs)
        
        # è®¡ç®—è¯¯å·®
        error = abs(total_after - 100.0)
        
        # æ›´æ–° outcomes
        normalized_outcomes = []
        valid_idx = 0
        
        # ã€Bugä¿®å¤ã€‘æ·»åŠ éªŒè¯ï¼Œç¡®ä¿ valid_idx ä¸ä¼šè¶Šç•Œ
        if len(normalized_probs) != len(ai_probs):
            print(f"âš ï¸ [WARNING] å½’ä¸€åŒ–æ•°ç»„é•¿åº¦ä¸åŒ¹é…ï¼šnormalized_probs={len(normalized_probs)}, ai_probs={len(ai_probs)}")
        
        for i, outcome in enumerate(outcomes):
            if i in skipped_indices:
                # è·³è¿‡çš„é€‰é¡¹ï¼Œä¿æŒåŸæ ·ï¼ˆç¡®ä¿ model_only_prob ä¿æŒä¸º Noneï¼‰
                skipped_outcome = outcome.copy()
                # ã€Bugä¿®å¤ã€‘æ˜ç¡®ç¡®ä¿è·³è¿‡çš„é€‰é¡¹çš„ model_only_prob ä¸º None
                if skipped_outcome.get("model_only_prob") is not None:
                    print(f"[DEBUG] è·³è¿‡é€‰é¡¹ {outcome.get('name', i)}ï¼Œä½† model_only_prob ä¸ä¸º Noneï¼Œå¼ºåˆ¶è®¾ä¸º None")
                skipped_outcome["model_only_prob"] = None
                normalized_outcomes.append(skipped_outcome)
            else:
                # æ›´æ–° AI é¢„æµ‹æ¦‚ç‡ï¼ˆéœ€è¦åŒæ—¶æ›´æ–° predictionï¼Œå› ä¸ºå®ƒæ˜¯èåˆåçš„å€¼ï¼‰
                if valid_idx >= len(normalized_probs):
                    print(f"âš ï¸ [ERROR] valid_idx ({valid_idx}) >= normalized_probs é•¿åº¦ ({len(normalized_probs)})")
                    # Fallback: ä¿æŒåŸæ ·ï¼Œä½†ä¸æ›´æ–° model_only_prob
                    normalized_outcomes.append(outcome.copy())
                else:
                    updated_outcome = outcome.copy()
                    
                    # æ›´æ–° model_only_probï¼ˆçº¯AIé¢„æµ‹ï¼Œå½’ä¸€åŒ–åçš„å€¼ï¼‰
                    normalized_value = round(normalized_probs[valid_idx], 2)
                    updated_outcome["model_only_prob"] = normalized_value
                    
                    # ã€Bugä¿®å¤ã€‘éªŒè¯å½’ä¸€åŒ–å€¼æ˜¯å¦åˆç†
                    if normalized_value < 0 or normalized_value > 100:
                        print(f"âš ï¸ [WARNING] å½’ä¸€åŒ–å€¼å¼‚å¸¸ï¼š{outcome.get('name', i)} = {normalized_value}%")
                    
                    # æ›´æ–° predictionï¼ˆèåˆåçš„æ¦‚ç‡ï¼‰ï¼šéœ€è¦é‡æ–°èåˆ
                    market_prob = outcome.get("market_prob", 0)
                    updated_prediction = (
                        FusionEngine.MODEL_WEIGHT * normalized_probs[valid_idx] +
                        FusionEngine.MARKET_WEIGHT * market_prob
                    )
                    updated_outcome["prediction"] = round(updated_prediction, 2)
                    
                    # æ›´æ–°ä¸ç¡®å®šåº¦
                    updated_outcome["uncertainty"] = round(normalized_uncertainties[valid_idx], 2)
                    
                    # ã€æ–°å¢ã€‘æ·»åŠ å½’ä¸€åŒ–æ ‡è®°
                    updated_outcome["normalized"] = True
                    
                    normalized_outcomes.append(updated_outcome)
                
                valid_idx += 1
        
        # ã€æ–°å¢ã€‘ä¸ºè·³è¿‡çš„é€‰é¡¹ä¹Ÿæ·»åŠ  normalized æ ‡è®°
        for i, outcome in enumerate(normalized_outcomes):
            if outcome.get("normalized") is None:
                outcome["normalized"] = False  # è·³è¿‡çš„é€‰é¡¹æœªå½’ä¸€åŒ–
        
        # ã€ä¿®å¤ã€‘æ·»åŠ  normalization_reason: "sum_guard_scaled" æˆ– "ok"
        normalization_reason_value = "ok"
        if normalize_reason == "sum_guard":
            normalization_reason_value = "sum_guard_scaled"
        elif normalize_reason == "type":
            normalization_reason_value = "ok"  # æ­£å¸¸å½’ä¸€åŒ–
        
        return {
            "normalized_outcomes": normalized_outcomes,
            "total_before": round(total_before, 2),
            "total_after": round(total_after, 2),
            "error": round(error, 4),
            "skipped_count": len(skipped_indices),
            "normalized": True,  # æ ‡è®°å·²å½’ä¸€åŒ–
            "event_type": event_type,
            "reason": normalize_reason,
            "normalization_reason": normalization_reason_value  # æ–°å¢å­—æ®µ
        }
    
    def _should_use_consensus_coef(self) -> bool:
        """
        å†³å®šæ˜¯å¦å¯ç”¨å…±è¯†åŠ æƒï¼ˆconsensus coefficientï¼‰ã€‚
        
        Returns:
            True if consensus coefficient should be used (default), False otherwise
        """
        if self.experiment_config is None:
            return True  # Default: use consensus weighting
        
        try:
            # Handle ExperimentConfig object or dict
            if hasattr(self.experiment_config, 'config'):
                config = self.experiment_config.config
            else:
                config = self.experiment_config
            
            return config.get("FUSION", {}).get("consensus_coef", True)
        except Exception:
            return True  # Default on error
    
    def _should_use_market_bias(self) -> bool:
        """
        å†³å®šæ˜¯å¦å¯ç”¨å¸‚åœºåå·®èåˆï¼ˆmarket biasï¼‰ã€‚
        
        Returns:
            True if market bias should be used (default), False otherwise
        """
        if self.experiment_config is None:
            return True  # Default: use market bias
        
        try:
            # Handle ExperimentConfig object or dict
            if hasattr(self.experiment_config, 'config'):
                config = self.experiment_config.config
            else:
                config = self.experiment_config
            
            return config.get("FUSION", {}).get("market_bias", True)
        except Exception:
            return True  # Default on error
    
    def _should_use_demarket_penalty(self) -> bool:
        """
        å†³å®šæ˜¯å¦å¯ç”¨åå¸‚åœºè·Ÿè¸ªæƒ©ç½šï¼ˆde-marketization penaltyï¼‰ã€‚
        
        Returns:
            True if de-marketization penalty should be applied, False otherwise (default: False for experiments)
        """
        if self.experiment_config is None:
            return False  # Default: disabled (experimental feature)
        
        try:
            # Handle ExperimentConfig object or dict
            if hasattr(self.experiment_config, 'config'):
                config = self.experiment_config.config
            else:
                config = self.experiment_config
            
            return config.get("FUSION", {}).get("demarket_penalty", False)
        except Exception:
            return False  # Default on error
    
    def _get_calibration_method(self) -> Optional[str]:
        """
        è·å–åæ ¡å‡†æ–¹æ³•ã€‚
        
        Returns:
            Calibration method: "none", "binning", "platt", or None
        """
        if self.experiment_config is None:
            return "none"  # Default: no calibration
        
        try:
            # Handle ExperimentConfig object or dict
            if hasattr(self.experiment_config, 'config'):
                config = self.experiment_config.config
            else:
                config = self.experiment_config
            
            method = config.get("CALIBRATION", {}).get("post_calibration", "none")
            if method in ["none", "binning", "platt"]:
                return method
            return "none"
        except Exception:
            return "none"  # Default on error
    
    def _apply_demarket_penalty(
        self,
        final_prob: float,
        consensus_weight: float,
        market_prob: float,
        model_results: Dict[str, Dict]
    ) -> Optional[Dict]:
        """
        åº”ç”¨åå¸‚åœºè·Ÿè¸ªæƒ©ç½šï¼šå¦‚æœAIé¢„æµ‹ä¸å¸‚åœºæ¦‚ç‡è¿‡äºæ¥è¿‘ä¸”å…±è¯†åº¦é«˜ï¼Œæ·»åŠ å°æ‰°åŠ¨é¿å…çº¯è·Ÿè¸ªã€‚
        
        Args:
            final_prob: å½“å‰èåˆåçš„æ¦‚ç‡
            consensus_weight: æ¨¡å‹å…±è¯†æ¦‚ç‡
            market_prob: å¸‚åœºä»·æ ¼
            model_results: æ¨¡å‹ç»“æœå­—å…¸
        
        Returns:
            Dict with "adjusted_prob" if penalty applied, None otherwise
        """
        # Check if AI prediction is too close to market (within 2%)
        ai_market_diff = abs(consensus_weight - market_prob)
        
        if ai_market_diff < 0.02:  # Less than 2% difference
            # Calculate consensus (model agreement)
            probs = [r["probability"] for r in model_results.values()]
            if len(probs) > 1:
                avg_prob = self._mean(probs)
                if avg_prob > 0:
                    consensus = 1.0 - (self._std(probs) / avg_prob)
                else:
                    consensus = 0.0
            else:
                consensus = 1.0
            
            # Only apply penalty if consensus is high (>0.8)
            if consensus > 0.8:
                # Determine perturbation direction based on minority opinion
                # Use DeepSeek or Grok if available for minority signal
                perturbation = 0.015  # Â±1.5% perturbation
                
                # Try to get minority opinion (DeepSeek or Grok)
                minority_prob = None
                for model_name in ["deepseek-chat", "grok-4", "grok-3"]:
                    if model_name in model_results:
                        minority_prob = model_results[model_name]["probability"]
                        break
                
                # If minority opinion exists and differs from consensus, use it for direction
                if minority_prob is not None:
                    if minority_prob > consensus_weight:
                        adjusted_prob = final_prob + perturbation
                    else:
                        adjusted_prob = final_prob - perturbation
                else:
                    # Default: random direction (but deterministic based on prob value)
                    adjusted_prob = final_prob + perturbation if (final_prob % 0.02) > 0.01 else final_prob - perturbation
                
                # Clip to [0, 100]
                adjusted_prob = max(0.0, min(100.0, adjusted_prob))
                
                return {
                    "adjusted_prob": adjusted_prob,
                    "original_prob": final_prob,
                    "perturbation": perturbation
                }
        
        return None  # No penalty applied

    @staticmethod
    def _safe_shorten_text(text: str, limit: int = 800) -> str:
        text = (text or "").strip()
        if len(text) <= limit:
            return text
        sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ.!?])\s+', text)
        shortened = []
        total = 0
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            if total + len(sentence) > limit:
                break
            shortened.append(sentence)
            total += len(sentence)
        result = " ".join(shortened).strip()
        if not result:
            result = text[:limit]
        if not result.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?')):
            result = result.rstrip('â€¦') + "..."
        return result

    @staticmethod
    def _safe_float(value: Optional[float], default: float = 0.0) -> float:
        """Convert value to float safely with default fallback."""
        try:
            if value is None:
                return default
            return float(value)
        except (TypeError, ValueError):
            return default

    @staticmethod
    def _mean(values: List[float]) -> float:
        """Return arithmetic mean with empty guard."""
        if not values:
            return 0.0
        return math.fsum(values) / len(values)

    @classmethod
    def _std(cls, values: List[float]) -> float:
        """Return population standard deviation using safe math."""
        if not values:
            return 0.0
        mean_value = cls._mean(values)
        variance = math.fsum((val - mean_value) ** 2 for val in values) / len(values)
        return math.sqrt(variance)

    @classmethod
    def _weighted_mean(cls, values: List[float], weights: List[float]) -> float:
        """Compute weighted mean with graceful fallback."""
        if not values:
            return 0.0
        total_weight = math.fsum(weights) if weights else 0.0
        if total_weight <= 0:
            return cls._mean(values)
        weighted_sum = math.fsum(val * weight for val, weight in zip(values, weights))
        return weighted_sum / total_weight

    @classmethod
    def _weighted_std(cls, values: List[float], weights: List[float], mean_value: float) -> float:
        """Compute weighted standard deviation with safe defaults."""
        if not values:
            return 0.0
        total_weight = math.fsum(weights) if weights else 0.0
        if total_weight <= 0:
            # Fall back to unweighted std
            return cls._std(values)
        variance = math.fsum(
            weight * ((val - mean_value) ** 2)
            for val, weight in zip(values, weights)
        ) / total_weight
        variance = max(variance, 0.0)
        return math.sqrt(variance)

    @staticmethod
    def evaluate_trade_signal(
        ai_prob: Optional[float],
        market_prob: Optional[float],
        days_to_resolution: Optional[int],
        event_uncertainty: Optional[float],
        volatility_coefficient: float = 0.01
    ) -> Dict[str, Union[str, float]]:
        """Evaluate a simple trade signal using EV and risk heuristics."""
        ai_val = FusionEngine._safe_float(ai_prob)
        market_val = FusionEngine._safe_float(market_prob)
        days_raw = days_to_resolution if days_to_resolution and days_to_resolution > 0 else 1
        # [FIX] Normalize resolution days with float guard for EV/risk calculations.
        try:
            days = float(days_raw or 1.0)
        except (TypeError, ValueError):
            print("[FIX] Invalid days_to_resolution for trade signal; defaulting to 1.0")
            days = 1.0
        days = max(1.0, days)
        ev = ai_val - market_val
        annualized_ev = 0.0
        try:
            annualized_ev = ev / max((days / 365.0), 0.001)
        except Exception:
            annualized_ev = 0.0
        uncertainty = FusionEngine._safe_float(event_uncertainty)
        # [FIX] Use sanitized day value so risk factor remains stable for string inputs.
        risk_factor = uncertainty + (days * volatility_coefficient)

        signal = "HOLD"
        signal_reason = "Await better edge"
        if annualized_ev > 0.05 and risk_factor < 0.6:
            signal = "BUY"
            signal_reason = f"Positive EV ({annualized_ev:.2f}%) with low risk ({risk_factor:.2f})"
        elif annualized_ev < -0.05 or risk_factor > 0.8:
            signal = "SELL"
            if annualized_ev < -0.05:
                signal_reason = f"Negative EV ({annualized_ev:.2f}%), market overpriced"
            else:
                signal_reason = f"High risk factor ({risk_factor:.2f}), avoid position"
        print(
            f"[TRADE] Signal={signal}, EV={(ev or 0.0):.3f}, "
            f"Annualized={(annualized_ev or 0.0):.3f}, Risk={(risk_factor or 0.0):.2f}"
        )
        return {
            "signal": signal,
            "ev": round(ev or 0.0, 4),
            "annualized_ev": round(annualized_ev or 0.0, 4),
            "risk_factor": round(risk_factor or 0.0, 3),
            "signal_reason": signal_reason
        }
    
    def _apply_calibration(self, prob: float, method: str) -> Optional[float]:
        """
        åº”ç”¨åæ ¡å‡†ï¼ˆbinning æˆ– platt scalingï¼‰ã€‚
        
        Args:
            prob: åŸå§‹æ¦‚ç‡å€¼ (0-100)
            method: æ ¡å‡†æ–¹æ³• ("binning" or "platt")
        
        Returns:
            æ ¡å‡†åçš„æ¦‚ç‡å€¼ï¼Œå¦‚æœæ ¡å‡†æœªå¯ç”¨æˆ–å¤±è´¥åˆ™è¿”å› None
        """
        if method == "none" or self.calibration_map is None:
            return None  # No calibration configured
        
        try:
            if method == "binning":
                # Equal-frequency binning calibration
                # calibration_map should be a dict: {bin_index: calibrated_prob}
                # For simplicity, use linear interpolation if calibration_map is a function
                if callable(self.calibration_map):
                    return self.calibration_map(prob)
                elif isinstance(self.calibration_map, dict):
                    # Simple bin-based lookup (would need proper binning logic)
                    # For now, return original (proper implementation would need historical data)
                    return None
                else:
                    return None
            
            elif method == "platt":
                # Platt scaling (logistic regression calibration)
                # calibration_map should be fitted logistic regression parameters
                # For now, placeholder - would need historical data to fit
                return None
            
            return None
        except Exception as e:
            print(f"âš ï¸ æ ¡å‡†åº”ç”¨å¤±è´¥: {e}")
            return None
    
    def fit_calibration(self, historical_data: Optional[List[Tuple[float, int]]] = None):
        """
        ä½¿ç”¨å†å²æ•°æ®æ‹Ÿåˆæ ¡å‡†æ¨¡å‹ï¼ˆbinning æˆ– platt scalingï¼‰ã€‚
        
        Args:
            historical_data: List of (predicted_prob, true_label) tuples
        """
        # Placeholder for calibration fitting
        # This would be implemented when historical data is available
        calibration_method = self._get_calibration_method()
        if calibration_method != "none" and historical_data:
            # Fit calibration model here
            # For now, calibration_map remains None
            pass
