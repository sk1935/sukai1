"""
ä¸»ç¨‹åºï¼ˆMainï¼‰ï¼š
æ ¹æ® OPTIMIZATION_NOTES.md çš„äº”å±‚æ¶æ„è®¾è®¡

ç³»ç»Ÿæ¶æ„ï¼š
1. äº‹ä»¶å±‚ï¼ˆEventManagerï¼‰â†’ è§£æè¾“å…¥ã€è·å–å¸‚åœºæ•°æ®
2. æç¤ºå±‚ï¼ˆPromptBuilderï¼‰â†’ ç”Ÿæˆæ¨¡å‹æç¤ºè¯
3. æ¨ç†å±‚ï¼ˆModelOrchestratorï¼‰â†’ å¹¶å‘è°ƒç”¨å¤šä¸ªAIæ¨¡å‹
4. èåˆå±‚ï¼ˆFusionEngineï¼‰â†’ åŠ æƒèåˆé¢„æµ‹ç»“æœ
5. è¾“å‡ºå±‚ï¼ˆOutputFormatterï¼‰â†’ æ ¼å¼åŒ–ä¸­æ–‡æŠ¥å‘Š

æœ¬æ¨¡å—è´Ÿè´£åè°ƒäº”å±‚æ¶æ„ï¼Œå®ç°å®Œæ•´çš„é¢„æµ‹æµç¨‹ã€‚
"""
import asyncio
import inspect
import os
from dotenv import load_dotenv
import signal
import sys
from pathlib import Path
from types import SimpleNamespace

TELEGRAM_AVAILABLE = True
TELEGRAM_BACKEND = "application"

# ã€ä¿®å¤ã€‘åœ¨å¯¼å…¥ telegram.ext ä¹‹å‰ä¿®è¡¥ apscheduler çš„æ—¶åŒºé—®é¢˜
# Python 3.13 çš„ zoneinfo ä¸ apscheduler ä¸å…¼å®¹
try:
    import pytz
    import apscheduler.util
    # åˆ›å»º pytz æ—¶åŒºå¯¹è±¡
    try:
        default_tz = pytz.timezone('Asia/Shanghai')
    except:
        default_tz = pytz.UTC
    
    # ä¿®è¡¥ astimezone å‡½æ•°ï¼Œè®©å®ƒæ¥å— zoneinfo æ—¶åŒºå¹¶è½¬æ¢ä¸º pytz
    original_astimezone = apscheduler.util.astimezone
    def patched_astimezone(tz):
        if tz is None:
            return default_tz
        # å¦‚æœå·²ç»æ˜¯ pytz æ—¶åŒºï¼Œç›´æ¥è¿”å›
        if isinstance(tz, pytz.BaseTzInfo):
            return tz
        # å¦‚æœæ˜¯ zoneinfo æ—¶åŒºï¼Œè½¬æ¢ä¸º pytz
        try:
            from zoneinfo import ZoneInfo
            if isinstance(tz, ZoneInfo):
                # è·å–æ—¶åŒºåç§°
                tz_name = str(tz).split('/')[-1] if '/' in str(tz) else str(tz)
                # å°è¯•è½¬æ¢ä¸º pytz æ—¶åŒº
                try:
                    return pytz.timezone(tz_name)
                except:
                    return default_tz
        except:
            pass
        # å…¶ä»–æƒ…å†µï¼Œå°è¯•åŸå§‹å‡½æ•°
        try:
            return original_astimezone(tz)
        except:
            return default_tz
    
    apscheduler.util.astimezone = patched_astimezone
    print(f"âœ… å·²ä¿®è¡¥ apscheduler æ—¶åŒºå‡½æ•°ï¼Œé»˜è®¤æ—¶åŒº: {default_tz}")
except Exception as e:
    print(f"âš ï¸ ä¿®è¡¥ apscheduler æ—¶åŒºå‡½æ•°å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()

try:
    from telegram import Update
    from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes
except ImportError:
    try:
        from telegram import Update
        from telegram.ext import Updater, CommandHandler, MessageHandler, Filters, CallbackContext

        TELEGRAM_BACKEND = "legacy"
        ContextTypes = SimpleNamespace(DEFAULT_TYPE=CallbackContext)
        filters = SimpleNamespace(TEXT=Filters.text, COMMAND=Filters.command)

        class LegacyApplication:
            """Shim to provide Application-like interface on top of Updater."""

            def __init__(self, updater):
                self.updater = updater
                self.dispatcher = updater.dispatcher

            @classmethod
            def builder(cls):
                return LegacyApplicationBuilder()

            def add_handler(self, handler, group=0):
                self.dispatcher.add_handler(handler, group=group)

            def add_error_handler(self, handler):
                self.dispatcher.add_error_handler(handler)

            def run_polling(self, **kwargs):
                self.updater.start_polling()
                self.updater.idle()

        class LegacyApplicationBuilder:
            def __init__(self):
                self._token = None

            def token(self, token):
                self._token = token
                return self

            def build(self):
                if not self._token:
                    raise ValueError("TELEGRAM_BOT_TOKEN æœªé…ç½®")
                updater = Updater(token=self._token, use_context=True)
                return LegacyApplication(updater)

        Application = LegacyApplication  # type: ignore
    except ImportError as telegram_import_err:
        TELEGRAM_AVAILABLE = False
        TELEGRAM_IMPORT_ERROR = telegram_import_err
        
        class _DummyUpdate:
            ALL_TYPES = []
        Update = _DummyUpdate  # type: ignore
        
        class _DummyContextTypes:
            DEFAULT_TYPE = object
        ContextTypes = _DummyContextTypes  # type: ignore
        
        class _DummyFilters:
            TEXT = object()
            COMMAND = object()
        filters = _DummyFilters()
        
        class Application:  # type: ignore
            @classmethod
            def builder(cls):
                raise RuntimeError("Telegram dependency unavailable")
        
        class CommandHandler:  # type: ignore
            def __init__(self, *args, **kwargs):
                raise RuntimeError("Telegram dependency unavailable")
        
        class MessageHandler:  # type: ignore
            def __init__(self, *args, **kwargs):
                raise RuntimeError("Telegram dependency unavailable")

# Ensure local src imports always resolve
sys.path.insert(0, str(Path(__file__).parent))
sys.path.insert(0, str(Path(__file__).parent.parent))

from event_manager import EventManager
from prompt_builder import PromptBuilder
from model_orchestrator import ModelOrchestrator
from fusion_engine import FusionEngine
from output_formatter import OutputFormatter
from event_analyzer import EventAnalyzer
from notion_logger import NotionLogger

# OpenRouter é›†æˆ
try:
    from services.llm_clients.openrouter_layer import (
        call_openrouter_model,
        call_multiple_openrouter_models,
        get_available_models as get_openrouter_models,
        is_openrouter_available
    )
    OPENROUTER_INTEGRATION_AVAILABLE = True
except Exception as import_err:
    print(f"âš ï¸ OpenRouter é›†æˆä¸å¯ç”¨: {import_err}")
    OPENROUTER_INTEGRATION_AVAILABLE = False
    
    async def call_openrouter_model(*args, **kwargs):
        raise RuntimeError("OpenRouter integration disabled")
    
    async def call_multiple_openrouter_models(*args, **kwargs):
        return {}
    
    def get_openrouter_models():
        return []
    
    def is_openrouter_available():
        return False

# LMArena åŠ¨æ€æƒé‡æ›´æ–°
try:
    from config.update_lmarena_weights import update_lmarena_weights, should_update
    LMARENA_UPDATE_AVAILABLE = True
except ImportError:
    print("âš ï¸ LMArena æƒé‡æ›´æ–°æ¨¡å—æœªæ‰¾åˆ°ï¼Œè·³è¿‡è‡ªåŠ¨æ›´æ–°")
    LMARENA_UPDATE_AVAILABLE = False

load_dotenv()


async def maybe_await(result):
    """Await result if it is awaitable, otherwise return it directly."""
    if inspect.isawaitable(result):
        return await result
    return result


def wrap_async_handler(handler):
    """Wrap async handler for legacy (synchronous) telegram backends."""
    if TELEGRAM_AVAILABLE and TELEGRAM_BACKEND == "legacy" and inspect.iscoroutinefunction(handler):
        def _wrapper(update, context):
            asyncio.run(handler(update, context))
        return _wrapper
    return handler


class ForecastingBot:
    """
    Main bot class that orchestrates all components.
    
    é¢„æµ‹æœºå™¨äººä¸»ç±»ï¼š
    - åè°ƒäº”å±‚æ¶æ„çš„å®Œæ•´æµç¨‹
    - å¤„ç† Telegram æ¶ˆæ¯å’Œå‘½ä»¤
    - æ”¯æŒå•é€‰é¡¹å’Œå¤šé€‰é¡¹å¸‚åœºé¢„æµ‹
    - æ‰€æœ‰è¾“å‡ºå‡ä¸ºä¸­æ–‡
    """
    
    def __init__(self):
        self.event_manager = EventManager()
        self.prompt_builder = PromptBuilder()
        self.model_orchestrator = ModelOrchestrator()
        self.fusion_engine = FusionEngine()
        self.output_formatter = OutputFormatter()
        self.event_analyzer = EventAnalyzer()
        
        # åˆå§‹åŒ– Notion Loggerï¼ˆå¦‚æœé…ç½®äº†ç¯å¢ƒå˜é‡ï¼‰
        try:
            self.notion_logger = NotionLogger()
            if self.notion_logger and self.notion_logger.enabled:
                print("âœ… Notion Logger å·²å¯ç”¨ï¼Œé¢„æµ‹ç»“æœå°†è‡ªåŠ¨ä¿å­˜åˆ° Notion")
            elif self.notion_logger:
                print("âš ï¸ Notion Logger å·²åˆ›å»ºä½†æœªå¯ç”¨ï¼ˆè¯·æ£€æŸ¥é…ç½®ï¼‰")
        except Exception as e:
            print(f"âš ï¸ Notion Logger åˆå§‹åŒ–å¼‚å¸¸: {type(e).__name__}: {e}")
            import traceback
            traceback.print_exc()
            self.notion_logger = None
    
    async def handle_predict(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """Handle /predict command."""
        if not update.message:
            print("âš ï¸ handle_predict: No message in update")
            return
        
        message_text = update.message.text or ""
        print(f"\n{'='*60}")
        print(f"ğŸ“¨ æ”¶åˆ°æ¶ˆæ¯: {message_text[:100]}...")
        print(f"{'='*60}")
        
        # Show typing indicator
        try:
            await maybe_await(context.bot.send_chat_action(
                chat_id=update.effective_chat.id,
                action="typing"
            ))
        except Exception as e:
            print(f"âš ï¸ å‘é€typing indicatorå¤±è´¥: {e}")
        
        # ã€é›†æˆã€‘é¢„åŠ è½½æ–°é—»ç¼“å­˜ï¼ˆå¦‚æœå¯ç”¨ï¼ŒåŒæ­¥ç­‰å¾…ä»¥ç¡®ä¿æ•°æ®å¯ç”¨ï¼‰
        # ã€ç¨³å®šæ€§ä¿æŠ¤ã€‘æ·»åŠ å®Œæ•´çš„å¼‚å¸¸å¤„ç†å’Œæ—¥å¿—
        try:
            from src.news_cache import fetch_and_cache_news, NEWS_CACHE_ENABLED
            if NEWS_CACHE_ENABLED:
                print("ğŸ“° [NEWS_CACHE] å¼€å§‹é¢„åŠ è½½æ–°é—»ç¼“å­˜...")
                try:
                    await asyncio.wait_for(
                        fetch_and_cache_news(keyword="", force_refresh=False),
                        timeout=15.0  # æœ€å¤šç­‰å¾…15ç§’
                    )
                    print("âœ… [NEWS_CACHE] æ–°é—»ç¼“å­˜é¢„åŠ è½½å®Œæˆ")
                except asyncio.TimeoutError:
                    print("â±ï¸ [NEWS_CACHE] é¢„åŠ è½½è¶…æ—¶ï¼Œç»§ç»­æ‰§è¡Œï¼ˆä½¿ç”¨æ—§ç¼“å­˜ï¼‰")
                except Exception as e:
                    print(f"âš ï¸ [NEWS_CACHE] é¢„åŠ è½½å¤±è´¥: {type(e).__name__}: {e}ï¼Œç»§ç»­æ‰§è¡Œ")
            else:
                print("â„¹ï¸ [NEWS_CACHE] åŠŸèƒ½æœªå¯ç”¨ï¼Œè·³è¿‡é¢„åŠ è½½")
        except ImportError as e:
            print(f"âš ï¸ [NEWS_CACHE] æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        except Exception as e:
            print(f"âš ï¸ [NEWS_CACHE] é¢„åŠ è½½å¼‚å¸¸: {type(e).__name__}: {e}")
        
        try:
            # Parse event
            event_info = self.event_manager.parse_event_from_message(message_text)
            
            if not event_info.get('query'):
                await maybe_await(update.message.reply_text(
                    "è¯·æä¾›è¦é¢„æµ‹çš„äº‹ä»¶ã€‚\n"
                    "ç”¨æ³•: /predict <äº‹ä»¶æè¿°>\n"
                    "æˆ–: /predict <Polymarketé“¾æ¥>"
                ))
                return
            
            # Fetch Polymarket data
            await maybe_await(update.message.reply_text("ğŸ” æ­£åœ¨è·å–å¸‚åœºæ•°æ®..."))
            print(f"ğŸ” å¼€å§‹è·å–å¸‚åœºæ•°æ®ï¼Œevent_info: {event_info}")
            try:
                event_data = await asyncio.wait_for(
                    self.event_manager.fetch_polymarket_data(event_info),
                    timeout=25.0  # å‡å°‘åˆ°25ç§’ï¼ŒåŠ å¿«å¤±è´¥æ¢å¤
                )
                if event_data:
                    print(f"âœ… æˆåŠŸè·å–å¸‚åœºæ•°æ®:")
                    print(f"  question: {event_data.get('question', 'N/A')[:80]}")
                    print(f"  market_prob: {event_data.get('market_prob', 'N/A')}")
                    print(f"  is_mock: {event_data.get('is_mock', False)}")
                else:
                    print(f"âš ï¸ event_data ä¸º None")
            except asyncio.TimeoutError:
                print(f"â±ï¸ è·å–å¸‚åœºæ•°æ®è¶…æ—¶")
                await maybe_await(update.message.reply_text(
                    "â±ï¸ è·å–å¸‚åœºæ•°æ®è¶…æ—¶ï¼Œå°†ä½¿ç”¨ AI æ¨¡å‹è¿›è¡Œé¢„æµ‹ã€‚",
                    parse_mode="Markdown"
                ))
                # Create mock data
                event_data = self.event_manager._create_mock_market_data(event_info.get('query', ''))
                event_data["is_mock"] = True
            
            if not event_data:
                print(f"âŒ æœªèƒ½è·å–å¸‚åœºæ•°æ®ï¼Œåˆ›å»ºmockæ•°æ®")
                await maybe_await(update.message.reply_text(
                    self.output_formatter.format_error(
                        "è·å–å¸‚åœºæ•°æ®å¤±è´¥ï¼Œå°†ä½¿ç”¨ AI æ¨¡å‹è¿›è¡Œé¢„æµ‹ã€‚"
                    ),
                    parse_mode="Markdown"
                ))
                # Create mock data to continue
                event_data = self.event_manager._create_mock_market_data(event_info.get('query', ''))
                event_data["is_mock"] = True
            
            # å…¨é¢åˆ†æäº‹ä»¶ï¼ˆåŒ…å«å¸‚åœºè¶‹åŠ¿ã€äº‹ä»¶ç±»åˆ«ã€èˆ†æƒ…ä¿¡å·ã€è§„åˆ™æ‘˜è¦ï¼‰
            # æ·»åŠ è¶…æ—¶ä¿æŠ¤ï¼Œé¿å…åˆ†ææ­¥éª¤é˜»å¡ä¸»æµç¨‹
            market_slug = event_info.get('slug')
            try:
                full_analysis = await asyncio.wait_for(
                    self.event_analyzer.analyze_event_full(
                        event_title=event_data.get("question", ""),
                        event_rules=event_data.get("rules", ""),
                        market_prob=event_data.get("market_prob"),
                        market_slug=market_slug
                    ),
                    timeout=15.0  # åˆ†ææ­¥éª¤æœ€å¤šç­‰å¾…15ç§’
                )
            except asyncio.TimeoutError:
                print(f"â±ï¸ [WARNING] äº‹ä»¶åˆ†æè¶…æ—¶ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                # ä½¿ç”¨é»˜è®¤åˆ†æç»“æœï¼Œä¸é˜»å¡ä¸»æµç¨‹
                full_analysis = {
                    "event_category": "general",
                    "event_category_display": "é€šç”¨",
                    "market_trend": "æ•°æ®ä¸è¶³ï¼Œæ— æ³•è®¡ç®—",
                    "sentiment_trend": "unknown",
                    "sentiment_score": 0.0,
                    "sentiment_sample": 0,
                    "sentiment_source": "æœªçŸ¥",
                    "rules_summary": event_data.get("rules", "")[:100] if event_data.get("rules") else "æ— è§„åˆ™ä¿¡æ¯"
                }
            
            # æ‰“å°åˆ†æç»“æœ
            print(f"\nğŸ“Š äº‹ä»¶å…¨é¢åˆ†æ:")
            print(f"  ç±»åˆ«: {full_analysis['event_category']} ({full_analysis.get('event_category_display', 'æœªçŸ¥')})")
            print(f"  å¸‚åœºè¶‹åŠ¿: {full_analysis['market_trend']}")
            # ã€é˜²å¾¡ã€‘ç¡®ä¿ sentiment_score ä¸ä¸º None
            sentiment_score = full_analysis.get('sentiment_score') or 0.0
            if sentiment_score is None:
                print("âš ï¸ sentiment_score is None, using default 0.0")
                sentiment_score = 0.0
            print(f"  èˆ†æƒ…: {full_analysis['sentiment_trend']} ({(sentiment_score or 0.0):+.2f}), "
                  f"æ ·æœ¬: {full_analysis['sentiment_sample']} ({full_analysis['sentiment_source']})")
            print(f"  è§„åˆ™æ‘˜è¦: {full_analysis['rules_summary'][:60]}...")
            
            # æ‰“å°ä¸–ç•Œæ¸©åº¦ä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰- è½»é‡æè¿°æ¨¡å¼
            world_temp_data = full_analysis.get("world_temp_data")
            if world_temp_data:
                description = world_temp_data.get("description", "æœªçŸ¥")
                positive = world_temp_data.get("positive", 0)
                negative = world_temp_data.get("negative", 0)
                neutral = world_temp_data.get("neutral", 0)
                print(f"  ğŸ§  ä¸–ç•Œæƒ…ç»ª: {description}ï¼ˆæ­£é¢: {positive}, è´Ÿé¢: {negative}, ä¸­æ€§: {neutral}ï¼‰")
            elif full_analysis.get("world_sentiment_summary"):
                print(f"  ğŸ§  ä¸–ç•Œæƒ…ç»ª: {full_analysis['world_sentiment_summary'][:80]}...")
            
            # å¼‚æ­¥è·å–æ–°é—»æ‘˜è¦ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            # ã€ç¨³å®šæ€§ä¿æŠ¤ã€‘æ·»åŠ è¶…æ—¶å’Œå¼‚å¸¸å¤„ç†
            news_summary = None
            try:
                from src.openrouter_assistant import get_news_summary, OPENROUTER_ASSISTANT_ENABLED
                if OPENROUTER_ASSISTANT_ENABLED:
                    news_summary = await asyncio.wait_for(
                        get_news_summary(),
                        timeout=10.0  # æœ€å¤šç­‰å¾…10ç§’
                    )
                    if news_summary:
                        print(f"  ğŸ“° æ–°é—»æ‘˜è¦: å·²è·å–ï¼ˆ{len(news_summary)} å­—ç¬¦ï¼‰")
                else:
                    print("  â„¹ï¸ [OPENROUTER] åŠŸèƒ½æœªå¯ç”¨ï¼Œè·³è¿‡æ–°é—»æ‘˜è¦")
            except asyncio.TimeoutError:
                print(f"  â±ï¸ [OPENROUTER] è·å–æ–°é—»æ‘˜è¦è¶…æ—¶ï¼ˆ>10sï¼‰ï¼Œè·³è¿‡")
                news_summary = None
            except ImportError as e:
                print(f"  âš ï¸ [OPENROUTER] æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
                news_summary = None
            except Exception as e:
                print(f"  âš ï¸ [OPENROUTER] è·å–æ–°é—»æ‘˜è¦å¤±è´¥: {type(e).__name__}: {e}")
                news_summary = None
            
            # ä¿ç•™åŸæœ‰çš„äº‹ä»¶åˆ†æï¼ˆç”¨äºæ¨¡å‹åˆ†é…ï¼‰
            # Pass orchestrator to auto-fetch enabled models
            event_analysis = self.event_analyzer.analyze_event(
                event_data.get("question", ""),
                event_data.get("rules", ""),
                available_models=None,  # è®© analyze_event è‡ªåŠ¨ä» orchestrator è·å–
                orchestrator=self.model_orchestrator
            )
            
            print(f"\nğŸ“Š Event Category: {event_analysis['category']}")
            print(f"ğŸ“ Dimensions: {len(event_analysis['dimensions'])}")
            
            # Get model assignments from analysis
            model_assignments = event_analysis["model_assignments"]
            
            # å°†å…¨é¢åˆ†æç»“æœæ·»åŠ åˆ°event_dataä¸­ï¼Œä¾›PromptBuilderå’ŒOutputFormatterä½¿ç”¨
            event_data["full_analysis"] = full_analysis
            # ã€é›†æˆã€‘æ·»åŠ ä¸–ç•Œæ¸©åº¦å’Œæ–°é—»æ‘˜è¦åˆ° event_dataï¼ˆä¾› prompt_builder å’Œ output_formatter ä½¿ç”¨ï¼‰
            # è½»é‡æè¿°æ¨¡å¼ï¼šworld_temp ç°åœ¨æ˜¯æè¿°å­—ç¬¦ä¸²
            event_data["world_temp"] = full_analysis.get("world_temp")  # æè¿°å­—ç¬¦ä¸²
            event_data["world_temp_data"] = full_analysis.get("world_temp_data")  # å®Œæ•´æ•°æ®
            event_data["world_sentiment_summary"] = full_analysis.get("world_sentiment_summary")
            event_data["news_summary"] = news_summary
            
            # Get available models (only those with API keys)
            all_models = list(model_assignments.keys())
            model_names = [
                model for model in all_models
                if model in self.model_orchestrator.get_available_models()
            ]
            
            if not model_names:
                await maybe_await(update.message.reply_text(
                    self.output_formatter.format_error(
                        "æ²¡æœ‰å¯ç”¨çš„ AI æ¨¡å‹ã€‚è¯·è‡³å°‘é…ç½®ä¸€ä¸ª API å¯†é’¥ã€‚"
                    ),
                    parse_mode="Markdown"
                ))
                return
            
            # Check if this is a multi-option event
            if event_data.get("is_multi_option", False):
                # Multi-option event: predict each option separately
                outcomes = event_data.get("outcomes", [])
                print(f"\nğŸ¯ å¤šé€‰é¡¹äº‹ä»¶æ£€æµ‹:")
                print(f"  is_multi_option: {event_data.get('is_multi_option', False)}")
                print(f"  outcomesæ•°é‡: {len(outcomes)}")
                if len(outcomes) == 0:
                    print(f"  âš ï¸ è­¦å‘Šï¼šå¤šé€‰é¡¹äº‹ä»¶ä½†outcomesä¸ºç©ºï¼")
                    print(f"  event_data keys: {list(event_data.keys())}")
                    # Try to reconstruct from markets if available
                    # This shouldn't happen, but let's add a fallback
                else:
                    print(f"  âœ… å‰3ä¸ªé€‰é¡¹: {[o.get('name', 'N/A') for o in outcomes[:3]]}")
                
                await maybe_await(update.message.reply_text(
                    f"ğŸ” æ£€æµ‹åˆ°å¤šé€‰é¡¹äº‹ä»¶ï¼Œå…±æœ‰ {len(outcomes)} ä¸ªé€‰é¡¹\n"
                    f"ğŸ¤– æ­£åœ¨ä¸ºæ¯ä¸ªé€‰é¡¹è¿›è¡Œé¢„æµ‹..."
                ))
                
                # Sequentially call models for each outcome
                outcome_predictions = {}
                for outcome in outcomes:
                    outcome_name = outcome["name"]
                    print(f"\nğŸ¯ å¤„ç†é€‰é¡¹: {outcome_name}")
                    prompts = {}
                    for model_name in model_names:
                        assignment = model_assignments.get(model_name)
                        option_event_data = event_data.copy()
                        option_event_data["question"] = f"{event_data.get('question', '')} - {outcome_name}"
                        option_event_data["market_prob"] = outcome["market_prob"]
                        prompt = self.prompt_builder.build_prompt(
                            option_event_data,
                            model_name,
                            model_assignment=assignment
                        )
                        prompts[model_name] = prompt
                        if assignment:
                            print(f"   - {model_name}: {assignment['dimension_name']}")
                    if not prompts:
                        print("   âš ï¸ æ— å¯ç”¨æ¨¡å‹ï¼Œä½¿ç”¨å¸‚åœºä»·æ ¼")
                        outcome_predictions[outcome_name] = {}
                        continue
                    
                    # ã€Bugä¿®å¤ã€‘å¢åŠ é‡è¯•æœºåˆ¶
                    max_retries = 2
                    model_results = {}
                    timeout = min(
                        self.model_orchestrator.MAX_TOTAL_WAIT_TIME,
                        30.0  # æ¯ä¸ªé€‰é¡¹æœ€å¤š30ç§’
                    )
                    
                    for retry in range(max_retries):
                        try:
                            print(f"ğŸ“¤ è°ƒç”¨ {len(prompts)} ä¸ªæ¨¡å‹ï¼ˆå°è¯• {retry + 1}/{max_retries}ï¼‰")
                            model_results = await asyncio.wait_for(
                                self.model_orchestrator.call_all_models(prompts),
                                timeout=timeout
                            )
                            success_count = sum(1 for r in model_results.values() if r)
                            
                            # å¦‚æœæœ‰æˆåŠŸçš„ç»“æœï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                            if success_count > 0:
                                break
                            
                            # å¦‚æœæ²¡æœ‰æˆåŠŸç»“æœä¸”è¿˜æœ‰é‡è¯•æœºä¼šï¼Œç­‰å¾…åé‡è¯•
                            if retry < max_retries - 1 and success_count == 0:
                                print(f"  âš ï¸ {outcome_name} é¦–æ¬¡è°ƒç”¨æ— ç»“æœï¼Œç­‰å¾… 1 ç§’åé‡è¯•...")
                                await asyncio.sleep(1)
                                continue
                            
                        except asyncio.TimeoutError:
                            if retry < max_retries - 1:
                                print(f"  â±ï¸ {outcome_name} è¶…æ—¶ï¼ˆ>{timeout}sï¼‰ï¼Œé‡è¯• {retry + 1}/{max_retries}...")
                                await asyncio.sleep(1)  # ç­‰å¾…1ç§’åé‡è¯•
                                continue
                            else:
                                print(f"  â±ï¸ [ERROR] {outcome_name} é‡è¯•åä»è¶…æ—¶ï¼ˆ>{timeout}sï¼‰ï¼Œä½¿ç”¨å¸‚åœºä»·æ ¼")
                                model_results = {}
                                break
                        except Exception as e:
                            if retry < max_retries - 1:
                                print(f"  âš ï¸ {outcome_name} è°ƒç”¨å¼‚å¸¸ ({type(e).__name__})ï¼Œé‡è¯• {retry + 1}/{max_retries}...")
                                await asyncio.sleep(1)
                                continue
                            else:
                                print(f"  âŒ [ERROR] {outcome_name} é‡è¯•åä»å¼‚å¸¸: {type(e).__name__}: {e}")
                                model_results = {}
                                break
                    
                    # ã€Bugä¿®å¤ã€‘è°ƒç”¨ OpenRouter å…è´¹æ¨¡å‹ä½œä¸ºè¾…åŠ©å±‚ï¼ˆå¤šé€‰é¡¹äº‹ä»¶ï¼‰
                    if OPENROUTER_INTEGRATION_AVAILABLE and is_openrouter_available():
                        openrouter_models = get_openrouter_models()
                        # åªè°ƒç”¨ç¬¬ä¸€ä¸ªæ¨¡å‹ï¼ˆå¤šé€‰é¡¹äº‹ä»¶æ—¶å‡å°‘è°ƒç”¨ï¼‰
                        if openrouter_models:
                            selected_model = openrouter_models[0]
                            # ä½¿ç”¨å½“å‰é€‰é¡¹çš„ prompt
                            option_prompt = list(prompts.values())[0] if prompts else ""
                            
                            try:
                                openrouter_result = await asyncio.wait_for(
                                    call_openrouter_model(selected_model, option_prompt),
                                    timeout=25.0
                                )
                                
                                if openrouter_result:
                                    display_name = selected_model.split('/')[-1]
                                    model_results[f"openrouter_{display_name}"] = openrouter_result
                                    print(f"âœ… [OpenRouter] {display_name} è°ƒç”¨æˆåŠŸï¼ˆ{outcome_name}ï¼‰")
                                
                            except Exception as e:
                                print(f"âš ï¸ [OpenRouter] {outcome_name} è°ƒç”¨å¼‚å¸¸: {type(e).__name__}")
                    
                    # ã€Bugä¿®å¤ã€‘è®¡ç®—æˆåŠŸæ•°é‡ï¼ˆåŒ…æ‹¬ OpenRouterï¼‰
                    success_count = sum(1 for r in model_results.values() if r)
                    expected_count = len(prompts) + (
                        1 if OPENROUTER_INTEGRATION_AVAILABLE and is_openrouter_available() and get_openrouter_models() else 0
                    )
                    print(f"ğŸ“¥ {outcome_name} æ”¶åˆ° {success_count}/{expected_count} ä¸ªæ¨¡å‹å“åº”")
                    
                    # ã€Bugä¿®å¤ã€‘å¢å¼ºè°ƒè¯•æ—¥å¿—
                    if success_count == 0:
                        print(f"  âš ï¸ [WARNING] {outcome_name} æ‰€æœ‰æ¨¡å‹è°ƒç”¨å¤±è´¥ï¼Œå°†ä½¿ç”¨å¸‚åœºä»·æ ¼")
                        print(f"  [DEBUG] æ¨¡å‹ç»“æœè¯¦æƒ…: {model_results}")
                        print(f"  [DEBUG] æ˜¯å¦æœ‰ç»“æœ: {bool(model_results)}, ç»“æœæ•°é‡: {len(model_results)}")
                    else:
                        print(f"  âœ… {outcome_name} æˆåŠŸè·å¾— {success_count} ä¸ªæ¨¡å‹å“åº”")
                    
                    outcome_predictions[outcome_name] = model_results
                    await asyncio.sleep(0.5)
                
                # Fuse predictions for each outcome
                model_weights = {
                    model_name: self.model_orchestrator.get_model_weight(model_name)
                    for model_name in model_names
                }
                
                fused_outcomes = []
                # æ”¶é›† DeepSeek reasoningï¼ˆæ‰€æœ‰ outcome å…±äº«ï¼‰
                deepseek_reasoning = None
                
                for outcome in outcomes:
                    outcome_name = outcome["name"]
                    outcome_results = outcome_predictions.get(outcome_name, {})
                    
                    # ã€Bugä¿®å¤ã€‘æ”¹è¿›ç©ºç»“æœåˆ¤æ–­ï¼šæ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„ï¼ˆé Noneï¼‰æ¨¡å‹ç»“æœ
                    # å…³é”®ä¿®å¤ï¼šä¸ä»…æ£€æŸ¥å­—å…¸é•¿åº¦ï¼Œè¿˜è¦æ£€æŸ¥å€¼çš„æœ‰æ•ˆæ€§
                    valid_count = sum(1 for r in outcome_results.values() if r is not None)
                    
                    if valid_count > 0:
                        # æœ‰æœ‰æ•ˆçš„æ¨¡å‹ç»“æœï¼Œè¿›è¡Œèåˆ
                        fusion_result = self.fusion_engine.fuse_predictions(
                            model_results=outcome_results,
                            model_weights=model_weights,
                            market_prob=outcome["market_prob"],
                            orchestrator=self.model_orchestrator  # Pass orchestrator for version info
                        )
                        # æå– DeepSeek reasoningï¼ˆç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„ï¼‰
                        if not deepseek_reasoning and fusion_result.get("deepseek_reasoning"):
                            deepseek_reasoning = fusion_result.get("deepseek_reasoning")
                        
                        fused_outcomes.append({
                            "name": outcome_name,
                            "prediction": fusion_result["final_prob"],
                            "market_prob": outcome["market_prob"],
                            "uncertainty": fusion_result["uncertainty"],
                            "summary": fusion_result["summary"],
                            "model_only_prob": fusion_result.get("model_only_prob"),  # ä¿å­˜çº¯AIé¢„æµ‹
                            "model_versions": fusion_result.get("model_versions", {}),  # ä¿å­˜æ¨¡å‹ç‰ˆæœ¬ä¿¡æ¯
                            "weight_source": fusion_result.get("weight_source", {}),  # ä¿å­˜æƒé‡æ¥æºä¿¡æ¯
                            "deepseek_reasoning": fusion_result.get("deepseek_reasoning")  # ä¿å­˜ DeepSeek reasoning
                        })
                        # ã€é˜²å¾¡ã€‘ç¡®ä¿ final_prob ä¸ä¸º None
                        final_prob = fusion_result.get('final_prob') or 0.0
                        if final_prob is None:
                            print(f"âš ï¸ final_prob is None for {outcome_name}, using default 0.0")
                            final_prob = 0.0
                        model_only_prob_display = fusion_result.get('model_only_prob')
                        if model_only_prob_display is None:
                            model_only_prob_display = 'N/A'
                        else:
                            # ã€é˜²å¾¡ã€‘ç¡®ä¿ model_only_prob_display ä¸ä¸º None
                            model_only_prob_display = model_only_prob_display or 0.0
                            if model_only_prob_display is None:
                                print("âš™ï¸ [SAFE] ä¿®å¤ç©ºå€¼ä¿æŠ¤: model_only_prob_display")
                                model_only_prob_display = 0.0
                            model_only_prob_display = f"{(model_only_prob_display or 0.0):.1f}%"
                        print(f"  âœ… èåˆå®Œæˆ: {outcome_name} = {(final_prob or 0.0):.1f}% (AI: {model_only_prob_display})")
                    else:
                        # ã€Bugä¿®å¤ã€‘æ˜ç¡®å¤„ç†ï¼šæ‰€æœ‰æ¨¡å‹è°ƒç”¨å¤±è´¥/è¶…æ—¶ï¼Œä½¿ç”¨å¸‚åœºä»·æ ¼
                        # outcome_results ä¸ºç©ºå­—å…¸ {} æˆ–æ‰€æœ‰å€¼éƒ½æ˜¯ None
                        if not outcome_results:
                            reason = "æ— æ¨¡å‹ç»“æœ"
                        elif valid_count == 0:
                            reason = "æ‰€æœ‰æ¨¡å‹è°ƒç”¨å¤±è´¥/è¶…æ—¶"
                        else:
                            reason = "æ— æœ‰æ•ˆæ¨¡å‹ç»“æœ"
                        
                        fused_outcomes.append({
                            "name": outcome_name,
                            "prediction": outcome["market_prob"],  # Use market prob as default
                            "market_prob": outcome["market_prob"],
                            "uncertainty": 10.0,  # Default uncertainty
                            "summary": f"âš ï¸ {reason}ï¼Œæš‚æ—  AI æ¨¡å‹é¢„æµ‹ï¼Œæ˜¾ç¤ºå¸‚åœºä»·æ ¼",
                            "model_only_prob": None  # æ˜ç¡®æ ‡è®°ä¸º Noneï¼Œè¡¨ç¤ºæ²¡æœ‰AIé¢„æµ‹
                        })
                        # ã€é˜²å¾¡ã€‘ç¡®ä¿ market_prob ä¸ä¸º None
                        market_prob = outcome.get('market_prob') or 0.0
                        if market_prob is None:
                            print(f"âš ï¸ market_prob is None for {outcome_name}, using default 0.0")
                            market_prob = 0.0
                        print(f"  âš ï¸ æ— AIé¢„æµ‹: {outcome_name}ï¼ˆ{reason}ï¼Œæœ‰æ•ˆç»“æœæ•°: {valid_count}ï¼‰ï¼Œä½¿ç”¨å¸‚åœºä»·æ ¼ {(market_prob or 0.0):.1f}%")
                
                # Final safety check: ensure we have at least market data
                # This should never be empty if outcomes exist, but add as ultimate fallback
                if not fused_outcomes:
                    if outcomes and len(outcomes) > 0:
                        print(f"âš ï¸ fused_outcomes ä¸ºç©ºï¼Œä»åŸå§‹ outcomes åˆ›å»º fallback æ•°æ®...")
                        fused_outcomes = [{
                            "name": outcome["name"],
                            "prediction": outcome["market_prob"],
                            "market_prob": outcome["market_prob"],
                            "uncertainty": 10.0,
                            "summary": "æš‚æ—  AI æ¨¡å‹é¢„æµ‹ï¼Œæ˜¾ç¤ºå¸‚åœºä»·æ ¼ã€‚",
                            "model_only_prob": None
                        } for outcome in outcomes]
                        print(f"âœ… åˆ›å»ºäº† {len(fused_outcomes)} ä¸ª fallback outcomes")
                    else:
                        print(f"âŒ ä¸¥é‡é”™è¯¯ï¼šæ—¢æ²¡æœ‰ fused_outcomes ä¹Ÿæ²¡æœ‰ outcomesï¼")
                
                # ã€å…³é”®æ”¹è¿›ã€‘å½’ä¸€åŒ–æ‰€æœ‰ AI é¢„æµ‹æ¦‚ç‡ï¼Œä½¿æ€»å’Œä¸º 100%
                print(f"ğŸ“Š å½’ä¸€åŒ–å‰ fused_outcomes æ•°é‡: {len(fused_outcomes)}")
                # ã€æ–°å¢ã€‘ä¼ é€’äº‹ä»¶æ ‡é¢˜ä»¥è¿›è¡Œäº‹ä»¶ç±»å‹è¯†åˆ«
                event_title = event_data.get("question", "")
                normalization_result = self.fusion_engine.normalize_all_predictions(
                    fused_outcomes,
                    event_title=event_title,
                    event_rules=event_data.get("rules", ""),
                    now_probabilities=[
                        outcome.get("market_prob")
                        for outcome in fused_outcomes
                        if outcome.get("market_prob") is not None
                    ]
                )
                
                fused_outcomes = normalization_result["normalized_outcomes"]
                
                print(f"ğŸ“Š å½’ä¸€åŒ–ç»“æœ:")
                # ã€ä¿®å¤ã€‘ç¡®ä¿å€¼ä¸ä¸º None å†æ ¼å¼åŒ–
                total_before = normalization_result.get('total_before')
                total_after = normalization_result.get('total_after')
                error = normalization_result.get('error', 0)
                skipped_count = normalization_result.get('skipped_count', 0)
                
                try:
                    # ã€é˜²å¾¡ã€‘ç¡®ä¿æ‰€æœ‰å€¼ä¸ä¸º None
                    if total_before is not None:
                        total_before = total_before or 0.0
                        if total_before is None:
                            print("âš™ï¸ [SAFE] ä¿®å¤ç©ºå€¼ä¿æŠ¤: total_before")
                            total_before = 0.0
                        print(f"   å½’ä¸€åŒ–å‰æ€»å’Œ: {float(total_before or 0.0):.2f}%")
                    else:
                        print(f"   å½’ä¸€åŒ–å‰æ€»å’Œ: N/A")
                    
                    if total_after is not None:
                        total_after = total_after or 0.0
                        if total_after is None:
                            print("âš™ï¸ [SAFE] ä¿®å¤ç©ºå€¼ä¿æŠ¤: total_after")
                            total_after = 0.0
                        print(f"   å½’ä¸€åŒ–åæ€»å’Œ: {float(total_after or 0.0):.2f}%")
                    else:
                        print(f"   å½’ä¸€åŒ–åæ€»å’Œ: N/Aï¼ˆæ¡ä»¶äº‹ä»¶æœªå½’ä¸€åŒ–ï¼‰")
                    
                    if error is not None:
                        error = error or 0.0
                        if error is None:
                            print("âš™ï¸ [SAFE] ä¿®å¤ç©ºå€¼ä¿æŠ¤: error")
                            error = 0.0
                        print(f"   è¯¯å·®: {float(error or 0.0):.4f}%")
                    else:
                        print(f"   è¯¯å·®: N/A")
                    
                    print(f"   è·³è¿‡é€‰é¡¹: {skipped_count} ä¸ª")
                except (TypeError, ValueError):
                    print("  âš ï¸ å½’ä¸€åŒ–ç»“æœæ•°æ®æ ¼å¼é”™è¯¯ï¼Œè·³è¿‡æ ¼å¼åŒ–")
                    print(f"   è·³è¿‡é€‰é¡¹: {skipped_count} ä¸ª")
                
                # ã€Bugä¿®å¤ã€‘éªŒè¯ normalization_result çš„å®Œæ•´æ€§
                if normalization_result.get('total_after', 0) == 0 and normalization_result.get('total_before', 0) > 0:
                    print(f"âš ï¸ [WARNING] å½’ä¸€åŒ–å¼‚å¸¸ï¼štotal_before={normalization_result['total_before']}ï¼Œä½† total_after=0")
                print(f"[DEBUG] normalization_result keys: {list(normalization_result.keys())}")
                print(f"[DEBUG] normalization_result['total_after'] = {normalization_result.get('total_after')}")
                
                print(f"ğŸ“Š å½’ä¸€åŒ–å fused_outcomes æ•°é‡: {len(fused_outcomes)}")

                # Compute trade signal from the option with the largest AI-market gap
                trade_signal_info = None
                if fused_outcomes:
                    def _diff_metric(outcome):
                        ai_val = outcome.get("model_only_prob")
                        if ai_val is None:
                            ai_val = outcome.get("prediction", 0.0)
                        return abs((ai_val or 0.0) - (outcome.get("market_prob") or 0.0))
                    top_outcome = max(fused_outcomes, key=_diff_metric)
                    ai_prob_trade = top_outcome.get("model_only_prob")
                    if ai_prob_trade is None:
                        ai_prob_trade = top_outcome.get("prediction")
                    market_prob_trade = top_outcome.get("market_prob")
                    days_to_resolution = event_data.get("days_left") or 30
                    uncertainty_ratio = (top_outcome.get("uncertainty") or 0.0) / 100.0
                    trade_data = self.fusion_engine.evaluate_trade_signal(
                        ai_prob_trade,
                        market_prob_trade,
                        days_to_resolution,
                        uncertainty_ratio
                    )
                    trade_signal_info = {
                        "option": top_outcome.get("name", "N/A"),
                        "data": trade_data
                    }
                    top_outcome["trade_signal"] = trade_data
                    event_data["trade_signal"] = trade_signal_info
                
                # Format multi-option output
                # ä¼ é€’å½’ä¸€åŒ–ç»“æœå’Œ DeepSeek reasoning ç»™è¾“å‡ºå±‚
                output = self.output_formatter.format_multi_option_prediction(
                    event_data=event_data,
                    outcomes=fused_outcomes,
                    normalization_info=normalization_result,  # ä¼ é€’å½’ä¸€åŒ–ä¿¡æ¯
                    fusion_result={"deepseek_reasoning": deepseek_reasoning},  # ä¼ é€’ DeepSeek reasoning
                    trade_signal=trade_signal_info
                )
                
                print(f"ğŸ“¤ å‡†å¤‡å‘é€è¾“å‡ºï¼Œé•¿åº¦: {len(output)} å­—ç¬¦")
                
                await maybe_await(update.message.reply_text(
                    output,
                    parse_mode="Markdown"
                ))
                
                # è®°å½•åˆ° Notionï¼ˆå¤šé€‰é¡¹äº‹ä»¶ï¼‰
                # éœ€è¦æ”¶é›†æ‰€æœ‰é€‰é¡¹çš„èåˆç»“æœä¿¡æ¯
                if self.notion_logger:
                    if not self.notion_logger.enabled:
                        print("âš ï¸ Notion Logger æœªå¯ç”¨ï¼Œè·³è¿‡è®°å½•ï¼ˆå¤šé€‰é¡¹äº‹ä»¶ï¼‰")
                if self.notion_logger and self.notion_logger.enabled:
                    try:
                        # ä»ç¬¬ä¸€ä¸ª outcome ä¸­æå– fusion_result ä¿¡æ¯ï¼ˆæ‰€æœ‰é€‰é¡¹å…±äº«ï¼‰
                        # æˆ–è€…ä½¿ç”¨æœ€è¿‘ä¸€æ¬¡èåˆçš„ç»“æœ
                        # ç”±äºå¤šé€‰é¡¹äº‹ä»¶ä¸­æ¯ä¸ªé€‰é¡¹éƒ½æœ‰ç‹¬ç«‹çš„ fusionï¼Œæˆ‘ä»¬éœ€è¦æ„é€ ä¸€ä¸ªèšåˆçš„ fusion_result
                        aggregated_fusion_result = {
                            "summary": fused_outcomes[0].get("summary", "æš‚æ— æ‘˜è¦") if fused_outcomes else "æš‚æ— æ‘˜è¦",
                            "deepseek_reasoning": deepseek_reasoning,
                            # å°è¯•ä» outcomes ä¸­æå– model_versions å’Œ weight_source
                            "model_versions": None,
                            "weight_source": None
                        }
                        
                        # ä»ç¬¬ä¸€ä¸ª outcome ä¸­æå–æ¨¡å‹ç‰ˆæœ¬ä¿¡æ¯
                        if fused_outcomes and len(fused_outcomes) > 0:
                            first_outcome = fused_outcomes[0]
                            if "model_versions" in first_outcome:
                                aggregated_fusion_result["model_versions"] = first_outcome["model_versions"]
                            if "weight_source" in first_outcome:
                                aggregated_fusion_result["weight_source"] = first_outcome["weight_source"]
                        
                        # ç¡®ä¿ aggregated_fusion_result åŒ…å«å¿…è¦å­—æ®µ
                        # æ·»åŠ  models åˆ—è¡¨
                        if "models" not in aggregated_fusion_result:
                            model_versions = aggregated_fusion_result.get("model_versions", {})
                            if model_versions:
                                aggregated_fusion_result["models"] = [
                                    info.get("display_name", model_id)
                                    for model_id, info in model_versions.items()
                                ]
                            else:
                                aggregated_fusion_result["models"] = model_names if 'model_names' in locals() else []
                        # æ·»åŠ  run_id
                        if "run_id" not in aggregated_fusion_result:
                            import uuid
                            aggregated_fusion_result["run_id"] = str(uuid.uuid4())
                        
                        # ç¡®ä¿ event_data åŒ…å«å¿…è¦å­—æ®µ
                        event_data_for_notion = event_data.copy()
                        # æ·»åŠ  category å­—æ®µ
                        if full_analysis:
                            event_data_for_notion["category"] = full_analysis.get("event_category_display",
                                full_analysis.get("event_category", "-"))
                        # ç¡®ä¿ outcomes å­—æ®µå­˜åœ¨
                        if "outcomes" not in event_data_for_notion and fused_outcomes:
                            event_data_for_notion["outcomes"] = [outcome.get("name", "-") for outcome in fused_outcomes[:1]]
                        
                        self.notion_logger.log_prediction(
                            event_data=event_data_for_notion,
                            fusion_result=aggregated_fusion_result,
                            full_analysis=full_analysis,
                            outcomes=fused_outcomes,
                            normalization_info=normalization_result
                        )
                        if trade_signal_info and trade_signal_info.get("data"):
                            self.notion_logger.log_trade_signal(
                                event_data_for_notion.get("question", event_data.get("question", "-")),
                                trade_signal_info["data"]
                            )
                    except Exception as e:
                        print(f"âš ï¸ Notion Logger è®°å½•å¤±è´¥: {e}")
            else:
                # Binary event: existing logic
                # Use the model_assignments we already got from event_analysis above (line 168)
                # Build specialized prompts for each model
                prompts = {}
                for model_name in model_names:
                    assignment = model_assignments.get(model_name)
                    prompt = self.prompt_builder.build_prompt(
                        event_data, 
                        model_name, 
                        model_assignment=assignment
                    )
                    prompts[model_name] = prompt
                    
                    if assignment:
                        print(f"  âœ… {model_name}: {assignment['dimension_name']}")
                
                # Call all models in parallel with timeout
                await maybe_await(update.message.reply_text("ğŸ¤– æ­£åœ¨æŸ¥è¯¢ AI æ¨¡å‹..."))
                print(f"\nğŸ“ Calling {len(prompts)} models: {list(prompts.keys())}")
                
                try:
                    # Add overall timeout for model calls
                    # Use model_orchestrator's timeout constant for consistency
                    timeout = self.model_orchestrator.MAX_TOTAL_WAIT_TIME
                    model_results = await asyncio.wait_for(
                        self.model_orchestrator.call_all_models(prompts),
                        timeout=float(timeout)
                    )
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•æ¨¡å‹æˆåŠŸ
                    success_count = sum(1 for r in model_results.values() if r is not None)
                    
                    # ã€æ–°å¢ã€‘è°ƒç”¨ OpenRouter å…è´¹æ¨¡å‹ä½œä¸ºè¾…åŠ©å±‚ï¼ˆå•é€‰é¡¹äº‹ä»¶ï¼‰
                    if OPENROUTER_INTEGRATION_AVAILABLE and is_openrouter_available():
                        print(f"\nğŸ†“ [OpenRouter] è°ƒç”¨å…è´¹æ¨¡å‹ä½œä¸ºè¾…åŠ©å±‚...")
                        openrouter_models = get_openrouter_models()
                        # åªè°ƒç”¨å‰ 2 ä¸ªæ¨¡å‹ï¼ˆé¿å…è¿‡å¤šè°ƒç”¨ï¼‰
                        selected_models = openrouter_models[:2] if len(openrouter_models) >= 2 else openrouter_models
                        
                        if selected_models:
                            # ä½¿ç”¨é€šç”¨ promptï¼ˆç¬¬ä¸€ä¸ªæ¨¡å‹çš„ promptï¼‰
                            common_prompt = list(prompts.values())[0] if prompts else ""
                            
                            try:
                                openrouter_results = await asyncio.wait_for(
                                    call_multiple_openrouter_models(selected_models, common_prompt),
                                    timeout=30.0  # OpenRouter è¶…æ—¶æ—¶é—´
                                )
                                
                                # åˆå¹¶ OpenRouter ç»“æœåˆ° model_results
                                openrouter_success = 0
                                for model_name, result in openrouter_results.items():
                                    if result:
                                        # ä½¿ç”¨ç®€çŸ­çš„æ˜¾ç¤ºåç§°
                                        display_name = model_name.split('/')[-1]  # ä¾‹å¦‚ "mistral-7b-instruct"
                                        model_results[f"openrouter_{display_name}"] = result
                                        openrouter_success += 1
                                
                                if openrouter_success > 0:
                                    print(f"âœ… [OpenRouter] {openrouter_success}/{len(selected_models)} ä¸ªæ¨¡å‹è°ƒç”¨æˆåŠŸ")
                                    success_count += openrouter_success
                                else:
                                    print(f"âš ï¸ [OpenRouter] æ‰€æœ‰æ¨¡å‹è°ƒç”¨å¤±è´¥")
                                    
                            except asyncio.TimeoutError:
                                print(f"â±ï¸ [OpenRouter] è°ƒç”¨è¶…æ—¶ï¼Œè·³è¿‡")
                            except Exception as e:
                                print(f"âš ï¸ [OpenRouter] è°ƒç”¨å¼‚å¸¸: {type(e).__name__}: {e}")
                    else:
                        print(f"â„¹ï¸ [OpenRouter] API å¯†é’¥æœªé…ç½®ï¼Œè·³è¿‡å…è´¹æ¨¡å‹è°ƒç”¨")
                    
                    if success_count == 0:
                        # æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥äº†ï¼Œä½†ä»ç„¶å°è¯•ç”¨å¸‚åœºä»·æ ¼ç»§ç»­
                        print(f"âš ï¸ [WARNING] æ‰€æœ‰æ¨¡å‹è°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨å¸‚åœºä»·æ ¼ä½œä¸ºfallback")
                        await maybe_await(update.message.reply_text(
                            "âš ï¸ AIæ¨¡å‹æš‚æ—¶æ— å“åº”ï¼Œå°†ä½¿ç”¨å¸‚åœºä»·æ ¼è¿›è¡Œé¢„æµ‹ã€‚",
                            parse_mode="Markdown"
                        ))
                        # ç»§ç»­æ‰§è¡Œï¼Œä½¿ç”¨å¸‚åœºä»·æ ¼
                    elif success_count < len(prompts):
                        # éƒ¨åˆ†æ¨¡å‹æˆåŠŸï¼Œæç¤ºç”¨æˆ·ä½†ç»§ç»­
                        print(f"âš ï¸ [WARNING] éƒ¨åˆ†æ¨¡å—å“åº”æ…¢ï¼š{success_count}/{len(prompts)} ä¸ªæ¨¡å‹æˆåŠŸ")
                        # ä¸å‘é€é¢å¤–æ¶ˆæ¯ï¼Œç›´æ¥ç»§ç»­ï¼ˆé¿å…è¿‡å¤šæç¤ºï¼‰
                        
                except asyncio.TimeoutError:
                    print(f"â±ï¸ [ERROR] æ¨¡å‹æŸ¥è¯¢æ€»è¶…æ—¶ï¼ˆ>{timeout}sï¼‰")
                    import traceback
                    print(f"[DEBUG] Timeout exception traceback:")
                    traceback.print_exc()
                    
                    # å°è¯•è·å–å·²å®Œæˆçš„æ¨¡å‹ç»“æœï¼ˆå³ä½¿éƒ¨åˆ†å¤±è´¥ï¼‰
                    # ç”±äºcall_all_modelså†…éƒ¨æœ‰è¶…æ—¶ä¿æŠ¤ï¼Œå¯èƒ½å·²æœ‰éƒ¨åˆ†ç»“æœ
                    try:
                        # å¦‚æœcall_all_modelså·²å®Œæˆï¼ˆå³ä½¿è¶…æ—¶ï¼‰ï¼Œå¯èƒ½å·²è¿”å›éƒ¨åˆ†ç»“æœ
                        # è¿™é‡Œæˆ‘ä»¬å°è¯•ç»§ç»­ï¼Œä½¿ç”¨å·²æœ‰çš„ç»“æœæˆ–å¸‚åœºä»·æ ¼
                        # å®é™…ä¸Šcall_all_modelsåº”è¯¥å·²ç»è¿”å›äº†ï¼Œæ‰€ä»¥è¿™é‡Œå°è¯•ä½¿ç”¨é»˜è®¤å€¼
                        model_results = {
                            name: {
                                "probability": 50.0,
                                "confidence": "low",
                                "reasoning": "Overall timeout"
                            }
                            for name in prompts.keys()
                        }
                        await maybe_await(update.message.reply_text(
                            "âš ï¸ éƒ¨åˆ†æ¨¡å—å“åº”å»¶è¿Ÿï¼Œç»“æœå¯èƒ½ä¸å®Œå…¨å‡†ç¡®ã€‚",
                            parse_mode="Markdown"
                        ))
                        # ç»§ç»­æ‰§è¡Œï¼Œä¸returnï¼Œè®©ç³»ç»Ÿå°è¯•ç”¨å¸‚åœºä»·æ ¼ç»§ç»­
                    except Exception as e:
                        print(f"âŒ [ERROR] å¤„ç†è¶…æ—¶å¼‚å¸¸å¤±è´¥: {type(e).__name__}: {e}")
                        import traceback
                        traceback.print_exc()
                        await maybe_await(update.message.reply_text(
                            "â±ï¸ æ¨¡å‹æŸ¥è¯¢è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•ã€‚",
                            parse_mode="Markdown"
                        ))
                        return
                
                # Debug: Print results
                print(f"\nğŸ“Š Model Results:")
                for model_name, result in model_results.items():
                    if result:
                        print(f"  âœ… {model_name}: {result.get('probability')}% ({result.get('confidence')})")
                    else:
                        print(f"  âŒ {model_name}: No response")
                
                # Get model weights
                model_weights = {
                    model_name: self.model_orchestrator.get_model_weight(model_name)
                    for model_name in model_names
                }
                
                # ã€æ–°å¢ã€‘ä¸º OpenRouter æ¨¡å‹æ·»åŠ æƒé‡ï¼ˆä½¿ç”¨è¾ƒä½çš„æƒé‡ï¼Œå› ä¸ºæ˜¯è¾…åŠ©å±‚ï¼‰
                if OPENROUTER_INTEGRATION_AVAILABLE and is_openrouter_available():
                    openrouter_models = get_openrouter_models()
                    # åªæ·»åŠ å®é™…è°ƒç”¨æˆåŠŸçš„ OpenRouter æ¨¡å‹
                    for model_name in openrouter_models[:2]:  # åªè€ƒè™‘å‰2ä¸ª
                        display_name = model_name.split('/')[-1]
                        openrouter_key = f"openrouter_{display_name}"
                        # æ£€æŸ¥æ˜¯å¦åœ¨ model_results ä¸­ï¼ˆè¯´æ˜è°ƒç”¨æˆåŠŸï¼‰
                        if openrouter_key in model_results and model_results[openrouter_key]:
                            # OpenRouter æ¨¡å‹æƒé‡è¾ƒä½ï¼ˆ0.5ï¼‰ï¼Œä½œä¸ºè¾…åŠ©å±‚
                            model_weights[openrouter_key] = 0.5
                
                # Fuse predictions
                fusion_result = self.fusion_engine.fuse_predictions(
                    model_results=model_results,
                    model_weights=model_weights,
                    market_prob=event_data["market_prob"],
                    orchestrator=self.model_orchestrator  # Pass orchestrator for version info
                )
                
                trade_signal_data = None
                if fusion_result:
                    ai_prob_trade = fusion_result.get("model_only_prob")
                    if ai_prob_trade is None:
                        ai_prob_trade = fusion_result.get("final_prob")
                    market_prob_trade = event_data.get("market_prob") or fusion_result.get("final_prob")
                    days_to_resolution = event_data.get("days_left") or 30
                    uncertainty_ratio = (fusion_result.get("uncertainty") or 0.0) / 100.0
                    trade_signal_data = self.fusion_engine.evaluate_trade_signal(
                        ai_prob_trade,
                        market_prob_trade,
                        days_to_resolution,
                        uncertainty_ratio
                    )
                    fusion_result["trade_signal"] = trade_signal_data
                
                # Format and send output
                output = self.output_formatter.format_prediction(
                    event_data=event_data,
                    fusion_result=fusion_result,
                    trade_signal=trade_signal_data
                )
                
                await maybe_await(update.message.reply_text(
                    output,
                    parse_mode="Markdown"
                ))
                
                # è®°å½•åˆ° Notionï¼ˆå•é€‰é¡¹äº‹ä»¶ï¼‰
                if self.notion_logger:
                    if not self.notion_logger.enabled:
                        print("âš ï¸ Notion Logger æœªå¯ç”¨ï¼Œè·³è¿‡è®°å½•ï¼ˆå•é€‰é¡¹äº‹ä»¶ï¼‰")
                if self.notion_logger and self.notion_logger.enabled:
                    try:
                        # ç¡®ä¿ event_data åŒ…å«å¿…è¦å­—æ®µ
                        event_data_for_notion = event_data.copy()
                        # æ·»åŠ  category å­—æ®µï¼ˆä» full_analysis è·å–ï¼‰
                        if full_analysis:
                            event_data_for_notion["category"] = full_analysis.get("event_category_display", 
                                full_analysis.get("event_category", "-"))
                        # ç¡®ä¿æœ‰ outcomes å­—æ®µï¼ˆå•é€‰é¡¹äº‹ä»¶åªæœ‰ä¸€ä¸ªé€‰é¡¹ï¼‰
                        if "outcomes" not in event_data_for_notion:
                            event_data_for_notion["outcomes"] = ["Yes"]  # å•é€‰é¡¹äº‹ä»¶é»˜è®¤é€‰é¡¹
                        
                        # ç¡®ä¿ fusion_result åŒ…å«å¿…è¦å­—æ®µ
                        fusion_result_for_notion = fusion_result.copy()
                        # æ·»åŠ  models åˆ—è¡¨ï¼ˆä» model_versions æˆ– model_names æå–ï¼‰
                        if "models" not in fusion_result_for_notion:
                            model_versions = fusion_result.get("model_versions", {})
                            if model_versions:
                                fusion_result_for_notion["models"] = [
                                    info.get("display_name", model_id)
                                    for model_id, info in model_versions.items()
                                ]
                            else:
                                # Fallback: ä½¿ç”¨å½“å‰ä½¿ç”¨çš„æ¨¡å‹åˆ—è¡¨
                                fusion_result_for_notion["models"] = model_names if 'model_names' in locals() else []
                        # æ·»åŠ  run_idï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
                        if "run_id" not in fusion_result_for_notion:
                            import uuid
                            fusion_result_for_notion["run_id"] = str(uuid.uuid4())
                        
                        self.notion_logger.log_prediction(
                            event_data=event_data_for_notion,
                            fusion_result=fusion_result_for_notion,
                            full_analysis=full_analysis,
                            outcomes=None,
                            normalization_info=None
                        )
                    except Exception as e:
                        print(f"âš ï¸ Notion Logger è®°å½•å¤±è´¥: {e}")
            
        except Exception as e:
            error_type = type(e).__name__
            error_msg = str(e)
            print(f"âŒ [ERROR] Error in handle_predict: {error_type}: {error_msg}")
            import traceback
            print(f"[DEBUG] Full traceback:")
            traceback.print_exc()
            
            await maybe_await(update.message.reply_text(
                self.output_formatter.format_error(
                    f"å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {error_type}: {error_msg}"
                ),
                parse_mode="Markdown"
            ))
    
    async def handle_start(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """Handle /start command."""
        welcome_message = """
ğŸ¤– *Polymarket é¢„æµ‹æœºå™¨äºº*

æˆ‘å¯ä»¥ä½¿ç”¨å¤šä¸ª AI æ¨¡å‹é¢„æµ‹ Polymarket äº‹ä»¶ï¼

*ä½¿ç”¨æ–¹æ³•:*
/predict <äº‹ä»¶æè¿°>

*ç¤ºä¾‹:*
/predict Sora ä¼šåœ¨ 10 æœˆ 31 æ—¥æˆä¸ºç¾å›½ Apple App Store å…è´¹åº”ç”¨æ’è¡Œæ¦œç¬¬ä¸€åå—ï¼Ÿ

æœºå™¨äººå°†ï¼š
1. ä» Polymarket è·å–å¸‚åœºæ•°æ®
2. æŸ¥è¯¢å¤šä¸ª AI æ¨¡å‹ï¼ˆDeepSeek + OpenRouterï¼‰
3. èåˆé¢„æµ‹ç»“æœä¸å¸‚åœºæ¦‚ç‡
4. æä¾›è¯¦ç»†çš„é¢„æµ‹æŠ¥å‘Š
        """
        await maybe_await(update.message.reply_text(
            welcome_message,
            parse_mode="Markdown"
        ))
    
    async def handle_ping(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """Handle /ping command for testing."""
        await maybe_await(update.message.reply_text("âœ… Bot æ­£å¸¸è¿è¡Œï¼"))
    
    async def handle_help(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """Handle /help command."""
        help_message = """
*å‘½ä»¤:*
/start - å¯åŠ¨æœºå™¨äºº
/help - æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
/predict <äº‹ä»¶> - é¢„æµ‹ä¸€ä¸ª Polymarket äº‹ä»¶

*å·¥ä½œåŸç†:*
æœºå™¨äººä½¿ç”¨å¤šä¸ª AI æ¨¡å‹çš„é›†æˆæ¥è¿›è¡Œé¢„æµ‹ï¼š
- DeepSeek: æ ¸å¿ƒé‡åŒ–æ¨ç†
- NVIDIA Nemotron: å¸‚åœºåˆ†æ
- DeepSeek (OpenRouter): é‡åŒ–æ¨ç†ä¸å¸‚åœºåˆ†æ

ç»“æœä¼šä¸ Polymarket å¸‚åœºæ¦‚ç‡èåˆï¼Œä»¥æé«˜å‡†ç¡®æ€§ã€‚
        """
        await maybe_await(update.message.reply_text(
            help_message,
            parse_mode="Markdown"
        ))


def list_models():
    """List all configured models and their versions."""
    from model_orchestrator import ModelOrchestrator
    
    orchestrator = ModelOrchestrator()
    
    print("=" * 80)
    print("ğŸ¤– å½“å‰é…ç½®çš„æ¨¡å‹ç‰ˆæœ¬".center(80))
    print("=" * 80)
    
    models_summary = orchestrator.get_active_models_summary()
    
    print(f"\n{'æ¨¡å‹åç§°':<25} {'æ¨¡å‹ID':<30} {'æ›´æ–°æ—¶é—´':<12} {'æƒé‡':<8}")
    print("-" * 80)
    
    for model_id, info in models_summary.items():
        display_name = info.get("display_name", model_id)
        last_updated = info.get("last_updated", "æœªçŸ¥")
        weight = info.get("weight", 0)
        print(f"{display_name:<25} {model_id:<30} {last_updated:<12} {weight:<8.1f}")
    
    print("\n" + "=" * 80)
    print("ğŸ“Š æ¨¡å‹ç»Ÿè®¡".center(80))
    print("=" * 80)
    print(f"æ€»æ¨¡å‹æ•°: {len(models_summary)}")
    
    # Show default model
    default_models = [
        model_id for model_id, info in orchestrator.MODELS.items()
        if info.get("is_default", False)
    ]
    if default_models:
        print(f"é»˜è®¤æ¨¡å‹: {', '.join(default_models)}")
    
    print("\n" + "=" * 80)


def main():
    """Main entry point."""
    load_dotenv()
    
    # Check for --list-models command early to support offline diagnostics
    if len(sys.argv) > 1 and sys.argv[1] == "--list-models":
        list_models()
        return
    
    if not TELEGRAM_AVAILABLE:
        print("ğŸ›‘ Telegram ä¾èµ–æœªå®‰è£…ï¼Œæœºå™¨äººæ— æ³•å¯åŠ¨ã€‚è¯·è¿è¡Œ: pip install python-telegram-bot==13.15")
        print(f"   åŸå§‹é”™è¯¯: {TELEGRAM_IMPORT_ERROR}")
        return
    
    # ã€æ–°å¢ã€‘æ£€æŸ¥å¹¶æ›´æ–° LMArena æƒé‡ï¼ˆä»…åœ¨å¯åŠ¨æ—¶ï¼‰
    if LMARENA_UPDATE_AVAILABLE:
        try:
            if should_update():
                print(f"\n[LMArena] æ£€æµ‹åˆ°æƒé‡æ–‡ä»¶éœ€è¦æ›´æ–°ï¼Œå¼€å§‹åˆ·æ–°...")
                update_success = update_lmarena_weights()
                if not update_success:
                    print(f"[LMArena] æ‹‰å–å¤±è´¥ï¼Œä½¿ç”¨æ—§æƒé‡")
            else:
                print(f"[LMArena] æƒé‡æ–‡ä»¶ä»ç„¶æœ‰æ•ˆï¼Œè·³è¿‡æ›´æ–°")
        except Exception as e:
            print(f"âš ï¸ [LMArena] è‡ªåŠ¨æ›´æ–°æƒé‡æ—¶å‡ºé”™: {type(e).__name__}: {e}")
            print(f"   ç»§ç»­ä½¿ç”¨ç°æœ‰æƒé‡æ–‡ä»¶")
    
    token = os.getenv("TELEGRAM_BOT_TOKEN")
    
    if not token:
        print("Error: TELEGRAM_BOT_TOKEN not found in environment variables.")
        print("Please create a .env file with your Telegram bot token.")
        return
    
    # Create bot instance
    bot = ForecastingBot()
    
    # Create application
    try:
        # apscheduler æ—¶åŒºé—®é¢˜å·²åœ¨æ¨¡å—å¯¼å…¥æ—¶ä¿®è¡¥
        builder = Application.builder().token(token)
        application = builder.build()
        
        # Register handlers
        application.add_handler(CommandHandler("start", wrap_async_handler(bot.handle_start)))
        application.add_handler(CommandHandler("help", wrap_async_handler(bot.handle_help)))
        application.add_handler(CommandHandler("ping", wrap_async_handler(bot.handle_ping)))
        application.add_handler(CommandHandler("predict", wrap_async_handler(bot.handle_predict)))
        
        # Handle direct Polymarket URLs - check both text and entities
        async def handle_url_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
            """Handle messages containing Polymarket URLs."""
            print(f"\nğŸ” [URL Handler] æ”¶åˆ°æ¶ˆæ¯ï¼Œæ£€æŸ¥æ˜¯å¦ä¸º URL...")
            
            if not update.message:
                print(f"âš ï¸ [URL Handler] update.message ä¸ºç©º")
                return
            
            text = update.message.text or ""
            print(f"ğŸ“ [URL Handler] æ¶ˆæ¯æ–‡æœ¬: {text[:100]}...")
            print(f"ğŸ“ [URL Handler] æ¶ˆæ¯æ–‡æœ¬é•¿åº¦: {len(text)}")
            
            # Method 1: Check text content directly
            has_polymarket_url = False
            if text:
                try:
                    has_polymarket_url = 'polymarket.com' in text.lower()
                    if has_polymarket_url:
                        print(f"âœ… [URL Handler] ä»æ–‡æœ¬å†…å®¹æ£€æµ‹åˆ° Polymarket URL")
                except Exception as e:
                    print(f"âš ï¸ [URL Handler] æ£€æŸ¥æ–‡æœ¬å†…å®¹æ—¶å‡ºé”™: {e}")
            
            # Method 2: Check message entities (URL links, text links, etc.)
            if not has_polymarket_url and update.message.entities:
                print(f"ğŸ” [URL Handler] æ£€æŸ¥æ¶ˆæ¯å®ä½“ï¼Œæ•°é‡: {len(update.message.entities)}")
                try:
                    from telegram import MessageEntity
                    for entity in update.message.entities:
                        print(f"   - å®ä½“ç±»å‹: {entity.type}, offset: {entity.offset}, length: {entity.length}")
                        if entity.type in [MessageEntity.URL, MessageEntity.TEXT_LINK]:
                            # Extract URL from entity
                            if entity.type == MessageEntity.URL:
                                url_text = text[entity.offset:entity.offset + entity.length]
                                print(f"   âœ… æ‰¾åˆ° URL å®ä½“: {url_text[:80]}")
                            elif entity.type == MessageEntity.TEXT_LINK:
                                url_text = entity.url
                                print(f"   âœ… æ‰¾åˆ° TEXT_LINK å®ä½“: {url_text[:80]}")
                            else:
                                continue
                            
                            if url_text and 'polymarket.com' in url_text.lower():
                                has_polymarket_url = True
                                print(f"âœ… [URL Handler] ä»æ¶ˆæ¯å®ä½“æ£€æµ‹åˆ° URL: {url_text[:80]}")
                                break
                except Exception as e:
                    print(f"âš ï¸ [URL Handler] æ£€æŸ¥æ¶ˆæ¯å®ä½“æ—¶å‡ºé”™: {e}")
                    import traceback
                    traceback.print_exc()
            
            if has_polymarket_url:
                print(f"âœ… [URL Handler] æ£€æµ‹åˆ° Polymarket URLï¼Œå¼€å§‹å¤„ç†...")
                try:
                    await bot.handle_predict(update, context)
                except Exception as e:
                    print(f"âŒ [URL Handler] è°ƒç”¨ handle_predict æ—¶å‡ºé”™: {e}")
                    import traceback
                    traceback.print_exc()
            else:
                print(f"â„¹ï¸ [URL Handler] æœªæ£€æµ‹åˆ° Polymarket URLï¼Œè·³è¿‡å¤„ç†")
        
        # Add handler for URLs - register with lower priority (group=1)
        application.add_handler(
            MessageHandler(
                filters.TEXT & ~filters.COMMAND,
                wrap_async_handler(handle_url_message)
            ),
            group=1
        )
        
        # Add error handler
        async def error_handler(update: object, context: ContextTypes.DEFAULT_TYPE) -> None:
            """Log the error and send a message to the user."""
            import traceback
            error = context.error
            print(f"\nâŒ Exception while handling an update:")
            print(f"   é”™è¯¯ç±»å‹: {type(error).__name__}")
            print(f"   é”™è¯¯ä¿¡æ¯: {error}")
            traceback.print_exc()
            
            # Try to send error message to user if possible
            if update and hasattr(update, 'message') and update.message:
                try:
                    await maybe_await(context.bot.send_message(
                        chat_id=update.message.chat_id,
                        text=f"âŒ å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”™: {type(error).__name__}\nè¯·ç¨åé‡è¯•æˆ–ä½¿ç”¨ /help æŸ¥çœ‹å¸®åŠ©ã€‚"
                    ))
                except:
                    pass
        
        application.add_error_handler(wrap_async_handler(error_handler))
        
        # Start bot
        print("=" * 50)
        print("ğŸ¤– Polymarket é¢„æµ‹æœºå™¨äºº")
        print("=" * 50)
        print(f"âœ… Bot Token: {token[:10]}...")
        print("âœ… æœºå™¨äººæ­£åœ¨å¯åŠ¨...")
        print("âœ… ç­‰å¾…æ¶ˆæ¯...")
        print("=" * 50)
        print("\nğŸ’¡ æç¤ºï¼šåœ¨ Telegram ä¸­æ‰¾åˆ°ä½ çš„ Bot å¹¶å‘é€ /start")
        print("=" * 50)
        
        application.run_polling(
            allowed_updates=Update.ALL_TYPES,
            drop_pending_updates=True
        )
    except Exception as e:
        print(f"âŒ å¯åŠ¨æœºå™¨äººæ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        print("\nè¯·æ£€æŸ¥ï¼š")
        print("1. Bot Token æ˜¯å¦æ­£ç¡®")
        print("2. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
        print("3. æ˜¯å¦èƒ½å¤Ÿè®¿é—® Telegram API")


def test_notion_write():
    """æµ‹è¯• Notion å†™å…¥åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("ğŸ§ª æµ‹è¯• Notion å†™å…¥åŠŸèƒ½")
    print("=" * 60)
    
    try:
        from notion_logger import NotionLogger
        
        notion_logger = NotionLogger(
            notion_token=os.getenv("NOTION_TOKEN"),
            database_id=os.getenv("NOTION_DB_ID")
        )
        
        if not notion_logger.enabled:
            print("âš ï¸ Notion Logger æœªå¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•")
            return
        
        print("âœ… Notion Logger å·²åˆå§‹åŒ–")
        
        # æµ‹è¯•æ•°æ®
        test_event_data = {
            "question": "Notion æµ‹è¯•å†™å…¥",
            "market_prob": 0.25,
            "category": "system",
            "rules": "å†™å…¥æµ‹è¯•"
        }
        
        test_fusion_result = {
            "model_only_prob": 0.35,
            "final_prob": 0.33,
            "summary": "æµ‹è¯•æ˜¯å¦èƒ½æˆåŠŸå†™å…¥ Notionã€‚",
            "model_versions": {
                "gpt-4o": {"display_name": "GPT-4o"},
                "claude-3-7-sonnet-latest": {"display_name": "Claude"}
            },
            "run_id": "test-001"
        }
        
        print(f"ğŸ“ æ­£åœ¨å†™å…¥æµ‹è¯•æ•°æ®...")
        result = notion_logger.log_prediction(
            event_data=test_event_data,
            fusion_result=test_fusion_result,
            full_analysis={"event_category": "system", "rules_summary": "å†™å…¥æµ‹è¯•"},
            outcomes=None,
            normalization_info=None
        )
        
        if result:
            print(f"âœ… æˆåŠŸå†™å…¥ Notion: {test_event_data.get('question')}")
            print(f"ğŸ’¡ è¯·å‰å¾€æ•°æ®åº“æŸ¥çœ‹: https://www.notion.so/{notion_logger.database_id}")
        else:
            print(f"âŒ å†™å…¥å¤±è´¥")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # æ£€æŸ¥æ˜¯å¦æœ‰å‘½ä»¤è¡Œå‚æ•°æ¥è¿è¡Œæµ‹è¯•
    if len(sys.argv) > 1 and sys.argv[1] == "--test-notion":
        test_notion_write()
    else:
        main()
