"""
èåˆå±‚ï¼ˆFusion Engineï¼‰ï¼š
æ ¹æ® OPTIMIZATION_NOTES.md çš„äº”å±‚æ¶æ„è®¾è®¡

èŒè´£ï¼š
- å¯¹ä¸åŒæ¨¡å‹çš„é¢„æµ‹è¿›è¡ŒåŠ æƒèåˆ
- è®¡ç®— AI å…±è¯†æ¦‚ç‡ä¸å¸‚åœºæ¦‚ç‡çš„èåˆç»“æœï¼ˆ80% AI + 20% å¸‚åœºï¼‰
- ç”Ÿæˆç®€çŸ­çš„"AIå…±è¯†è§‚ç‚¹æ€»ç»“"ï¼ˆä¸­æ–‡ï¼‰
- è¯„ä¼°æ¨¡å‹é—´çš„åˆ†æ­§ç¨‹åº¦
- ä» LMArena.ai åŸºç¡€æƒé‡é…ç½®æ–‡ä»¶åŠ è½½æ¨¡å‹æƒé‡

è¾“å…¥ï¼šå„æ¨¡å‹çš„é¢„æµ‹ç»“æœ {probability, confidence, reasoning}
è¾“å‡ºï¼šèåˆåçš„é¢„æµ‹ {final_prob, model_only_prob, uncertainty, summary, disagreement}
"""
import json
import math
import time
import re
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from datetime import datetime, timezone


class FusionEngine:
    """
    Fuses multiple model predictions with weighted averaging.
    
    èåˆå¤šä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœï¼š
    - ä½¿ç”¨æ¨¡å‹æƒé‡å’Œç½®ä¿¡åº¦æƒé‡è®¡ç®—åŠ æƒå¹³å‡
    - å°†AIå…±è¯†ï¼ˆ80%ï¼‰ä¸å¸‚åœºæ¦‚ç‡ï¼ˆ20%ï¼‰èåˆ
    - ç”Ÿæˆä¸­æ–‡æ‘˜è¦
    - è¯„ä¼°åˆ†æ­§ç¨‹åº¦
    - ä» LMArena.ai é…ç½®æ–‡ä»¶åŠ è½½åŸºç¡€æƒé‡
    """
    
    CONFIDENCE_WEIGHTS = {
        "low": 1.0,
        "medium": 2.0,
        "high": 3.0
    }
    
    MARKET_WEIGHT = 0.2  # Weight for market probability (é™ä½å¸‚åœºæƒé‡)
    MODEL_WEIGHT = 0.8   # Weight for model consensus (æé«˜æ¨¡å‹æƒé‡)
    
    # Model name mapping: actual model_id -> LMArena config key
    MODEL_NAME_MAPPING = {
        "gpt-4o": "gpt-4o-latest",
        "claude-3-7-sonnet-latest": "claude-opus-4-1",  # Map to closest match
        "claude-3-5-opus-latest": "claude-opus-4-1",
        "gemini-2.5-pro": "gemini-2.5-pro",
        "gemini-2.5-flash": "gemini-2.5-pro",  # Fallback to pro if flash not found
        "grok-4": "grok-4-fast",
        "grok-3": "grok-4-fast",  # Fallback
        "deepseek-chat": "deepseek-v3.2-exp"
    }
    
    def __init__(self, experiment_config=None):
        """
        Initialize FusionEngine and load LMArena base weights.
        
        Args:
            experiment_config: Optional ExperimentConfig instance for ablation studies
        """
        self.lmarena_config = self._load_lmarena_config()
        self.base_weights = self.lmarena_config.get("weights", {})
        self.metadata = self.lmarena_config.get("metadata", {})
        self.fusion_config = self.lmarena_config.get("fusion", {})
        
        # Experiment configuration (for ablation studies)
        self.experiment_config = experiment_config
        
        # Update confidence weights from config if available
        if "default_confidence_multiplier" in self.fusion_config:
            self.CONFIDENCE_WEIGHTS.update(self.fusion_config["default_confidence_multiplier"])
        
        # Log weights on initialization
        self._log_base_weights()
        
        # Calibration state (for binning/platt calibration)
        self.calibration_map = None  # Will be fitted during calibration
    
    def _load_lmarena_config(self) -> Dict:
        """Load base weights from LMArena configuration file."""
        config_path = Path(__file__).parent.parent / "config" / "base_weights_lmarena.json"
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            return config
        except FileNotFoundError:
            print(f"âš ï¸ LMArenaæƒé‡é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {config_path}")
            print("   ä½¿ç”¨é»˜è®¤æƒé‡é…ç½®...")
            return {
                "metadata": {"source": "é»˜è®¤é…ç½®", "updated_at": "æœªçŸ¥"},
                "weights": {},
                "fusion": {}
            }
        except json.JSONDecodeError as e:
            print(f"âš ï¸ LMArenaæƒé‡é…ç½®æ–‡ä»¶è§£æé”™è¯¯: {e}")
            print("   ä½¿ç”¨é»˜è®¤æƒé‡é…ç½®...")
            return {
                "metadata": {"source": "é»˜è®¤é…ç½®", "updated_at": "æœªçŸ¥"},
                "weights": {},
                "fusion": {}
            }
    
    def _log_base_weights(self):
        """Log base weights information on startup."""
        source = self.metadata.get("source", "æœªçŸ¥æ¥æº")
        updated_at = self.metadata.get("updated_at", "æœªçŸ¥")
        
        print(f"ğŸ“Š æ¨¡å‹åŸºç¡€æƒé‡æ¥æºï¼š{source}")
        
        # Collect weight info for active models
        weight_info = []
        for model_key, config in self.base_weights.items():
            base_weight = config.get("base_weight", 1.0) or 0.0
            # ã€é˜²å¾¡ã€‘ç¡®ä¿ base_weight ä¸ä¸º None
            if base_weight is None:
                print(f"âš ï¸ base_weight is None for {model_key}, using default 0.0")
                base_weight = 0.0
            # Find display name (simplified)
            if "gemini" in model_key:
                weight_info.append(f"Gemini {(base_weight or 0.0):.2f}")
            elif "claude" in model_key:
                weight_info.append(f"Claude {(base_weight or 0.0):.2f}")
            elif "gpt" in model_key:
                weight_info.append(f"GPT-4o {(base_weight or 0.0):.2f}")
            elif "grok" in model_key:
                weight_info.append(f"Grok {(base_weight or 0.0):.2f}")
            elif "deepseek" in model_key:
                weight_info.append(f"DeepSeek {(base_weight or 0.0):.2f}")
        
        if weight_info:
            print(" / ".join(weight_info))
        else:
            print("æœªåŠ è½½åˆ°æƒé‡é…ç½®")
    
    def _get_base_weight(self, model_name: str) -> float:
        """
        Get base weight for a model from LMArena config.
        
        Args:
            model_name: Actual model ID used in the system
            
        Returns:
            Base weight from LMArena config, or 1.0 if not found
        """
        # Map model name to LMArena config key
        lmarena_key = self.MODEL_NAME_MAPPING.get(model_name, model_name)
        
        # Try exact match first
        if lmarena_key in self.base_weights:
            return self.base_weights[lmarena_key].get("base_weight", 1.0)
        
        # Try partial match (e.g., "claude-3-7-sonnet" might match "claude-opus-4-1")
        for key, config in self.base_weights.items():
            if model_name in key or key in model_name:
                return config.get("base_weight", 1.0)
        
        # Fallback: return 1.0
        return 1.0
    
    def fuse_predictions(
        self,
        model_results: Dict[str, Optional[Dict]],
        model_weights: Dict[str, float],
        market_prob: float,
        orchestrator=None  # Optional: pass orchestrator instance to avoid creating new one
    ) -> Dict:
        """
        Fuse predictions from multiple models.
        
        Args:
            model_results: Dict mapping model_name -> {probability, confidence, reasoning}
            model_weights: Dict mapping model_name -> weight
            market_prob: Market probability from Polymarket
        
        Returns:
            Dict with final_prob, uncertainty, summary, disagreement
        """
        start_time = time.time()
        print(f"[DEBUG] ========== fuse_predictions START ==========")
        print(f"[DEBUG] Model results count: {len(model_results)}")
        print(f"[DEBUG] Market prob: {market_prob}")
        # Filter out None results
        valid_results = {
            name: result
            for name, result in model_results.items()
            if result is not None
        }
        
        if not valid_results:
            # Fallback if no models responded
            return {
                "final_prob": market_prob,
                "model_only_prob": None,  # No model prediction available
                "uncertainty": 10.0,
                "summary": "æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹å“åº”ã€‚ä½¿ç”¨å¸‚åœºæ¦‚ç‡ã€‚",
                "disagreement": "é«˜",
                "model_count": 0
            }
        
        # Calculate weighted probabilities
        probabilities: List[float] = []
        weights: List[float] = []
        reasonings = []
        deepseek_reasoning = None  # Store DeepSeek's reasoning separately
        
        for model_name, result in valid_results.items():
            prob = self._safe_float(result.get("probability"))
            confidence = result["confidence"]
            reasoning = result["reasoning"]
            
            # Use LMArena base weight instead of model_weights parameter
            base_weight = self._get_base_weight(model_name)
            confidence_weight = self.CONFIDENCE_WEIGHTS.get(confidence, 2.0)
            total_weight = self._safe_float(base_weight, 0.0) * self._safe_float(confidence_weight, 0.0)
            
            probabilities.append(prob)
            weights.append(total_weight)
            # Only add reasoning, not model name
            reasonings.append(reasoning)
            
            # Extract DeepSeek reasoning separately
            if model_name == "deepseek-chat":
                deepseek_reasoning = reasoning
        
        # Weighted mean / variance without numpy (é˜²æ­¢ numpy å¯¼å…¥å¯¼è‡´å´©æºƒ)
        weighted_mean = self._weighted_mean(probabilities, weights)
        uncertainty = self._weighted_std(probabilities, weights, weighted_mean)
        
        # Apply consensus coefficient if enabled
        if self._should_use_consensus_coef():
            consensus_weight = weighted_mean
        else:
            # If consensus_coef is disabled, use simple average
            consensus_weight = self._mean(probabilities)
        
        # Combine with market probability (if market_bias enabled)
        if self._should_use_market_bias():
            final_prob = (
                self.MODEL_WEIGHT * consensus_weight +
                self.MARKET_WEIGHT * market_prob
            )
        else:
            # If market_bias is disabled, use pure AI consensus
            final_prob = consensus_weight
        
        # Apply de-marketization penalty (if enabled)
        demarket_applied = False
        if self._should_use_demarket_penalty():
            penalty_result = self._apply_demarket_penalty(
                final_prob, consensus_weight, market_prob, valid_results
            )
            if penalty_result:
                final_prob = penalty_result["adjusted_prob"]
                demarket_applied = True
        
        # Apply post-calibration if enabled
        calibration_applied = False
        calibration_method = self._get_calibration_method()
        if calibration_method and calibration_method != "none":
            calibrated_prob = self._apply_calibration(final_prob, calibration_method)
            if calibrated_prob is not None:
                final_prob = calibrated_prob
                calibration_applied = True
        
        # Calculate disagreement level
        disagreement = self._calculate_disagreement(probabilities, uncertainty)
        
        # Generate summary
        summary = self._generate_summary(reasonings, consensus_weight, market_prob)
        
        # Add weight source information
        weight_source = {
            "file": "base_weights_lmarena.json",
            "source": self.metadata.get("source", "æœªçŸ¥"),
            "updated_at": self.metadata.get("updated_at", "æœªçŸ¥")
        }
        
        result = {
            "final_prob": round(final_prob, 2),
            "model_only_prob": round(consensus_weight, 2),  # Pure model prediction before market fusion
            "uncertainty": round(uncertainty, 2),
            "summary": summary,
            "disagreement": disagreement,
            "model_count": len(valid_results),
            "deepseek_reasoning": deepseek_reasoning,  # DeepSeek's reasoning for separate display
            "model_versions": self._get_model_versions(valid_results, orchestrator),  # Add model versions info
            "weight_source": weight_source  # Add weight source info
        }
        
        # Add experiment flags if applicable
        if demarket_applied:
            result["demarket_applied"] = True
            result["demarket_note"] = "Applied de-marketization penalty to avoid pure tracking."
        if calibration_applied:
            result["calibration_applied"] = True
            result["calibration_method"] = calibration_method
        
        total_time = time.time() - start_time
        # ã€é˜²å¾¡ã€‘ç¡®ä¿ total_time ä¸ä¸º None
        total_time = total_time or 0.0
        if total_time is None:
            print("âš ï¸ total_time is None, using default 0.0")
            total_time = 0.0
        print(f"[DEBUG] Total model fusion time: {(total_time or 0.0):.2f}s")
        print(f"[DEBUG] ========== fuse_predictions END ==========")
        
        return result
    
    def _get_model_versions(self, model_results: Dict[str, Optional[Dict]], orchestrator=None) -> Dict[str, Dict]:
        """Get model version information from ModelOrchestrator."""
        try:
            # Use provided orchestrator if available, otherwise create a new one
            if orchestrator is None:
                from src.model_orchestrator import ModelOrchestrator
                orchestrator = ModelOrchestrator()
            
            versions = {}
            for model_name in model_results.keys():
                if model_results[model_name] is not None:  # Only for successful calls
                    info = orchestrator.get_model_info(model_name)
                    if info:
                        versions[model_name] = {
                            "display_name": info["display_name"],
                            "last_updated": info.get("last_updated", "æœªçŸ¥")
                        }
            
            return versions
        except Exception as e:
            print(f"âš ï¸ è·å–æ¨¡å‹ç‰ˆæœ¬ä¿¡æ¯å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {}
    
    def _calculate_disagreement(self, probabilities: List[float], uncertainty: float) -> str:
        """Calculate disagreement level based on uncertainty."""
        # Handle edge case: single model or zero uncertainty
        if len(probabilities) <= 1 or uncertainty == 0.0:
            return "ä½"
        elif uncertainty < 3.0:
            return "ä½"
        elif uncertainty < 8.0:
            return "ä¸­"
        else:
            return "é«˜"
    
    def _generate_summary(self, reasonings: List[str], model_prob: float, market_prob: float) -> str:
        """
        Generate a brief summary from model reasonings.
        ä¼˜åŒ–ï¼šç¡®ä¿ç”Ÿæˆçš„æ‘˜è¦æ›´ç®€æ´ï¼Œçªå‡ºå…³é”®è§‚ç‚¹ï¼Œä¸”ä¸ºä¸­æ–‡ã€‚
        """
        if not reasonings:
            return "æš‚æ— è¯¦ç»†æ¨ç†ä¿¡æ¯ã€‚"
        
        # Combine first few reasonings (already cleaned, no model names)
        # ä¼˜å…ˆä½¿ç”¨å‰2ä¸ªæ¨¡å‹çš„æ¨ç†ï¼Œå¦‚æœéƒ½æ˜¯ä¸­æ–‡åˆ™ç›´æ¥åˆå¹¶
        summary_parts = reasonings[:2]  # Use first 2 models' reasoning
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦
        has_chinese = any(any('\u4e00' <= char <= '\u9fff' for char in part) for part in summary_parts)
        
        if has_chinese:
            # å¦‚æœåŒ…å«ä¸­æ–‡ï¼Œç›´æ¥åˆå¹¶
            summary = " ".join(summary_parts)
        else:
            # å¦‚æœéƒ½æ˜¯è‹±æ–‡ï¼Œæ·»åŠ æç¤ºè¯´æ˜è¿™æ˜¯è‹±æ–‡æ¨ç†
            # å®é™…ä½¿ç”¨ä¸­ï¼Œç”±äºpromptè¦æ±‚ä¸­æ–‡ï¼Œè¿™é‡Œåº”è¯¥ä¸ä¼šè§¦å‘
            summary = "æ¨¡å‹æ¨ç†ï¼š" + " ".join(summary_parts)
        
        # Truncate if too long (ä¿æŒç®€æ´)
        if len(summary) > 200:
            summary = summary[:197] + "..."
        
        return summary
    
    @staticmethod
    def classify_multi_option_event(event_title: str, outcomes: List[Dict]) -> str:
        """
        æ ¹æ®äº‹ä»¶æ ‡é¢˜å’Œé€‰é¡¹å†…å®¹è‡ªåŠ¨è¯†åˆ«äº‹ä»¶ç±»å‹ã€‚
        
        Args:
            event_title: äº‹ä»¶æ ‡é¢˜
            outcomes: é€‰é¡¹åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å« name å­—æ®µ
        
        Returns:
            "mutually_exclusive" - äº’æ–¥äº‹ä»¶ï¼ˆå€™é€‰äººã€æ”¿å…šã€çƒé˜Ÿç­‰ï¼‰
            "conditional" - æ¡ä»¶äº‹ä»¶ï¼ˆæ—¶é—´ã€ä»·æ ¼ã€åœ°ç†åˆ†ç»„ç­‰ï¼‰
            "hybrid" - æ··åˆäº‹ä»¶ï¼ˆæ— æ³•ç¡®å®šï¼‰
        """
        if not outcomes or len(outcomes) == 0:
            return "hybrid"
        
        event_title_lower = event_title.lower() if event_title else ""
        
        # ã€ä¿®å¤ #4ã€‘æ—¥æœŸæ¨¡å¼è¯†åˆ«ï¼ˆæ­£åˆ™è¡¨è¾¾å¼ï¼‰
        date_patterns = [
            r'\bon\s+\w+\s+\d+',  # "on Oct 30", "on November 15"
            r'\bby\s+\w+\s+\d+',  # "by Dec 31", "by December 31"
            r'\bbefore\s+\w+\s+\d+',  # "before Jan 1"
            r'\bafter\s+\w+\s+\d+',  # "after Nov 30"
            r'\buntil\s+\w+\s+\d+',  # "until Dec 31"
            r'\bdeadline\s+\w+\s+\d+',  # "deadline Oct 30"
        ]
        
        # ã€ä¿®å¤ #4ã€‘æ£€æŸ¥äº‹ä»¶æ ‡é¢˜ä¸­çš„æ—¥æœŸæ¨¡å¼
        has_date_pattern = any(re.search(pattern, event_title_lower, re.IGNORECASE) for pattern in date_patterns)
        if has_date_pattern:
            print(f"[DEBUG] æ£€æµ‹åˆ°æ—¥æœŸæ¨¡å¼ï¼Œç›´æ¥åˆ¤å®šä¸º conditional")
            return "conditional"
        
        # ã€ä¿®å¤ #4ã€‘ä»·æ ¼æ¨¡å¼è¯†åˆ«
        price_patterns = [
            r'price\s+(above|below|over|under|at least|at most)\s+',
            r'\$\d+',  # "$100k", "$1M"
            r'\d+\s*(million|billion|trillion|k|m|b)\s*(usd|eur|â‚¬|Â¥)?',
        ]
        has_price_pattern = any(re.search(pattern, event_title_lower, re.IGNORECASE) for pattern in price_patterns)
        if has_price_pattern:
            print(f"[DEBUG] æ£€æµ‹åˆ°ä»·æ ¼æ¨¡å¼ï¼Œç›´æ¥åˆ¤å®šä¸º conditional")
            return "conditional"
        
        # æ¡ä»¶äº‹ä»¶ç‰¹å¾å…³é”®è¯
        conditional_keywords = {
            # ã€ä¿®å¤ #2ã€‘æ—¶é—´ç›¸å…³ - æ·»åŠ æœˆä»½å…¨å†™å’Œæ˜ŸæœŸ
            "time": [
                # æœˆä»½ç¼©å†™
                "oct", "nov", "dec", "jan", "feb", "mar", "apr", "may", "jun", "jul", "aug", "sep",
                # ã€ä¿®å¤ #2ã€‘æœˆä»½å…¨å†™
                "october", "november", "december", "january", "february", "march", "april",
                "june", "july", "august", "september",
                # ã€ä¿®å¤ #2ã€‘æ˜ŸæœŸ
                "monday", "tuesday", "wednesday", "thursday", "friday", "saturday", "sunday",
                "mon", "tue", "wed", "thu", "fri", "sat", "sun",
                # å¹´ä»½
                "2024", "2025", "2026", "2027", "2028", "2029", "2030",
                # å…¶ä»–æ—¶é—´ç›¸å…³
                "day", "month", "year", "q1", "q2", "q3", "q4", "h1", "h2",
                "before", "after", "by", "until", "deadline",
                # ã€ä¿®å¤ #2ã€‘æ—¥æœŸæ ¼å¼æ¨¡å¼
                r'\d{1,2}[/-]\d{1,2}',  # "10/30", "10-30"
                r'\d{1,2}[/-]\d{1,2}[/-]\d{4}',  # "10/30/2025"
            ],
            # ä»·æ ¼/æ•°å€¼ç›¸å…³
            "price": ["$", "â‚¬", "Â¥", "usd", "eur", "million", "billion", "trillion",
                     "k", "m", "b", "%", "percent", "bps", "basis points",
                     "above", "below", "more than", "less than", "at least", "at most",
                     "range", "between", "over", "under"],
            # ã€ä¿®å¤ #1ã€‘åœ°ç†/æŒ‡æ ‡ç›¸å…³ - æ·»åŠ å…·ä½“å›½å®¶åå’ŒåŸå¸‚å
            "geographic": [
                # é€šç”¨åœ°ç†è¯æ±‡
                "region", "country", "state", "city", "province", "area",
                # ã€ä¿®å¤ #1ã€‘å¸¸è§å›½å®¶å
                "israel", "lebanon", "palestine", "syria", "iran", "iraq", "saudi arabia",
                "russia", "ukraine", "china", "taiwan", "north korea", "south korea",
                "japan", "india", "pakistan", "afghanistan", "turkey", "egypt",
                "us", "usa", "united states", "uk", "united kingdom", "france", "germany",
                "italy", "spain", "poland", "greece", "brazil", "mexico", "canada",
                "australia", "new zealand", "south africa", "nigeria", "kenya",
                # ã€ä¿®å¤ #1ã€‘å¸¸è§åŸå¸‚å
                "beijing", "shanghai", "moscow", "london", "paris", "berlin", "tokyo",
                "tel aviv", "beirut", "damascus", "baghdad", "tehran", "riyadh",
                "new york", "washington", "los angeles", "chicago", "san francisco",
                "kiev", "kyiv", "istanbul", "cairo", "jerusalem", "gaza",
            ],
            # æŒ‡æ ‡ç›¸å…³
            "indicator": ["gdp", "inflation", "unemployment", "rate", "ratio", "index",
                         "temperature", "rainfall", "pollution", "score"]
        }
        
        conditional_score = 0
        mutually_exclusive_score = 0
        
        # æ£€æŸ¥äº‹ä»¶æ ‡é¢˜
        for category, keywords in conditional_keywords.items():
            for kw in keywords:
                if isinstance(kw, str) and kw in event_title_lower:
                    conditional_score += 2
                elif isinstance(kw, str) and kw.startswith('\\'):  # æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
                    if re.search(kw, event_title_lower):
                        conditional_score += 2
        
        # æ£€æŸ¥é€‰é¡¹å†…å®¹
        for outcome in outcomes:
            name = outcome.get('name', '').strip()
            if not name:
                continue
            
            name_lower = name.lower()
            
            # ã€ä¿®å¤ #5ã€‘å…ˆæ£€æŸ¥æ˜¯å¦ä¸ºæ—¥æœŸæ ¼å¼ï¼Œé¿å…è¢«è¯¯åˆ¤ä¸ºäº’æ–¥å‹
            is_date_format = False
            # æ£€æŸ¥æœˆä»½åç§°
            month_names = ["jan", "feb", "mar", "apr", "may", "jun", 
                          "jul", "aug", "sep", "oct", "nov", "dec",
                          "january", "february", "march", "april",
                          "june", "july", "august", "september",
                          "october", "november", "december"]
            if any(month in name_lower for month in month_names):
                is_date_format = True
            # æ£€æŸ¥æ—¥æœŸæ¨¡å¼ï¼šMM/DD, DD/MM, MM-DD ç­‰
            if re.search(r'\d{1,2}[/-]\d{1,2}', name):
                is_date_format = True
            # æ£€æŸ¥æ•°å­— + æœˆä»½æ ¼å¼ï¼š30 Oct, Oct 30, November 15 ç­‰
            if re.search(r'\d{1,2}\s+(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec|january|february|march|april|june|july|august|september|october|november|december)', name_lower):
                is_date_format = True
            if re.search(r'(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec|january|february|march|april|june|july|august|september|october|november|december)\s+\d{1,2}', name_lower):
                is_date_format = True
            
            # æ£€æŸ¥æ¡ä»¶å‹ç‰¹å¾
            has_time = any((kw if isinstance(kw, str) else None) and kw in name_lower 
                          for kw in conditional_keywords["time"] if isinstance(kw, str) and not kw.startswith('\\'))
            # æ£€æŸ¥æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
            for kw in conditional_keywords["time"]:
                if isinstance(kw, str) and kw.startswith('\\'):
                    if re.search(kw, name_lower):
                        has_time = True
                        break
            
            has_price = any(kw in name_lower for kw in conditional_keywords["price"])
            has_geo = any(kw in name_lower for kw in conditional_keywords["geographic"])
            has_indicator = any(kw in name_lower for kw in conditional_keywords["indicator"])
            has_number = bool(re.search(r'\d', name))
            has_symbol = bool(re.search(r'[%$â‚¬Â¥<>\-â€“â€”]', name))
            
            if has_time or has_price or has_geo or has_indicator or (has_number and has_symbol):
                conditional_score += 2
            elif has_number and not has_symbol:
                # çº¯æ•°å­—å¯èƒ½æ˜¯æ¡ä»¶å‹ï¼ˆæ—¥æœŸã€ä»·æ ¼ç­‰ï¼‰
                conditional_score += 1
            
            # ã€ä¿®å¤ #5ã€‘æ£€æŸ¥äº’æ–¥å‹ç‰¹å¾ï¼ˆäººåã€ç»„ç»‡åç­‰ï¼‰- æ’é™¤æ—¥æœŸæ ¼å¼
            # äººåç‰¹å¾ï¼šé¦–å­—æ¯å¤§å†™ã€åŒ…å«ç©ºæ ¼ã€æ— ç‰¹æ®Šç¬¦å·ï¼Œä¸”ä¸æ˜¯æ—¥æœŸæ ¼å¼
            if not is_date_format:
                words = name.split()
                if len(words) <= 4:
                    is_capitalized = all(word and word[0].isupper() for word in words if word)
                    has_no_special = not bool(re.search(r'[%$â‚¬Â¥<>\-â€“â€”\d]', name))
                    # é¢å¤–æ£€æŸ¥ï¼šä¸åŒ…å«æ˜æ˜¾çš„æ¡ä»¶å‹å…³é”®è¯ï¼ˆæœˆä»½ã€æ˜ŸæœŸç­‰ï¼‰
                    has_no_time_keywords = not any(month in name_lower for month in month_names)
                    if is_capitalized and has_no_special and has_no_time_keywords:
                        mutually_exclusive_score += 2  # å¢åŠ æƒé‡ï¼Œå› ä¸ºè¿™æ˜¯å¼ºçƒˆçš„äººåç‰¹å¾
        
        # ã€ä¿®å¤ #6ã€‘è¾“å‡ºè¯¦ç»†è°ƒè¯•æ—¥å¿—
        print(f"[DEBUG] äº‹ä»¶ç±»å‹è¯†åˆ«è¯¦æƒ…:")
        print(f"  æ ‡é¢˜: {event_title[:60]}...")
        print(f"  é€‰é¡¹æ•°é‡: {len(outcomes)}")
        print(f"  conditional_score: {conditional_score}")
        print(f"  mutually_exclusive_score: {mutually_exclusive_score}")
        
        # ã€ä¿®å¤ #3ã€‘æ”¹è¿›åˆ¤æ–­é˜ˆå€¼ - ä½¿ç”¨æ›´æ™ºèƒ½çš„é€»è¾‘
        # ç­–ç•¥ï¼šå¦‚æœ mutually_exclusive_score æ˜æ˜¾æ›´é«˜ï¼Œä¼˜å…ˆåˆ¤å®šä¸º mutually_exclusive
        # å¦åˆ™ä¼˜å…ˆåˆ¤å®šä¸º conditionalï¼ˆé¿å…è¯¯å½’ä¸€åŒ–ï¼‰
        
        # è®¡ç®—é€‰é¡¹ä¸­äººåæ¯”ä¾‹ï¼ˆç”¨äºæ›´å‡†ç¡®çš„åˆ¤æ–­ï¼‰
        total_outcomes = len(outcomes)
        person_like_count = sum(1 for o in outcomes 
                               if o.get('name', '').strip() and 
                               not re.search(r'[%$â‚¬Â¥<>\-â€“â€”\d]', o.get('name', '')) and
                               all(word and word[0].isupper() for word in o.get('name', '').split()[:4]))
        person_ratio = person_like_count / total_outcomes if total_outcomes > 0 else 0
        
        # å¦‚æœå¤§éƒ¨åˆ†é€‰é¡¹éƒ½æ˜¯äººåæ ¼å¼ï¼Œä¸” mutually_exclusive_score è¶³å¤Ÿé«˜ï¼Œåˆ¤å®šä¸ºäº’æ–¥å‹
        # æ³¨æ„ï¼šæ ‡é¢˜ä¸­çš„å¹´ä»½ï¼ˆå¦‚ "2024 Election"ï¼‰ä¸åº”è¯¥å½±å“åˆ¤æ–­ï¼Œæ‰€ä»¥æé«˜ person_ratio çš„æƒé‡
        if person_ratio > 0.6:
            # å¦‚æœå¤§éƒ¨åˆ†é€‰é¡¹æ˜¯äººåï¼Œä¸” mutually_exclusive_score ä¸ä¸º0ï¼Œåˆ¤å®šä¸ºäº’æ–¥å‹
            # è¿™å¯ä»¥è¦†ç›–æ ‡é¢˜ä¸­æœ‰å¹´ä»½ç­‰æ¡ä»¶å…³é”®è¯ä½†å®é™…æ˜¯é€‰ä¸¾ç±»äº‹ä»¶çš„æƒ…å†µ
            if mutually_exclusive_score > 0:
                result = "mutually_exclusive"
            else:
                # å¦‚æœæ²¡æœ‰ä»»ä½•äº’æ–¥ç‰¹å¾ï¼Œå¯èƒ½æ˜¯å…¶ä»–ç±»å‹
                result = "conditional" if conditional_score > 0 else "hybrid"
        elif person_ratio > 0.4 and mutually_exclusive_score >= conditional_score * 0.7:
            # å¦‚æœ 40%+ æ˜¯äººåæ ¼å¼ï¼Œä¸”äº’æ–¥å¾—åˆ†è¶³å¤Ÿé«˜ï¼Œä¹Ÿåˆ¤å®šä¸ºäº’æ–¥å‹
            result = "mutually_exclusive"
        elif conditional_score > 0 and conditional_score >= mutually_exclusive_score * 1.5:
            # conditional_score æ˜æ˜¾æ›´é«˜
            result = "conditional"
        elif mutually_exclusive_score > 0 and mutually_exclusive_score > conditional_score * 1.5:
            # mutually_exclusive_score æ˜æ˜¾æ›´é«˜
            result = "mutually_exclusive"
        elif conditional_score > mutually_exclusive_score:
            # conditional_score ç¨é«˜ï¼Œåˆ¤å®šä¸º conditionalï¼ˆä¿å®ˆç­–ç•¥ï¼Œé¿å…è¯¯å½’ä¸€åŒ–ï¼‰
            result = "conditional"
        elif mutually_exclusive_score > conditional_score:
            result = "mutually_exclusive"
        else:
            # ç›¸ç­‰æˆ–éƒ½ä¸º0ï¼Œåˆ¤å®šä¸º hybridï¼ˆä¸å½’ä¸€åŒ–ï¼‰
            result = "hybrid"
        
        print(f"  åˆ¤å®šç»“æœ: {result}")
        return result
    
    @staticmethod
    def filter_invalid_outcomes(outcomes: List[Dict]) -> List[Dict]:
        """
        è‡ªåŠ¨è·³è¿‡å·²è¿‡æœŸæˆ–æ— æ•ˆçš„å­å¸‚åœºã€‚
        
        è¿‡æ»¤è§„åˆ™ï¼š
        - æ—¶é—´å‹ï¼šæ—¥æœŸæ—©äºå½“å‰ UTC æ—¥æœŸ
        - ä»·æ ¼å‹ï¼šæ¡ä»¶é€»è¾‘æ— æ•ˆï¼ˆå¦‚ "above 1000000%"ï¼‰
        - å…¶ä»–æ— æ•ˆé¡¹ï¼šé‡å¤æˆ–ç©ºæ ‡é¢˜
        
        Args:
            outcomes: é€‰é¡¹åˆ—è¡¨
        
        Returns:
            è¿‡æ»¤åçš„æœ‰æ•ˆé€‰é¡¹åˆ—è¡¨
        """
        if not outcomes:
            return []
        
        valid_outcomes = []
        seen_names = set()
        now_utc = datetime.now(timezone.utc)
        
        # æ—¥æœŸæ¨¡å¼åŒ¹é…
        date_patterns = [
            r'(\d{1,2})[/-](\d{1,2})[/-](\d{4})',  # MM/DD/YYYY or DD/MM/YYYY
            r'(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)\s+(\d{1,2}),?\s+(\d{4})',  # Month DD, YYYY
            r'(\d{4})[/-](\d{1,2})[/-](\d{1,2})',  # YYYY/MM/DD
        ]
        
        month_map = {
            'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6,
            'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12
        }
        
        for outcome in outcomes:
            name = outcome.get('name', '').strip()
            
            # è¿‡æ»¤ç©ºæ ‡é¢˜
            if not name:
                continue
            
            # è¿‡æ»¤é‡å¤é¡¹
            name_normalized = name.lower().strip()
            if name_normalized in seen_names:
                print(f"[DEBUG] è·³è¿‡é‡å¤é€‰é¡¹: {name}")
                continue
            seen_names.add(name_normalized)
            
            # å°è¯•è§£ææ—¥æœŸï¼ˆæ—¶é—´å‹é€‰é¡¹ï¼‰
            is_expired = False
            name_lower = name.lower()
            
            for pattern in date_patterns:
                match = re.search(pattern, name_lower, re.IGNORECASE)
                if match:
                    try:
                        if 'jan' in name_lower or 'feb' in name_lower:  # æœˆä»½åç§°æ ¼å¼
                            month_name = match.group(1).lower()[:3]
                            day = int(match.group(2))
                            year = int(match.group(3))
                            month = month_map.get(month_name)
                            if month:
                                option_date = datetime(year, month, day, tzinfo=timezone.utc)
                                if option_date < now_utc:
                                    is_expired = True
                                    print(f"[DEBUG] è·³è¿‡è¿‡æœŸé€‰é¡¹: {name} (æ—¥æœŸ: {option_date.date()})")
                        else:  # æ•°å­—æ ¼å¼æ—¥æœŸ
                            parts = match.groups()
                            if len(parts) == 3:
                                # å°è¯•ä¸åŒçš„æ—¥æœŸæ ¼å¼
                                try:
                                    # MM/DD/YYYY
                                    month, day, year = int(parts[0]), int(parts[1]), int(parts[2])
                                    if month <= 12 and day <= 31:
                                        option_date = datetime(year, month, day, tzinfo=timezone.utc)
                                        if option_date < now_utc:
                                            is_expired = True
                                            print(f"[DEBUG] è·³è¿‡è¿‡æœŸé€‰é¡¹: {name} (æ—¥æœŸ: {option_date.date()})")
                                except:
                                    try:
                                        # DD/MM/YYYY
                                        day, month, year = int(parts[0]), int(parts[1]), int(parts[2])
                                        if month <= 12 and day <= 31:
                                            option_date = datetime(year, month, day, tzinfo=timezone.utc)
                                            if option_date < now_utc:
                                                is_expired = True
                                                print(f"[DEBUG] è·³è¿‡è¿‡æœŸé€‰é¡¹: {name} (æ—¥æœŸ: {option_date.date()})")
                                    except:
                                        pass
                    except:
                        pass
                    break
            
            if is_expired:
                continue
            
            # è¿‡æ»¤æ— æ•ˆä»·æ ¼æ¡ä»¶ï¼ˆå¦‚ "above 1000000%" è¿™ç§æ˜æ˜¾ä¸åˆç†çš„æƒ…å†µï¼‰
            if '%' in name or '$' in name or 'usd' in name_lower:
                # æ£€æŸ¥æ˜¯å¦æœ‰æç«¯æ•°å€¼
                numbers = re.findall(r'[\d,]+\.?\d*', name)
                for num_str in numbers:
                    try:
                        num = float(num_str.replace(',', ''))
                        # å¦‚æœç™¾åˆ†æ¯”è¶…è¿‡ 1000% æˆ–ä»·æ ¼è¶…è¿‡åˆç†èŒƒå›´ï¼Œå¯èƒ½æ˜¯æ— æ•ˆé€‰é¡¹
                        if '%' in name and num > 1000:
                            print(f"[DEBUG] è·³è¿‡æ— æ•ˆä»·æ ¼é€‰é¡¹: {name} (æ•°å€¼å¼‚å¸¸: {num}%)")
                            is_expired = True
                            break
                    except:
                        pass
            
            if is_expired:
                continue
            
            # æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼Œæ·»åŠ æœ‰æ•ˆé€‰é¡¹
            valid_outcomes.append(outcome)
        
        if len(outcomes) != len(valid_outcomes):
            print(f"[DEBUG] è¿‡æ»¤æ— æ•ˆé€‰é¡¹: {len(outcomes)} â†’ {len(valid_outcomes)}")
        
        return valid_outcomes
    
    @staticmethod
    def normalize_all_predictions(outcomes: List[Dict], event_title: str = "") -> Dict:
        """
        å¯¹æ‰€æœ‰ outcome çš„ AI é¢„æµ‹æ¦‚ç‡è¿›è¡Œå½’ä¸€åŒ–ï¼Œä½¿å…¶æ€»å’Œç­‰äº 100%ã€‚
        
        åŠŸèƒ½ï¼š
        1. æ ¹æ®äº‹ä»¶ç±»å‹åˆ¤æ–­æ˜¯å¦éœ€è¦å½’ä¸€åŒ–ï¼ˆäº’æ–¥äº‹ä»¶å½’ä¸€åŒ–ï¼Œæ¡ä»¶äº‹ä»¶ä¸å½’ä¸€åŒ–ï¼‰
        2. è¿‡æ»¤æ— æ•ˆ/è¿‡æœŸé€‰é¡¹
        3. æå–æ‰€æœ‰æœ‰æ•ˆçš„ AI é¢„æµ‹ï¼ˆmodel_only_probï¼‰
        4. è‹¥ Î£(AI_probs) â‰  100ï¼ŒæŒ‰æ¯”ä¾‹ç¼©æ”¾ï¼ˆä»…äº’æ–¥äº‹ä»¶ï¼‰
        5. åŒæ—¶å½’ä¸€åŒ–ä¸ç¡®å®šåº¦ï¼ˆuncertaintyï¼‰ï¼Œä¿æŒç›¸å¯¹æ¯”ä¾‹
        6. å¤„ç†å¼‚å¸¸å€¼ï¼ˆ>100 æˆ– <0ï¼‰è‡ªåŠ¨è£å‰ªè‡³ [0,100]
        7. å®¹é”™å¤„ç†ï¼šè·³è¿‡ None å€¼ï¼Œå¤„ç†å…¨ä¸º 0 çš„æƒ…å†µ
        
        Args:
            outcomes: List of dicts, each containing:
                - "prediction": float (èåˆåçš„æ¦‚ç‡ï¼Œå¯èƒ½åŒ…å«å¸‚åœºæƒé‡)
                - "market_prob": float (å¸‚åœºä»·æ ¼)
                - "uncertainty": float (ä¸ç¡®å®šåº¦)
                - "model_only_prob": Optional[float] (çº¯AIé¢„æµ‹ï¼Œå¦‚æœå­˜åœ¨)
                - å…¶ä»–å­—æ®µ...
            event_title: äº‹ä»¶æ ‡é¢˜ï¼Œç”¨äºåˆ¤æ–­äº‹ä»¶ç±»å‹
        
        Returns:
            Dict with:
                - "normalized_outcomes": List[Dict] (å½’ä¸€åŒ–åçš„ outcomesï¼Œæ·»åŠ äº† normalized å­—æ®µ)
                - "total_before": float (å½’ä¸€åŒ–å‰çš„æ€»å’Œ)
                - "total_after": float (å½’ä¸€åŒ–åçš„æ€»å’Œï¼Œäº’æ–¥äº‹ä»¶=100.0ï¼Œæ¡ä»¶äº‹ä»¶ä¿æŒåŸå€¼)
                - "error": float (è¯¯å·®ï¼Œåº”è¯¥â‰¤0.01)
                - "skipped_count": int (è·³è¿‡çš„æ— æ•ˆé€‰é¡¹æ•°é‡)
                - "normalized": bool (æ˜¯å¦æ‰§è¡Œäº†å½’ä¸€åŒ–)
                - "event_type": str (äº‹ä»¶ç±»å‹ï¼šmutually_exclusive / conditional / hybrid)
        """
        if not outcomes or len(outcomes) == 0:
            return {
                "normalized_outcomes": [],
                "total_before": 0.0,
                "total_after": 0.0,
                "error": 0.0,
                "skipped_count": 0,
                "normalized": False,
                "event_type": "hybrid"
            }
        
        # ã€æ–°å¢ã€‘è¿‡æ»¤æ— æ•ˆ/è¿‡æœŸé€‰é¡¹
        filtered_outcomes = FusionEngine.filter_invalid_outcomes(outcomes)
        if len(filtered_outcomes) != len(outcomes):
            print(f"[DEBUG] è¿‡æ»¤åé€‰é¡¹æ•°é‡: {len(outcomes)} â†’ {len(filtered_outcomes)}")
        
        # ã€æ–°å¢ã€‘è¯†åˆ«äº‹ä»¶ç±»å‹
        if not event_title:
            print(f"[WARNING] event_title ä¸ºç©ºï¼Œå¯èƒ½å½±å“äº‹ä»¶ç±»å‹è¯†åˆ«")
        event_type = FusionEngine.classify_multi_option_event(event_title or "", filtered_outcomes)
        print(f"[DEBUG] æœ€ç»ˆäº‹ä»¶ç±»å‹: {event_type} (äº‹ä»¶: {(event_title or '')[:50]}...)")
        
        # ã€å…³é”®æ”¹è¿›ã€‘æ ¹æ®äº‹ä»¶ç±»å‹å†³å®šæ˜¯å¦å½’ä¸€åŒ–
        should_normalize = (event_type == "mutually_exclusive")
        
        if not should_normalize:
            # æ¡ä»¶äº‹ä»¶æˆ–æ··åˆäº‹ä»¶ï¼šä¸å½’ä¸€åŒ–ï¼Œåªæ·»åŠ  normalized æ ‡è®°
            if event_type == "conditional":
                print(f"[FusionEngine] æ¡ä»¶äº‹ä»¶æ£€æµ‹åˆ°ï¼Œè·³è¿‡å½’ä¸€åŒ–ã€‚")
            
            marked_outcomes = []
            for outcome in filtered_outcomes:
                marked_outcome = outcome.copy()
                marked_outcome["normalized"] = False
                marked_outcomes.append(marked_outcome)
            
            # è®¡ç®—æ€»å’Œï¼ˆä»…ç”¨äºè°ƒè¯•ï¼Œä¸è¿”å›ç»™è¾“å‡ºå±‚ï¼‰
            total_before = sum(
                outcome.get("model_only_prob") or 0
                for outcome in filtered_outcomes
                if outcome.get("model_only_prob") is not None
            )
            
            return {
                "normalized_outcomes": marked_outcomes,
                "total_before": round(total_before, 2),
                "total_after": None,  # ã€ä¿®å¤ã€‘æ¡ä»¶äº‹ä»¶ä¸è¿”å› total_afterï¼Œè¡¨ç¤ºä¸åº”æ˜¾ç¤ºå½’ä¸€åŒ–æ£€æŸ¥
                "error": 0.0,
                "skipped_count": len(outcomes) - len(filtered_outcomes),
                "normalized": False,
                "event_type": event_type
            }
        
        # äº’æ–¥äº‹ä»¶ï¼šæ‰§è¡Œå½’ä¸€åŒ–ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
        outcomes = filtered_outcomes  # ä½¿ç”¨è¿‡æ»¤åçš„é€‰é¡¹
        
        # æå–æ‰€æœ‰æœ‰æ•ˆçš„ AI é¢„æµ‹å€¼ï¼ˆä¼˜å…ˆä½¿ç”¨ model_only_probï¼Œå¦‚æœæ²¡æœ‰åˆ™å°è¯•ä» prediction æ¨ç®—ï¼‰
        valid_outcomes = []
        ai_probs = []
        uncertainties = []
        skipped_indices = []
        
        for i, outcome in enumerate(outcomes):
            # ä¼˜å…ˆä½¿ç”¨ model_only_probï¼ˆçº¯AIé¢„æµ‹ï¼‰
            # æ³¨æ„ï¼šå¦‚æœ model_only_prob ä¸º Noneï¼Œè¯´æ˜è¯¥é€‰é¡¹æ²¡æœ‰æœ‰æ•ˆçš„AIé¢„æµ‹ï¼Œåº”è¯¥è·³è¿‡å½’ä¸€åŒ–
            ai_prob = outcome.get("model_only_prob")
            
            # å¦‚æœ model_only_prob ä¸º Noneï¼Œè·³è¿‡è¯¥é€‰é¡¹ï¼ˆä¸è¿›è¡Œå½’ä¸€åŒ–ï¼‰
            if ai_prob is None:
                skipped_indices.append(i)
                continue
            
            # è£å‰ªå¼‚å¸¸å€¼åˆ° [0, 100]
            ai_prob = max(0.0, min(100.0, float(ai_prob)))
            
            uncertainty = outcome.get("uncertainty", 0.0)
            if uncertainty is None:
                uncertainty = 0.0
            
            valid_outcomes.append(outcome)
            ai_probs.append(ai_prob)
            uncertainties.append(float(uncertainty))
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®
        if len(valid_outcomes) == 0:
            # æ‰€æœ‰é€‰é¡¹éƒ½æ— æ•ˆï¼Œè¿”å›åŸæ•°æ®
            return {
                "normalized_outcomes": outcomes,
                "total_before": 0.0,
                "total_after": 0.0,
                "error": 0.0,
                "skipped_count": len(outcomes)
            }
        
        # è®¡ç®—æ€»å’Œ
        total_before = sum(ai_probs)
        
        # ã€å…³é”®æ”¹è¿›ã€‘äº’æ–¥äº‹ä»¶ï¼šå½’ä¸€åŒ–å¤„ç†
        if total_before == 0:
            # å…¨ä¸º 0 çš„æƒ…å†µï¼šå‡åˆ†ï¼ˆé¿å…é™¤é›¶ï¼‰
            normalized_probs = [100.0 / len(ai_probs)] * len(ai_probs)
            # ä¸ç¡®å®šåº¦ä¿æŒåŸå€¼ï¼ˆå·²ç»æ˜¯ç›¸å¯¹å€¼ï¼‰
            normalized_uncertainties = uncertainties
            total_after = 100.0
        else:
            # æŒ‰æ¯”ä¾‹ç¼©æ”¾
            scale_factor = 100.0 / total_before
            normalized_probs = [prob * scale_factor for prob in ai_probs]
            
            # å½’ä¸€åŒ–ä¸ç¡®å®šåº¦ï¼šä¿æŒç›¸å¯¹æ¯”ä¾‹ï¼Œä½†éœ€è¦ç›¸åº”ç¼©æ”¾
            # ç”±äºæ¦‚ç‡è¢«ç¼©æ”¾ï¼Œä¸ç¡®å®šåº¦ä¹Ÿåº”è¯¥æŒ‰ç›¸åŒæ¯”ä¾‹ç¼©æ”¾ï¼ˆä¿æŒç›¸å¯¹å…³ç³»ï¼‰
            normalized_uncertainties = [unc * scale_factor for unc in uncertainties]
            
            # éªŒè¯æ€»å’Œ
            total_after = sum(normalized_probs)
        
        # è®¡ç®—è¯¯å·®
        error = abs(total_after - 100.0)
        
        # æ›´æ–° outcomes
        normalized_outcomes = []
        valid_idx = 0
        
        # ã€Bugä¿®å¤ã€‘æ·»åŠ éªŒè¯ï¼Œç¡®ä¿ valid_idx ä¸ä¼šè¶Šç•Œ
        if len(normalized_probs) != len(ai_probs):
            print(f"âš ï¸ [WARNING] å½’ä¸€åŒ–æ•°ç»„é•¿åº¦ä¸åŒ¹é…ï¼šnormalized_probs={len(normalized_probs)}, ai_probs={len(ai_probs)}")
        
        for i, outcome in enumerate(outcomes):
            if i in skipped_indices:
                # è·³è¿‡çš„é€‰é¡¹ï¼Œä¿æŒåŸæ ·ï¼ˆç¡®ä¿ model_only_prob ä¿æŒä¸º Noneï¼‰
                skipped_outcome = outcome.copy()
                # ã€Bugä¿®å¤ã€‘æ˜ç¡®ç¡®ä¿è·³è¿‡çš„é€‰é¡¹çš„ model_only_prob ä¸º None
                if skipped_outcome.get("model_only_prob") is not None:
                    print(f"[DEBUG] è·³è¿‡é€‰é¡¹ {outcome.get('name', i)}ï¼Œä½† model_only_prob ä¸ä¸º Noneï¼Œå¼ºåˆ¶è®¾ä¸º None")
                skipped_outcome["model_only_prob"] = None
                normalized_outcomes.append(skipped_outcome)
            else:
                # æ›´æ–° AI é¢„æµ‹æ¦‚ç‡ï¼ˆéœ€è¦åŒæ—¶æ›´æ–° predictionï¼Œå› ä¸ºå®ƒæ˜¯èåˆåçš„å€¼ï¼‰
                if valid_idx >= len(normalized_probs):
                    print(f"âš ï¸ [ERROR] valid_idx ({valid_idx}) >= normalized_probs é•¿åº¦ ({len(normalized_probs)})")
                    # Fallback: ä¿æŒåŸæ ·ï¼Œä½†ä¸æ›´æ–° model_only_prob
                    normalized_outcomes.append(outcome.copy())
                else:
                    updated_outcome = outcome.copy()
                    
                    # æ›´æ–° model_only_probï¼ˆçº¯AIé¢„æµ‹ï¼Œå½’ä¸€åŒ–åçš„å€¼ï¼‰
                    normalized_value = round(normalized_probs[valid_idx], 2)
                    updated_outcome["model_only_prob"] = normalized_value
                    
                    # ã€Bugä¿®å¤ã€‘éªŒè¯å½’ä¸€åŒ–å€¼æ˜¯å¦åˆç†
                    if normalized_value < 0 or normalized_value > 100:
                        print(f"âš ï¸ [WARNING] å½’ä¸€åŒ–å€¼å¼‚å¸¸ï¼š{outcome.get('name', i)} = {normalized_value}%")
                    
                    # æ›´æ–° predictionï¼ˆèåˆåçš„æ¦‚ç‡ï¼‰ï¼šéœ€è¦é‡æ–°èåˆ
                    market_prob = outcome.get("market_prob", 0)
                    updated_prediction = (
                        FusionEngine.MODEL_WEIGHT * normalized_probs[valid_idx] +
                        FusionEngine.MARKET_WEIGHT * market_prob
                    )
                    updated_outcome["prediction"] = round(updated_prediction, 2)
                    
                    # æ›´æ–°ä¸ç¡®å®šåº¦
                    updated_outcome["uncertainty"] = round(normalized_uncertainties[valid_idx], 2)
                    
                    # ã€æ–°å¢ã€‘æ·»åŠ å½’ä¸€åŒ–æ ‡è®°
                    updated_outcome["normalized"] = True
                    
                    normalized_outcomes.append(updated_outcome)
                
                valid_idx += 1
        
        # ã€æ–°å¢ã€‘ä¸ºè·³è¿‡çš„é€‰é¡¹ä¹Ÿæ·»åŠ  normalized æ ‡è®°
        for i, outcome in enumerate(normalized_outcomes):
            if outcome.get("normalized") is None:
                outcome["normalized"] = False  # è·³è¿‡çš„é€‰é¡¹æœªå½’ä¸€åŒ–
        
        return {
            "normalized_outcomes": normalized_outcomes,
            "total_before": round(total_before, 2),
            "total_after": round(total_after, 2),
            "error": round(error, 4),
            "skipped_count": len(skipped_indices),
            "normalized": True,  # æ ‡è®°å·²å½’ä¸€åŒ–
            "event_type": event_type
        }
    
    def _should_use_consensus_coef(self) -> bool:
        """
        å†³å®šæ˜¯å¦å¯ç”¨å…±è¯†åŠ æƒï¼ˆconsensus coefficientï¼‰ã€‚
        
        Returns:
            True if consensus coefficient should be used (default), False otherwise
        """
        if self.experiment_config is None:
            return True  # Default: use consensus weighting
        
        try:
            # Handle ExperimentConfig object or dict
            if hasattr(self.experiment_config, 'config'):
                config = self.experiment_config.config
            else:
                config = self.experiment_config
            
            return config.get("FUSION", {}).get("consensus_coef", True)
        except Exception:
            return True  # Default on error
    
    def _should_use_market_bias(self) -> bool:
        """
        å†³å®šæ˜¯å¦å¯ç”¨å¸‚åœºåå·®èåˆï¼ˆmarket biasï¼‰ã€‚
        
        Returns:
            True if market bias should be used (default), False otherwise
        """
        if self.experiment_config is None:
            return True  # Default: use market bias
        
        try:
            # Handle ExperimentConfig object or dict
            if hasattr(self.experiment_config, 'config'):
                config = self.experiment_config.config
            else:
                config = self.experiment_config
            
            return config.get("FUSION", {}).get("market_bias", True)
        except Exception:
            return True  # Default on error
    
    def _should_use_demarket_penalty(self) -> bool:
        """
        å†³å®šæ˜¯å¦å¯ç”¨åå¸‚åœºè·Ÿè¸ªæƒ©ç½šï¼ˆde-marketization penaltyï¼‰ã€‚
        
        Returns:
            True if de-marketization penalty should be applied, False otherwise (default: False for experiments)
        """
        if self.experiment_config is None:
            return False  # Default: disabled (experimental feature)
        
        try:
            # Handle ExperimentConfig object or dict
            if hasattr(self.experiment_config, 'config'):
                config = self.experiment_config.config
            else:
                config = self.experiment_config
            
            return config.get("FUSION", {}).get("demarket_penalty", False)
        except Exception:
            return False  # Default on error
    
    def _get_calibration_method(self) -> Optional[str]:
        """
        è·å–åæ ¡å‡†æ–¹æ³•ã€‚
        
        Returns:
            Calibration method: "none", "binning", "platt", or None
        """
        if self.experiment_config is None:
            return "none"  # Default: no calibration
        
        try:
            # Handle ExperimentConfig object or dict
            if hasattr(self.experiment_config, 'config'):
                config = self.experiment_config.config
            else:
                config = self.experiment_config
            
            method = config.get("CALIBRATION", {}).get("post_calibration", "none")
            if method in ["none", "binning", "platt"]:
                return method
            return "none"
        except Exception:
            return "none"  # Default on error
    
    def _apply_demarket_penalty(
        self,
        final_prob: float,
        consensus_weight: float,
        market_prob: float,
        model_results: Dict[str, Dict]
    ) -> Optional[Dict]:
        """
        åº”ç”¨åå¸‚åœºè·Ÿè¸ªæƒ©ç½šï¼šå¦‚æœAIé¢„æµ‹ä¸å¸‚åœºæ¦‚ç‡è¿‡äºæ¥è¿‘ä¸”å…±è¯†åº¦é«˜ï¼Œæ·»åŠ å°æ‰°åŠ¨é¿å…çº¯è·Ÿè¸ªã€‚
        
        Args:
            final_prob: å½“å‰èåˆåçš„æ¦‚ç‡
            consensus_weight: æ¨¡å‹å…±è¯†æ¦‚ç‡
            market_prob: å¸‚åœºä»·æ ¼
            model_results: æ¨¡å‹ç»“æœå­—å…¸
        
        Returns:
            Dict with "adjusted_prob" if penalty applied, None otherwise
        """
        # Check if AI prediction is too close to market (within 2%)
        ai_market_diff = abs(consensus_weight - market_prob)
        
        if ai_market_diff < 0.02:  # Less than 2% difference
            # Calculate consensus (model agreement)
            probs = [r["probability"] for r in model_results.values()]
            if len(probs) > 1:
                avg_prob = self._mean(probs)
                if avg_prob > 0:
                    consensus = 1.0 - (self._std(probs) / avg_prob)
                else:
                    consensus = 0.0
            else:
                consensus = 1.0
            
            # Only apply penalty if consensus is high (>0.8)
            if consensus > 0.8:
                # Determine perturbation direction based on minority opinion
                # Use DeepSeek or Grok if available for minority signal
                perturbation = 0.015  # Â±1.5% perturbation
                
                # Try to get minority opinion (DeepSeek or Grok)
                minority_prob = None
                for model_name in ["deepseek-chat", "grok-4", "grok-3"]:
                    if model_name in model_results:
                        minority_prob = model_results[model_name]["probability"]
                        break
                
                # If minority opinion exists and differs from consensus, use it for direction
                if minority_prob is not None:
                    if minority_prob > consensus_weight:
                        adjusted_prob = final_prob + perturbation
                    else:
                        adjusted_prob = final_prob - perturbation
                else:
                    # Default: random direction (but deterministic based on prob value)
                    adjusted_prob = final_prob + perturbation if (final_prob % 0.02) > 0.01 else final_prob - perturbation
                
                # Clip to [0, 100]
                adjusted_prob = max(0.0, min(100.0, adjusted_prob))
                
                return {
                    "adjusted_prob": adjusted_prob,
                    "original_prob": final_prob,
                    "perturbation": perturbation
                }
        
        return None  # No penalty applied

    @staticmethod
    def _safe_float(value: Optional[float], default: float = 0.0) -> float:
        """Convert value to float safely with default fallback."""
        try:
            if value is None:
                return default
            return float(value)
        except (TypeError, ValueError):
            return default

    @staticmethod
    def _mean(values: List[float]) -> float:
        """Return arithmetic mean with empty guard."""
        if not values:
            return 0.0
        return math.fsum(values) / len(values)

    @classmethod
    def _std(cls, values: List[float]) -> float:
        """Return population standard deviation using safe math."""
        if not values:
            return 0.0
        mean_value = cls._mean(values)
        variance = math.fsum((val - mean_value) ** 2 for val in values) / len(values)
        return math.sqrt(variance)

    @classmethod
    def _weighted_mean(cls, values: List[float], weights: List[float]) -> float:
        """Compute weighted mean with graceful fallback."""
        if not values:
            return 0.0
        total_weight = math.fsum(weights) if weights else 0.0
        if total_weight <= 0:
            return cls._mean(values)
        weighted_sum = math.fsum(val * weight for val, weight in zip(values, weights))
        return weighted_sum / total_weight

    @classmethod
    def _weighted_std(cls, values: List[float], weights: List[float], mean_value: float) -> float:
        """Compute weighted standard deviation with safe defaults."""
        if not values:
            return 0.0
        total_weight = math.fsum(weights) if weights else 0.0
        if total_weight <= 0:
            # Fall back to unweighted std
            return cls._std(values)
        variance = math.fsum(
            weight * ((val - mean_value) ** 2)
            for val, weight in zip(values, weights)
        ) / total_weight
        variance = max(variance, 0.0)
        return math.sqrt(variance)
    
    def _apply_calibration(self, prob: float, method: str) -> Optional[float]:
        """
        åº”ç”¨åæ ¡å‡†ï¼ˆbinning æˆ– platt scalingï¼‰ã€‚
        
        Args:
            prob: åŸå§‹æ¦‚ç‡å€¼ (0-100)
            method: æ ¡å‡†æ–¹æ³• ("binning" or "platt")
        
        Returns:
            æ ¡å‡†åçš„æ¦‚ç‡å€¼ï¼Œå¦‚æœæ ¡å‡†æœªå¯ç”¨æˆ–å¤±è´¥åˆ™è¿”å› None
        """
        if method == "none" or self.calibration_map is None:
            return None  # No calibration configured
        
        try:
            if method == "binning":
                # Equal-frequency binning calibration
                # calibration_map should be a dict: {bin_index: calibrated_prob}
                # For simplicity, use linear interpolation if calibration_map is a function
                if callable(self.calibration_map):
                    return self.calibration_map(prob)
                elif isinstance(self.calibration_map, dict):
                    # Simple bin-based lookup (would need proper binning logic)
                    # For now, return original (proper implementation would need historical data)
                    return None
                else:
                    return None
            
            elif method == "platt":
                # Platt scaling (logistic regression calibration)
                # calibration_map should be fitted logistic regression parameters
                # For now, placeholder - would need historical data to fit
                return None
            
            return None
        except Exception as e:
            print(f"âš ï¸ æ ¡å‡†åº”ç”¨å¤±è´¥: {e}")
            return None
    
    def fit_calibration(self, historical_data: Optional[List[Tuple[float, int]]] = None):
        """
        ä½¿ç”¨å†å²æ•°æ®æ‹Ÿåˆæ ¡å‡†æ¨¡å‹ï¼ˆbinning æˆ– platt scalingï¼‰ã€‚
        
        Args:
            historical_data: List of (predicted_prob, true_label) tuples
        """
        # Placeholder for calibration fitting
        # This would be implemented when historical data is available
        calibration_method = self._get_calibration_method()
        if calibration_method != "none" and historical_data:
            # Fit calibration model here
            # For now, calibration_map remains None
            pass
