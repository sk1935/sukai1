"""
æ–°é—»ç¼“å­˜æ¨¡å—

åŠŸèƒ½ï¼š
- å¼‚æ­¥æŠ“å–å¤šä¸ªæ–°é—»æºå¹¶ç¼“å­˜ç»“æœ
- æ”¯æŒ Google News RSSã€GDELTã€NewsData.ioã€Mediastack
- ç¼“å­˜æœ‰æ•ˆæœŸï¼š6å°æ—¶
- APIå¤±æ•ˆæ—¶ä½¿ç”¨æ—§ç¼“å­˜ï¼Œä¸é˜»å¡ä¸»ç¨‹åº
"""
import json
import asyncio
from datetime import datetime, timezone, timedelta
from pathlib import Path
from typing import Dict, List, Optional
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥æ–°é—»æŠ“å–æ¨¡å—ï¼ˆservices åœ¨ src/services ç›®å½•ä¸‹ï¼‰
try:
    # å°è¯•ä» src/services å¯¼å…¥
    sys.path.insert(0, str(Path(__file__).parent / "services"))
    from news_fetcher import (
        fetch_all_free_news,
        fetch_google_rss,
        fetch_gdelt_news,
        fetch_newsdata
    )
except ImportError:
    try:
        # å›é€€ï¼šä»é¡¹ç›®æ ¹ç›®å½•çš„ services å¯¼å…¥
        from services.news_fetcher import (
            fetch_all_free_news,
            fetch_google_rss,
            fetch_gdelt_news,
            fetch_newsdata
        )
    except ImportError:
        # å¦‚æœéƒ½ä¸å¯ç”¨ï¼Œä½¿ç”¨ç©ºå‡½æ•°å ä½
        print("âš ï¸ news_fetcher æ¨¡å—ä¸å¯ç”¨ï¼Œæ–°é—»æŠ“å–åŠŸèƒ½å°†è¢«ç¦ç”¨")
        async def fetch_all_free_news(*args, **kwargs):
            return []
        async def fetch_google_rss(*args, **kwargs):
            return []
        async def fetch_gdelt_news(*args, **kwargs):
            return []
        async def fetch_newsdata(*args, **kwargs):
            return []
from dotenv import load_dotenv

load_dotenv()

# ç¼“å­˜é…ç½®
CACHE_DIR = Path(__file__).parent.parent / "cache"
CACHE_FILE = CACHE_DIR / "news_cache.json"
CACHE_DURATION_HOURS = 6
NEWS_CACHE_ENABLED = False  # Stability mode: disable heavy news fetching


def ensure_cache_dir():
    """ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨"""
    CACHE_DIR.mkdir(parents=True, exist_ok=True)


def load_cache() -> Optional[Dict]:
    """åŠ è½½ç¼“å­˜æ•°æ®"""
    ensure_cache_dir()
    
    if not CACHE_FILE.exists():
        return None
    
    try:
        with open(CACHE_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
            # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
            timestamp_str = data.get("timestamp", "")
            if timestamp_str:
                try:
                    timestamp = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
                    now = datetime.now(timezone.utc)
                    
                    if (now - timestamp).total_seconds() < CACHE_DURATION_HOURS * 3600:
                        print(f"âœ… ä½¿ç”¨ç¼“å­˜çš„æ–°é—»æ•°æ®ï¼ˆå‰©ä½™æœ‰æ•ˆæœŸï¼š{int((CACHE_DURATION_HOURS * 3600 - (now - timestamp).total_seconds()) / 3600)} å°æ—¶ï¼‰")
                        return data
                    else:
                        print(f"âš ï¸ ç¼“å­˜å·²è¿‡æœŸï¼ˆ{int((now - timestamp).total_seconds() / 3600)} å°æ—¶å‰ï¼‰ï¼Œéœ€è¦æ›´æ–°")
                except Exception as e:
                    print(f"âš ï¸ è§£æç¼“å­˜æ—¶é—´æˆ³å¤±è´¥: {e}")
            
            return data
    except Exception as e:
        print(f"âš ï¸ åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
        return None


def save_cache(news_data: List[Dict]) -> bool:
    """ä¿å­˜ç¼“å­˜æ•°æ®"""
    try:
        ensure_cache_dir()
        
        cache_data = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "news": news_data
        }
        
        with open(CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… æ–°é—»ç¼“å­˜å·²ä¿å­˜: {len(news_data)} æ¡æ–°é—»")
        return True
    except Exception as e:
        print(f"âŒ ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
        return False


def calculate_sentiment_score(title: str, summary: str = "") -> float:
    """
    ç®€å•çš„æƒ…æ„Ÿåˆ†æ•°è®¡ç®—ï¼ˆåŸºäºå…³é”®è¯ï¼‰
    
    Args:
        title: æ–°é—»æ ‡é¢˜
        summary: æ–°é—»æ‘˜è¦ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        float: æƒ…æ„Ÿåˆ†æ•°ï¼ŒèŒƒå›´ [-1.0, 1.0]ï¼Œæ­£æ•°è¡¨ç¤ºæ­£é¢ï¼Œè´Ÿæ•°è¡¨ç¤ºè´Ÿé¢
    """
    text = f"{title} {summary}".lower()
    
    # æ­£é¢å…³é”®è¯
    positive_keywords = [
        "success", "growth", "gains", "rise", "surge", "win", "victory",
        "breakthrough", "improve", "positive", "optimistic", "recover",
        "ç¨³å®š", "å¢é•¿", "æˆåŠŸ", "èƒœåˆ©", "çªç ´", "æ”¹å–„", "ä¹è§‚", "æ¢å¤"
    ]
    
    # è´Ÿé¢å…³é”®è¯
    negative_keywords = [
        "crisis", "crash", "fall", "decline", "loss", "war", "conflict",
        "attack", "death", "disaster", "pandemic", "recession", "unemployment",
        "å±æœº", "å´©æºƒ", "ä¸‹é™", "æŸå¤±", "æˆ˜äº‰", "å†²çª", "è¢­å‡»", "æ­»äº¡",
        "ç¾éš¾", "ç–«æƒ…", "è¡°é€€", "å¤±ä¸š"
    ]
    
    pos_count = sum(1 for kw in positive_keywords if kw in text)
    neg_count = sum(1 for kw in negative_keywords if kw in text)
    
    total = pos_count + neg_count
    if total == 0:
        return 0.0  # ä¸­æ€§
    
    # è®¡ç®—åˆ†æ•°ï¼š(-1.0 åˆ° 1.0)
    score = (pos_count - neg_count) / max(total, 1)
    
    # å½’ä¸€åŒ–åˆ° [-1.0, 1.0]
    return max(-1.0, min(1.0, score))


async def fetch_and_cache_news(keyword: str = "", force_refresh: bool = False) -> List[Dict]:
    """
    å¼‚æ­¥æŠ“å–æ–°é—»å¹¶ç¼“å­˜ç»“æœ
    
    Args:
        keyword: æœç´¢å…³é”®è¯ï¼ˆå¯é€‰ï¼‰
        force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰
    
    Returns:
        List[Dict]: æ–°é—»åˆ—è¡¨ï¼Œæ ¼å¼ï¼š
        [
            {
                "title": str,
                "source": str,
                "summary": str,
                "sentiment_score": float,
                "date": str
            },
            ...
        ]
    """
    if not NEWS_CACHE_ENABLED:
        print("ğŸ›‘ [NEWS_CACHE] æ¨¡å—å·²å¤„äºç¦ç”¨çŠ¶æ€ï¼Œç›´æ¥è¿”å›ç©ºç»“æœ")
        return []
    
    # æ£€æŸ¥ç¼“å­˜
    if not force_refresh:
        cached_data = load_cache()
        if cached_data and cached_data.get("news"):
            return cached_data["news"]
    
    print(f"ğŸ“° å¼€å§‹æŠ“å–æ–°é—»ï¼ˆå…³é”®è¯: '{keyword or 'é€šç”¨'}'ï¼‰...")
    
    news_list = []
    
    try:
        # ä½¿ç”¨èšåˆå‡½æ•°æŠ“å–æ‰€æœ‰æ–°é—»æº
        all_news = await fetch_all_free_news(keyword=keyword, limit=50)
        
        # è½¬æ¢æ ¼å¼å¹¶è®¡ç®—æƒ…æ„Ÿåˆ†æ•°
        for item in all_news:
            try:
                # æå–æ—¥æœŸ
                date_str = ""
                if item.get("published"):
                    if isinstance(item["published"], datetime):
                        date_str = item["published"].strftime("%Y-%m-%d")
                    elif isinstance(item["published"], str):
                        try:
                            date_obj = datetime.fromisoformat(item["published"].replace('Z', '+00:00'))
                            date_str = date_obj.strftime("%Y-%m-%d")
                        except:
                            date_str = datetime.now(timezone.utc).strftime("%Y-%m-%d")
                
                if not date_str:
                    date_str = datetime.now(timezone.utc).strftime("%Y-%m-%d")
                
                # è®¡ç®—æƒ…æ„Ÿåˆ†æ•°
                sentiment_score = calculate_sentiment_score(
                    item.get("title", ""),
                    item.get("summary", "")
                )
                
                news_item = {
                    "title": item.get("title", ""),
                    "source": item.get("source", "Unknown"),
                    "summary": item.get("summary", "")[:300],  # é™åˆ¶æ‘˜è¦é•¿åº¦
                    "sentiment_score": round(sentiment_score, 3),
                    "date": date_str
                }
                
                if news_item["title"]:  # åªæ·»åŠ æœ‰æ•ˆæ–°é—»
                    news_list.append(news_item)
            except Exception as e:
                print(f"âš ï¸ å¤„ç†æ–°é—»é¡¹æ—¶å‡ºé”™: {e}")
                continue
        
        # ä¿å­˜ç¼“å­˜
        if news_list:
            save_cache(news_list)
            print(f"âœ… æˆåŠŸæŠ“å–å¹¶ç¼“å­˜ {len(news_list)} æ¡æ–°é—»")
        else:
            # å¦‚æœæ²¡æœ‰æ–°æ•°æ®ï¼Œå°è¯•ä½¿ç”¨æ—§ç¼“å­˜
            cached_data = load_cache()
            if cached_data and cached_data.get("news"):
                print(f"âš ï¸ æœ¬æ¬¡æŠ“å–æœªè·å–åˆ°æ–°é—»ï¼Œä½¿ç”¨æ—§ç¼“å­˜ï¼ˆ{len(cached_data['news'])} æ¡ï¼‰")
                return cached_data["news"]
            print(f"âš ï¸ æœªè·å–åˆ°æ–°é—»ä¸”æ— ç¼“å­˜å¯ç”¨")
            
    except Exception as e:
        print(f"âš ï¸ æŠ“å–æ–°é—»æ—¶å‡ºé”™: {e}")
        # å‡ºé”™æ—¶å°è¯•ä½¿ç”¨æ—§ç¼“å­˜
        cached_data = load_cache()
        if cached_data and cached_data.get("news"):
            print(f"âš ï¸ APIå¤±æ•ˆæˆ–é¢åº¦ç”¨å°½ï¼Œä½¿ç”¨æ—§ç¼“å­˜ï¼ˆ{len(cached_data['news'])} æ¡ï¼‰")
            return cached_data["news"]
    
    return news_list


async def get_cached_news() -> List[Dict]:
    """
    è·å–ç¼“å­˜çš„æ–°é—»ï¼ˆä¸åˆ·æ–°ï¼‰
    
    Returns:
        List[Dict]: ç¼“å­˜çš„æ–°é—»åˆ—è¡¨
    """
    if not NEWS_CACHE_ENABLED:
        print("ğŸ›‘ [NEWS_CACHE] ç¼“å­˜è®¿é—®å·²ç¦ç”¨ï¼Œè¿”å›ç©ºç»“æœ")
        return []
    cached_data = load_cache()
    if cached_data and cached_data.get("news"):
        return cached_data["news"]
    return []


# å¯¼å‡ºå‡½æ•°
__all__ = [
    "fetch_and_cache_news",
    "get_cached_news",
    "load_cache",
    "save_cache",
    "calculate_sentiment_score",
    "CACHE_FILE"
]
